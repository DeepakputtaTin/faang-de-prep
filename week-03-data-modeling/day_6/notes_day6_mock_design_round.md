# Week 3 â€” Day 6: Mock Design Round â€” Netflix Continue Watching
**Date:** 2026-03-01
**Status:** âœ… Complete
**Mock Score:** 9.5/10 ğŸ†

---

## The Problem

> You're a Data Engineer at Netflix. Design the complete data architecture
> for Netflix's "Continue Watching" feature.
> - 200M subscribers worldwide
> - Remembers episode + exact timestamp where user stopped
> - Updates in real-time as they watch
> - Works across all devices (phone, TV, laptop)

---

## Step 1 â€” Clarifying Questions (Always First!)

**Questions to ask before designing anything:**

1. Multiple devices at same time? â†’ Yes, sync across ALL devices in real-time
2. How many shows in "Continue Watching"? â†’ Last 10 shows max
3. Does it expire? â†’ 90 days of inactivity â†’ drops off
4. Read heavy or write heavy? â†’ Write heavy â€” position updates every 30 sec
5. Consistency requirement? â†’ Eventual consistency fine, 1-2 sec lag acceptable
6. Geography? â†’ Global â€” US, Europe, Asia Pacific
7. DAU + watch time? â†’ 150M DAU, 2 hrs/day avg watch time
8. Peak hours? â†’ 7PM-11PM local time per region

---

## Step 2 â€” Back of Envelope Math

```
1 day â‰ˆ 100,000 seconds

Users/second:    150M Ã· 100K  = 1,500 users/sec avg
Peak users/sec:  1,500 Ã— 5    = 7,500 users/sec (7-11PM)

Write heavy (every 30 sec while watching):
Writes/day:      150M Ã— 2hrs Ã— 120 updates/hr = 36 Billion writes/day
Writes/sec:      36B Ã· 100K  = 360,000 writes/sec avg ğŸ˜±

Read (app open / device switch):
Reads/day:       150M Ã— 5 app opens = 750M reads/day
Reads/sec:       750M Ã· 100K = 7,500 reads/sec

Write:Read ratio = 48:1 â€” extreme write heavy!
```

**Conclusion:**
- 360K writes/sec â†’ can't hit Cassandra every 30 sec directly
- Solution: **Redis buffer â†’ flush to Cassandra every 5 min**
- 180TB+/year storage â†’ needs distributed, horizontal scale

---

## Step 3 â€” Polyglot Persistence

| Functional Need | Database | Why |
|---|---|---|
| User profiles, movie details | PostgreSQL | Relational, ACID, low volume |
| Watch history, last 10, 90-day history | Cassandra | Time-series, high write throughput |
| Real-time position updates, sessions | Redis | In-memory, <1ms, TTL support |
| Analytics | Snowflake | OLAP, Star Schema |

---

## Step 4 â€” Data Model

### PostgreSQL

```sql
CREATE TABLE user_profile (
    user_id           UUID PRIMARY KEY,
    username          VARCHAR(50) UNIQUE NOT NULL,
    subscription_type VARCHAR(20),   -- basic/standard/premium
    region            VARCHAR(20),   -- US, APAC, EU
    created_at        TIMESTAMP DEFAULT NOW()
);

CREATE TABLE movie_details (
    movie_id   UUID PRIMARY KEY,
    title      VARCHAR(200),
    creator    VARCHAR(100),
    runtime    INT,              -- total duration in seconds
    genre      VARCHAR(50),
    created_at TIMESTAMP DEFAULT NOW()
);
```

### Cassandra

```sql
-- Primary watch history table
-- â­ progress_sec = WHERE user stopped â€” CORE of Continue Watching!
CREATE TABLE user_watch_history (
    user_id      UUID,
    watched_at   TIMESTAMP,
    movie_id     UUID,
    progress_sec INT,       -- seconds into video (e.g. 3420 = 57 mins in)
    device_type  TEXT,      -- phone/TV/laptop
    PRIMARY KEY (user_id, watched_at)
) WITH CLUSTERING ORDER BY (watched_at DESC)
  AND default_time_to_live = 7776000;  -- 90 days TTL (90 Ã— 86400)

-- Last 10 movies = same table, LIMIT 10
-- 90 day history = same table, TTL handles auto-deletion

-- Watch position by device (cross-device sync)
CREATE TABLE watch_position_by_device (
    user_id      UUID,
    movie_id     UUID,
    device_type  TEXT,
    progress_sec INT,
    updated_at   TIMESTAMP,
    PRIMARY KEY ((user_id, movie_id), device_type)
) WITH CLUSTERING ORDER BY (device_type ASC);
```

### â­ The Critical Column Everyone Forgets

```
progress_sec INT â€” seconds into the video where user stopped

Without this â†’ "Continue Watching" just shows the show title
With this    â†’ resumes from exactly 57 minutes 23 seconds in

This is THE most important column in the entire design.
Never forget it!
```

### Redis

```
# Real-time position buffer (updated every 30 sec)
last_timestamp:{user_id}:{movie_id} â†’ progress_sec  (TTL: 24hr)

# Continue Watching list cache
continue_watching:{user_id} â†’ [last 10 shows + positions]  (TTL: 1hr)

# User session + device
session:{user_id} â†’ {device_type, region, login_time}  (TTL: session)

# Region-based content availability
region:{region_code} â†’ available_show_ids  (TTL: 1hr)
```

---

## Step 5 â€” Write Path + Read Path

### Write Path (when user watches content)

```
1. User logs in
   â†’ Fetch user_profile from PostgreSQL
   â†’ Source movie_list based on region

2. User selects movie
   â†’ Based on region, Cassandra displays available shows

3. User starts watching
   â†’ Every 30 seconds:
        Redis last_timestamp:{user_id}:{movie_id} updated (heavy writing)
        (Buffer in Redis â€” NOT hitting Cassandra every 30 sec!)

4. User stops/pauses at particular timestamp
   â†’ Final cache value flushed to Cassandra user_watch_history
   â†’ progress_sec saved permanently

5. Background flush job (every 5 min)
   â†’ Batch write Redis buffer â†’ Cassandra
   â†’ Handles crash recovery (if user closes app abruptly)
```

### Read Path (when user opens Netflix)

```
1. User logs in / opens app
   â†’ Check Redis: continue_watching:{user_id}
   â†’ Cache hit â†’ serve last 10 shows with progress in <1ms âœ…

2. Cache miss
   â†’ Fetch from Cassandra user_watch_history
        WHERE user_id = X
        ORDER BY watched_at DESC
        LIMIT 10
   â†’ Populate Redis cache (TTL: 1hr)

3. User clicks "Continue Watching" on a show
   â†’ Check Redis last_timestamp:{user_id}:{movie_id}
   â†’ Resume from progress_sec âœ…

4. Every 30 sec while watching
   â†’ Redis cache updates
   â†’ Background flush to Cassandra

5. 90 days inactivity
   â†’ Cassandra TTL = 7,776,000 sec auto-deletes row âœ…
   â†’ No cron job needed!
```

### Cross-Device Sync (bonus point!)

```
User watching on TV â†’ switches to phone:
1. TV updates Redis every 30 sec
2. Phone opens app â†’ reads from Redis last_timestamp
3. Phone resumes from exact same second as TV âœ…
4. Eventual consistency (1-2 sec lag) is acceptable per requirements
```

---

## Step 6 â€” Analytics Layer

```
Cassandra â†’ Kafka (CDC) â†’ Spark â†’ S3 (Delta Lake)
                                          â†“
                                    Snowflake
                                          â†“
                                    Star Schema:
                                    dim_users
                                    dim_movies
                                    fact_watch_history
                                      â†’ last_10_movies
                                      â†’ 90_days_history
                                      â†’ content_type
                                      â†’ region_login
                                          â†“
                                    Dashboards
```

**Analytics questions answered:**
- What was user watching? â†’ fact_watch_history + dim_movies
- What type of content users liked? â†’ genre analysis on dim_movies
- How many mins/day user is watching? â†’ SUM(progress_sec) per user per day

---

## Key Interview Lines to Memorize

> *"I'd buffer position updates in Redis every 30 seconds â€” 36 billion direct Cassandra writes/day is too expensive. Redis absorbs the write storm, we flush to Cassandra every 5 minutes."*

> *"The most critical column is progress_sec â€” without it, Continue Watching is just a show list, not a resume feature."*

> *"90-day TTL on Cassandra insert handles expiry natively â€” no cron job, no scheduled deletion pipeline needed."*

> *"Cross-device sync works through Redis â€” TV writes position every 30 sec, phone reads it on open. Eventual consistency with 1-2 sec lag is acceptable here."*

---

## Common Mistakes in This Problem

| Mistake | Impact | Fix |
|---|---|---|
| Forgetting progress_sec | Core feature doesn't work | Always ask "what data enables the feature?" |
| Writing to Cassandra every 30 sec | 360K writes/sec kills cluster | Redis buffer + periodic flush |
| Peak multiplier too low (2x) | Under-provision for prime time | Always use 3-5x peak multiplier |
| No cross-device sync design | Misses multi-device requirement | Redis as shared real-time state |
| No TTL for 90-day expiry | Manual deletion pipeline needed | default_time_to_live in Cassandra |

---

## Week 3 Complete â€” Full Summary

| Day | Topic | Status |
|---|---|---|
| Day 1 | Dimensional Modeling (Star/Snowflake) | âœ… |
| Day 2 | SCD Types 1, 2, 3 | âœ… |
| Day 3 | Normalization (1NF â†’ 3NF) | âœ… |
| Day 4 | NoSQL Modeling + Cassandra | âœ… |
| Day 5 | Social Media Case Study (Twitter + Instagram) | âœ… |
| Day 6 | Mock Design Round (Netflix) â€” 9.5/10 | âœ… ğŸ† |

---

## The Week 3 Mental Model

```
Raw Data (flat table)
      â†“
  Normalize (3NF) â†’ remove anomalies, clean OLTP
      â†“
  Model (Star Schema) â†’ denormalize for analytics
      â†“
  Scale (NoSQL) â†’ Cassandra for high write throughput
      â†“
  Cache (Redis) â†’ absorb write storms, sub-ms reads
      â†“
  Analyze (Snowflake) â†’ Star Schema + SCD Type 2
      â†“
  Serve â†’ dashboards, ML features, product decisions
```

This is the complete data engineering stack. You built it from scratch in 6 days. ğŸ’ª

---

## Commit

```bash
git add .
git commit -m "week-03: day 6 complete - Netflix Continue Watching mock design, 9.5/10, week 3 DONE"
git push
```
