def L(n, emoji, title, body):
    return f'<div class="level level-{n}"><div class="level-badge">{emoji} Level {n} ‚Äî {title}</div><div class="rich">{body}</div></div>'

WEEK3 = {

    # dimensional_modeling
    'dimensional_modeling': {
        'basics': '<div class="lesson-levels"><div class="level level-1"><div class="level-badge">üü¢ Level 1 ‚Äî The History ‚Äî Why This Exists</div><div class="rich"><h4>Why Dimensional Modeling Exists ‚Äî The History</h4><p>In the 1960s databases were built for transactions: record an order, update inventory, log a payment. Every table was tightly normalized. Queries were simple lookups. IBM\'s relational model was perfect for this.</p><p>Then businesses wanted <em>analytics</em>. Not "what is this customer\'s balance?" but "how did western-region revenue compare to last year, by product category and month?" These questions required joining 8-12 tables, aggregating millions of rows, slicing across multiple dimensions simultaneously. Normalized OLTP schemas were terrible at this ‚Äî joins were slow, SQL was unreadable, answers took hours.</p><p><strong>Ralph Kimball\'s insight (1990s):</strong> every business question has the same structure ‚Äî "How much [measure] by [dimension] by [dimension]?" He designed the <strong>Star Schema</strong>: one central fact table of numbers, surrounded by dimension tables of context. The shape is a star. It mirrors how humans think about data.</p><pre>           DIM_PRODUCT             DIM_DATE\n           (category, brand)       (year, month, quarter)\n                    \\\\              /\n       DIM_STORE ‚îÄ‚îÄ FACT_SALES ‚îÄ‚îÄ DIM_CUSTOMER\n       (city,region)  (revenue,   (segment, age)\n                       units,\n                       discount)\n</pre><p>‚úçÔ∏è <strong>Core rule:</strong> Fact tables contain MEASUREMENTS (numbers you aggregate: revenue, clicks, quantity). Dimension tables contain CONTEXT (who, what, where, when). Quick test: "Can I SUM this column?" ‚Äî yes ‚Üí fact/measure, no ‚Üí dimension/attribute.</p></div></div><div class="level level-2"><div class="level-badge">üîµ Level 2 ‚Äî Building a Star Schema</div><div class="rich"><h4>Building a Star Schema ‚Äî Step by Step</h4><p>The fact table contains one row per business event ‚Äî one sale, one click, one payment. It has foreign keys to every dimension (who/what/where/when) and numeric measures.</p><pre>CREATE TABLE fact_sales (\n  sale_id      BIGINT PRIMARY KEY,\n  date_key     INT REFERENCES dim_date(date_key),       -- WHEN\n  product_key  INT REFERENCES dim_product(product_key), -- WHAT\n  store_key    INT REFERENCES dim_store(store_key),      -- WHERE\n  customer_key INT REFERENCES dim_customer(customer_key),-- WHO\n  quantity_sold INT,\n  unit_price    DECIMAL(10,2),\n  total_revenue DECIMAL(10,2)   -- additive: SUM across all dims\n);</pre><h4>The Date Dimension ‚Äî Why It\'s Special</h4><p>The date dimension is pre-populated for 10 years (3,650 rows). It stores year, quarter, month, week, day, holiday/weekend flags so analysts can <code>GROUP BY d.quarter</code> without ever calling EXTRACT(). It is the most important and most universally present dimension in any data warehouse.</p><pre>CREATE TABLE dim_date (\n  date_key    INT PRIMARY KEY,   -- e.g. 20240115\n  full_date   DATE,\n  year INT, quarter INT, month INT, month_name VARCHAR(20),\n  is_holiday BOOLEAN, is_weekend BOOLEAN\n);</pre><p><strong>Surrogate keys:</strong> We use simple integer PKs in dimension tables, not natural business keys. If an upstream product ID format changes, only the ETL mapping changes ‚Äî not the entire fact table.</p></div></div><div class="level level-3"><div class="level-badge">üü° Level 3 ‚Äî Star vs Snowflake + Grain</div><div class="rich"><h4>Star vs Snowflake Schema ‚Äî When to Normalize Dimensions</h4><p><strong>Star schema:</strong> dimension tables are denormalized ‚Äî category, subcategory, and brand all live in one flat product dimension. Some string repetition, but JOIN queries are simple (one join per dimension).</p><p><strong>Snowflake schema:</strong> dimension tables are normalized ‚Äî dim_product ‚Üí dim_subcategory ‚Üí dim_category, each linked by FK. Less redundancy, but every analytical query needs 3 joins to get from a sale to its category.</p><table><tr><th>Aspect</th><td>Star Schema</td><td>Snowflake Schema</td></tr><tr><th>Storage</th><td>More (repeated strings)</td><td>Less (normalized)</td></tr><tr><th>Query complexity</th><td>1 join per dimension</td><td>3‚Äì4 joins per dimension chain</td></tr><tr><th>Query speed</th><td>Faster (fewer joins)</td><td>Slower on large data sets</td></tr><tr><th>FAANG preference</th><td>‚úÖ Almost universal</td><td>‚ùå Rare ‚Äî only for very large dims</td></tr></table><h4>Grain ‚Äî The Most Critical Design Decision</h4><p>A fact table\'s <strong>grain</strong> is the precise definition of what one row represents. Getting grain wrong is the most expensive modeling mistake ‚Äî nearly impossible to fix without rebuilding.</p><pre>Grains for sales data:\n  ‚úÖ "One row per line item in a sales transaction" (finest grain)\n  ‚úÖ "One row per sales receipt"\n  ‚úÖ "One row per day per store total"\n  ‚ùå "One row per transaction... sometimes per day" ‚Äî MIXED grain!\n\nMixed grain: SUM(revenue) double-counts. Every analyst gets different numbers.\nTrust in the data warehouse collapses.</pre></div></div><div class="level level-4"><div class="level-badge">üî¥ Level 4 ‚Äî FAANG Interview Framework</div><div class="rich"><h4>The Interview Framework: Designing a Data Warehouse From Scratch</h4><p>FAANG interview: "Design a data warehouse for an e-commerce company." The interviewer scores: (1) grain identified first, (2) facts vs dimensions correct, (3) SCD strategy named, (4) query pattern driven design.</p><pre>Step 1 ‚Äî Ask clarifying questions:\n  "What questions will analysts need to answer?"\n  "What is the finest level of detail needed? Line item or order?"\n  "How often does product/customer data change?"\n  "Expected data volume ‚Äî rows per day?"\n\nStep 2 ‚Äî State the grain explicitly:\n  "One row per order line item"\n\nStep 3 ‚Äî Identify measures:\n  quantity, unit_price, discount, line_total, margin\n\nStep 4 ‚Äî Identify dimensions:\n  dim_date (when), dim_customer (who), dim_product (what),\n  dim_store (where), dim_promotion (why discounted)\n\nStep 5 ‚Äî Handle attribute changes:\n  "Products change prices, customers move ‚Äî SCD Type 2 on both"\n\nStep 6 ‚Äî Plan aggregations:\n  "Daily dashboard sums pre-aggregated in agg_daily_sales\n   so we don\'t scan the 10B-row fact table on every request"</pre></div></div></div>',
        'key_concepts': ['Star schema = fact table (measures) surrounded by dimension tables (context). Optimized for analytical queries.', 'Fact table: events (sales, clicks). Contains FKs to dimensions + numeric measures to aggregate.', 'Additive measures: SUM across all dims (revenue). Semi-additive: only across some. Non-additive: never SUM (ratios).', 'Surrogate keys: simple integer PKs. Insulate DW from upstream ID format changes.', 'Grain: precise definition of one fact row. Must be declared, consistent, never mixed.', 'Star vs Snowflake: star = denormalized = simpler queries. Snowflake = normalized = more joins.', 'Date dimension: pre-built, one row per day, 10 years. Year/quarter/month/holiday flags for fast GROUP BY.', 'Design sequence: grain ‚Üí measures ‚Üí dimensions ‚Üí SCD strategy ‚Üí aggregation plan.'],
        'hints': ["'Can I SUM this column?' ‚Üí fact/measure. 'Is this descriptive context?' ‚Üí dimension attribute.", 'Always declare grain explicitly before DDL. Mixed-grain fact tables are the #1 data warehouse bug.', 'Surrogate keys protect against upstream ID changes. Never use natural source keys as fact table FKs.', 'Date dimension: analysts GROUP BY d.quarter without EXTRACT() ‚Äî huge usability win.'],
        'tasks': ['<strong>Step 1:</strong> Draw on paper: a star schema for a food delivery app. Define the grain. Name all fact measures and all dimensions.', '<strong>Step 2:</strong> Write DDL for fact_orders and dim_restaurant. Include surrogate keys and all measures.', '<strong>Step 3:</strong> Write a query joining fact_sales ‚Üí dim_date ‚Üí dim_product to get monthly revenue by category.', '<strong>Step 4:</strong> Redesign as a snowflake ‚Äî add dim_subcategory and dim_category as separate tables. How many more JOINs does the same query require?'],
        'hard_problem': 'Boss Problem (Amazon): Design a DW for marketplace analytics. Sellers list products, customers place orders with multiple line items, items can be returned. Design fact + dimension tables for: (1) revenue by seller, product category, date, (2) return rate analysis, (3) seller performance ranking. State grain of each fact table. Handle: products changing categories over time, international orders with currency conversion.',
    },

    # slowly_changing_dimensions
    'slowly_changing_dimensions': {
        'basics': '<div class="lesson-levels"><div class="level level-1"><div class="level-badge">üü¢ Level 1 ‚Äî The Problem and Three Strategies</div><div class="rich"><h4>The Business Problem That Makes SCDs Hard</h4><p>You\'ve built a star schema. dim_customer has customer_id, name, city, loyalty_tier. Your boss asks: "Show revenue from gold-tier customers last year ‚Äî but I want the tier they had <em>AT THE TIME OF PURCHASE</em>, not what tier they have today."</p><p>This reveals the fundamental problem: <strong>the real world changes over time</strong>. Customers move cities. Products change categories. If you simply UPDATE a dimension row, you permanently lose the history. Yesterday\'s record no longer exists. Historical questions become unanswerable.</p><p>This is the <strong>Slowly Changing Dimension (SCD)</strong> problem. Three strategies dominate:</p><ul><li><strong>SCD Type 1:</strong> Overwrite. No history kept. Simple but lossy.</li><li><strong>SCD Type 2:</strong> Add a new row per change. Full history. The gold standard.</li><li><strong>SCD Type 3:</strong> Add a "previous value" column. One level of history.</li></ul><p>‚úçÔ∏è <strong>In 95% of FAANG interviews and DW jobs, SCD means Type 2. Know this one deeply.</strong></p></div></div><div class="level level-2"><div class="level-badge">üîµ Level 2 ‚Äî Type 1, Type 2, Type 3 ‚Äî Full Examples</div><div class="rich"><h4>SCD Type 1: Overwrite ‚Äî Simple but History-Destroying</h4><pre>-- Alice moves from Boston to Dallas:\nUPDATE dim_customer SET city = \'Dallas\' WHERE customer_id = 42;\n-- ‚ö†Ô∏è  Boston is GONE. All historical orders now falsely show Dallas.\n-- Use Type 1 ONLY for data corrections (typos) ‚Äî never for real changes.</pre><h4>SCD Type 2: New Row Per Change ‚Äî Full History Preserved</h4><pre>CREATE TABLE dim_customer (\n  customer_key   INT PRIMARY KEY,  -- surrogate: changes with each version\n  customer_id    INT,              -- natural key: stays the same always\n  name           VARCHAR(100),\n  city           VARCHAR(100),\n  loyalty_tier   VARCHAR(20),\n  effective_start DATE NOT NULL,\n  effective_end   DATE,            -- NULL = currently active\n  is_current      BOOLEAN DEFAULT TRUE\n);\n\n-- Alice starts in Boston, Silver tier:\nINSERT INTO dim_customer VALUES (1001,42,\'Alice\',\'Boston\',\'Silver\',\'2023-01-01\',NULL,TRUE);\n\n-- Alice moves to Dallas on March 15, 2024:\n-- Step 1: Close old row\nUPDATE dim_customer SET effective_end=\'2024-03-15\', is_current=FALSE\nWHERE customer_id=42 AND is_current=TRUE;\n\n-- Step 2: Insert new row (NEW surrogate key!)\nINSERT INTO dim_customer VALUES (1087,42,\'Alice\',\'Dallas\',\'Silver\',\'2024-03-15\',NULL,TRUE);\n\n-- Alice now has TWO rows:\n-- key=1001: Boston, Silver, 2023-01-01 ‚Üí 2024-03-15 (historical)\n-- key=1087: Dallas, Silver, 2024-03-15 ‚Üí NULL (current)\n\n-- fact_sales rows from 2023 ‚Üí customer_key=1001 (Boston)\n-- fact_sales rows from Apr 2024 ‚Üí customer_key=1087 (Dallas)\n-- Revenue by city is now historically accurate ‚úÖ</pre><h4>SCD Type 3: Previous Value Column ‚Äî One Level Only</h4><pre>ALTER TABLE dim_customer ADD COLUMN prev_city VARCHAR(100);\nUPDATE dim_customer SET prev_city=city, city=\'Dallas\' WHERE customer_id=42;\n-- city=Dallas, prev_city=Boston\n-- ‚ö†Ô∏è  If Alice moves again to Austin: prev_city=Dallas, Boston is gone.\n-- Use for: planned reorgs where you need both old+new for 6 months only.</pre></div></div><div class="level level-3"><div class="level-badge">üü° Level 3 ‚Äî Query Patterns and ETL Pipeline</div><div class="rich"><h4>Three Essential SCD Type 2 Query Patterns</h4><p><strong>Pattern 1: Current state</strong></p><pre>SELECT customer_id, name, city, loyalty_tier\nFROM dim_customer WHERE is_current = TRUE;</pre><p><strong>Pattern 2: Historical accuracy ‚Äî tier at time of purchase</strong></p><pre>-- Revenue by tier AT time of sale (not current tier)\nSELECT c.loyalty_tier, SUM(f.total_revenue)\nFROM fact_sales f\nJOIN dim_customer c ON f.customer_key = c.customer_key  -- direct FK!\nJOIN dim_date d ON f.date_key = d.date_key\nWHERE d.year = 2023\nGROUP BY c.loyalty_tier;\n-- Works automatically: fact FK already points to the right version</pre><p><strong>Pattern 3: Point-in-time</strong> ‚Äî what tier was customer 42 on Feb 1, 2024?</p><pre>SELECT customer_id, name, city, loyalty_tier\nFROM dim_customer\nWHERE customer_id = 42\n  AND effective_start <= \'2024-02-01\'\n  AND (effective_end > \'2024-02-01\' OR effective_end IS NULL);</pre><h4>The SCD ETL Pipeline ‚Äî How Nightly Loads Work</h4><ol><li>Extract: pull changed records from OLTP (via CDC ‚Äî Change Data Capture)</li><li>Compare: for each changed record, compare to current dim row. Any tracked attribute changed?</li><li>Expire: UPDATE old row ‚Äî set effective_end = today, is_current = FALSE</li><li>Insert: INSERT new row with new surrogate key and new attribute values</li><li>No-change: skip records with unchanged tracked attributes entirely</li></ol></div></div><div class="level level-4"><div class="level-badge">üî¥ Level 4 ‚Äî Bi-temporal + Late-Arriving Facts</div><div class="rich"><h4>Bi-temporal Modeling ‚Äî Two Timelines</h4><p>SCD Type 2 tracks ONE timeline: when did the attribute change in our database? Bi-temporal modeling tracks TWO: <strong>valid time</strong> (when true in reality) vs <strong>transaction time</strong> (when our system recorded it). A customer might move cities Jan 1 but we don\'t record it until Jan 15 ‚Äî these differ.</p><pre>CREATE TABLE dim_customer_bitemporal (\n  customer_key    INT PRIMARY KEY,\n  customer_id     INT,\n  city            VARCHAR(100),\n  -- Axis 1: when true in the real world\n  valid_start     DATE,\n  valid_end       DATE,\n  -- Axis 2: when our system recorded this\n  recorded_start  TIMESTAMP,\n  recorded_end    TIMESTAMP\n);\n-- Enables: "What did our SYSTEM BELIEVE on Jan 10 about Jan 1\'s state?"\n-- Critical for: auditing, compliance, retroactive corrections</pre><h4>Late-Arriving Facts ‚Äî The Night Shift Problem</h4><p>A mobile app logs an event offline, connects to WiFi 3 days later. The event arrives Day+3 but the dimension may have changed in those 3 days. Load the fact by joining to the dimension version active on the ORIGINAL event date.</p><pre>INSERT INTO fact_sales (customer_key, ...)\nSELECT c.customer_key, ...\nFROM source_events e\nJOIN dim_customer c\n  ON c.customer_id = e.customer_id\n  AND e.event_date BETWEEN c.effective_start AND COALESCE(c.effective_end, \'9999-12-31\');</pre></div></div></div>',
        'key_concepts': ['SCD = Slowly Changing Dimension. Strategy for handling dimension attribute changes over time.', 'SCD Type 1: overwrite. Simple, destroys history. Use only for data corrections (typos).', 'SCD Type 2: new row per change. Full history preserved. The industry standard.', 'SCD Type 2 columns: surrogate key (changes), natural key (constant), effective_start, effective_end, is_current.', 'Fact table FK to surrogate key automatically delivers historical accuracy in all joins.', 'SCD Type 3: previous_value column. One level of history. Use for planned reorgs, not permanent tracking.', 'Point-in-time query: WHERE natural_key=X AND start<=date AND (end>date OR end IS NULL).', 'Bi-temporal: valid time (real world) + transaction time (recorded). For auditing and late corrections.', 'Late-arriving facts: join to dimension version active on the original event date, not current.'],
        'hints': ["Interview: 'Which SCD type?' ‚Äî almost always Type 2. Explain why Type 1 loses history.", 'Surrogate key changes with each new version row. Natural/business key stays the same across all versions.', 'Current records: WHERE is_current=TRUE or WHERE effective_end IS NULL. Both work.', 'MERGE statement (UPSERT) handles the SCD Type 2 ETL expire+insert in a single SQL command.'],
        'tasks': ['<strong>Step 1:</strong> Create dim_customer with SCD Type 2 columns. Insert one customer (Boston, Silver tier).', '<strong>Step 2:</strong> Simulate a tier upgrade (Silver ‚Üí Gold). Write the UPDATE + INSERT correctly.', "<strong>Step 3:</strong> Write a point-in-time query: 'What tier was customer 42 on March 1, 2024?' ‚Äî dates span before and after the change.", '<strong>Step 4:</strong> A late-arriving fact arrives with event_date=2023-11-15. Write the INSERT that joins to the correct historical dimension version.'],
        'hard_problem': 'Boss Problem (Spotify): 50M users, average 4 tier changes over 3 years. (1) Design dim_user with SCD Type 2. (2) A user can change country too ‚Äî when both tier AND country change on the same day, do you create 1 or 2 new rows? Why? (3) Write a MERGE statement that handles insert/update(expire+insert)/no-change in one SQL command. (4) If 200K users change tier every day, how does this affect fact table FK updates?',
    },

    # normalization
    'normalization': {
        'basics': '<div class="lesson-levels"><div class="level level-1"><div class="level-badge">üü¢ Level 1 ‚Äî The Anomaly Problem</div><div class="rich"><h4>The Anomaly Problem ‚Äî What Happens Without Normalization</h4><p>Imagine storing everything in one flat table:</p><pre>orders_flat:\norder_id ‚îÇ customer_id ‚îÇ customer_email    ‚îÇ product_id ‚îÇ product_name ‚îÇ price\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n1001     ‚îÇ  42         ‚îÇ alice@example.com  ‚îÇ  P01       ‚îÇ iPhone 15    ‚îÇ  999\n1002     ‚îÇ  42         ‚îÇ alice@example.com  ‚îÇ  P02       ‚îÇ AirPods      ‚îÇ  199\n1003     ‚îÇ  55         ‚îÇ bob@example.com    ‚îÇ  P01       ‚îÇ iPhone 15    ‚îÇ  999\n1004     ‚îÇ  42         ‚îÇ alice_new@ex.com   ‚îÇ  P03       ‚îÇ MacBook      ‚îÇ 1999</pre><p>This structure has three types of anomalies:</p><ul><li><strong>Update anomaly:</strong> Alice changes email ‚Üí must UPDATE every row for customer 42. Miss one row ‚Üí customer 42 now has two different emails ‚Üí data is inconsistent.</li><li><strong>Insert anomaly:</strong> Cannot add a new product to the database until someone orders it ‚Äî product data lives inside the orders row, not in its own table.</li><li><strong>Delete anomaly:</strong> Cancel order 1003 ‚Üí lose all information about Bob\'s existence entirely.</li></ul><p><strong>Normalization</strong> eliminates these anomalies by ensuring each fact lives in exactly one place. Formalized by E.F. Codd (inventor of the relational model) in 1972.</p><p>The three main normal forms:</p><ul><li><strong>1NF</strong> ‚Äî Atomic values. No lists or multi-values in one cell.</li><li><strong>2NF</strong> ‚Äî Every non-key column depends on the ENTIRE primary key.</li><li><strong>3NF</strong> ‚Äî Non-key columns depend ONLY on the primary key, not on each other.</li></ul><p>‚úçÔ∏è <strong>Memory trick:</strong> "The key (1NF), the whole key (2NF), and nothing but the key (3NF)."</p></div></div><div class="level level-2"><div class="level-badge">üîµ Level 2 ‚Äî 1NF, 2NF, 3NF Step by Step</div><div class="rich"><h4>1NF: Atomic Values ‚Äî No Multi-Values in a Cell</h4><pre>-- ‚ùå VIOLATES 1NF: multiple phone numbers in one cell\ncustomer_id ‚îÇ name  ‚îÇ phone_numbers\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n42          ‚îÇ Alice ‚îÇ 555-1234, 555-9876   ‚Üê TWO values in one cell!\n\n-- ‚úÖ 1NF: separate table, one value per row\ncustomer_phones: (customer_id, phone_number)\n42, 555-1234\n42, 555-9876</pre><h4>2NF: No Partial Dependencies (Only for Composite PKs)</h4><p>2NF only matters when the primary key uses multiple columns. Every non-key column must depend on the FULL composite key, not just part of it.</p><pre>-- ‚ùå VIOLATES 2NF ‚Äî PK = (order_id, product_id)\norder_items: order_id‚îÇproduct_id‚îÇquantity‚îÇproduct_name‚îÇcategory\n-- product_name and category depend ONLY on product_id, not the full PK!\n\n-- ‚úÖ 2NF: split out partial-dependent columns\norder_items: (order_id, product_id, quantity)   ‚Üê full-key only\nproducts:    (product_id, product_name, category) ‚Üê product-specific</pre><h4>3NF: No Transitive Dependencies</h4><p>3NF: a non-key column must not determine another non-key column. zip_code ‚Üí city is a transitive dependency (zip determines city, not customer_id).</p><pre>-- ‚ùå VIOLATES 3NF: zip_code determines city (non-key ‚Üí non-key)\ncustomers: customer_id‚îÇname‚îÇzip_code‚îÇcity\n42, Alice, 10001, New York   ‚Üê city determined by zip, not by customer_id!\n55, Bob,   10001, New York   ‚Üê same zip, same city always\n\n-- ‚úÖ 3NF: separate the zip-to-city mapping\nzip_codes: (zip_code, city, state)   ‚Üê zip determines city here\ncustomers: (customer_id, name, zip_code FK)  ‚Üê customer holds the zip</pre></div></div><div class="level level-3"><div class="level-badge">üü° Level 3 ‚Äî Denormalization ‚Äî When to Break Rules</div><div class="rich"><h4>Denormalization ‚Äî When NOT to Normalize</h4><p>A fully normalized (3NF) schema is ideal for OLTP where updates are frequent. For analytical queries on large data sets, full normalization is a performance disaster ‚Äî every business question requires 5‚Äì8 JOINs, and JOINs on 100M+ rows are expensive.</p><p><strong>Denormalization</strong> is the deliberate re-introduction of redundancy for query speed. This is intentional engineering, not bad design.</p><pre>-- Normalized 3NF: 6 joins to get revenue by category by region\nSELECT r.region, p.category, SUM(f.revenue)\nFROM fact_sales f\nJOIN dim_order o       ON f.order_id = o.order_id\nJOIN dim_product p     ON o.product_id = p.product_id\nJOIN dim_category c    ON p.category_id = c.category_id\nJOIN dim_customer cu   ON o.customer_id = cu.customer_id\nJOIN dim_region r      ON cu.region_id = r.region_id\nGROUP BY r.region, p.category;\n\n-- Star schema (denormalized): 2 joins only\nSELECT ds.region, dp.category, SUM(f.revenue)\nFROM fact_sales f\nJOIN dim_product dp ON f.product_key = dp.product_key\nJOIN dim_store   ds ON f.store_key   = ds.store_key\nGROUP BY ds.region, dp.category;</pre><table><tr><th>Scenario</th><td>Normalize?</td><td>Denormalize?</td></tr><tr><th>OLTP (transactions, user accounts)</th><td>‚úÖ Yes ‚Äî updates frequent</td><td>‚ùå Causes anomalies</td></tr><tr><th>Analytics / BI dashboards</th><td>‚ùå Too many joins, too slow</td><td>‚úÖ Wide pre-joined tables</td></tr><tr><th>Data warehouse (star schema)</th><td>‚ùå Snowflake = complex queries</td><td>‚úÖ Flat dimension tables</td></tr></table></div></div><div class="level level-4"><div class="level-badge">üî¥ Level 4 ‚Äî BCNF, 4NF, Decision Framework</div><div class="rich"><h4>BCNF, 4NF, and the Limits of Normalization</h4><p><strong>BCNF (Boyce-Codd Normal Form)</strong> is a slightly stronger version of 3NF: every determinant must be a candidate key. Most 3NF tables are automatically in BCNF ‚Äî violations only occur with multiple overlapping candidate keys. In practice: achieve 3NF and BCNF is usually also satisfied.</p><p><strong>4NF</strong> handles multi-valued dependencies: when columns are independently multi-valued but stored together, creating false combinations.</p><pre>-- ‚ùå 4NF violation: Alice has 2 hobbies AND 2 languages\n-- stored together ‚Üí 4 rows, but only 2+2 independent facts\nperson ‚îÇ hobby    ‚îÇ language\nAlice  ‚îÇ painting ‚îÇ English\nAlice  ‚îÇ painting ‚îÇ French   ‚Üê painting accidentally paired with French\nAlice  ‚îÇ cooking  ‚îÇ English\nAlice  ‚îÇ cooking  ‚îÇ French\n\n-- ‚úÖ 4NF: separate independent multi-valued facts\nperson_hobbies:   (Alice,painting), (Alice,cooking)\nperson_languages: (Alice,English), (Alice,French)</pre><h4>The Practical Decision Framework</h4><p>At FAANG, "is this normalized correctly?" is always followed by "for what purpose?" The answer changes the entire design:</p><ul><li><strong>OLTP:</strong> aim for 3NF/BCNF ‚Äî consistency, update integrity paramount</li><li><strong>Analytics/DW:</strong> deliberately denormalize ‚Äî query speed paramount</li><li><strong>Streaming/NoSQL:</strong> schema may not be relational ‚Äî access pattern drives design</li><li>Know which context you are in before choosing normalization strategy</li></ul></div></div></div>',
        'key_concepts': ['Normalization eliminates insert/update/delete anomalies. Each fact lives in exactly one place.', '1NF: atomic values. No lists, no arrays, no comma-separated multiple values in one cell.', '2NF: every non-key column depends on the FULL composite PK. Only relevant for composite-PK tables.', '3NF: every non-key column depends ONLY on the PK. No column-to-column (transitive) dependencies.', "Memory trick: 'the key (1NF), the whole key (2NF), and nothing but the key (3NF)'.", 'Denormalization: deliberate redundancy for query speed. Appropriate for analytics. Not a mistake.', 'OLTP ‚Üí normalize (updates frequent). Analytics ‚Üí denormalize (queries frequent).', 'BCNF: stronger than 3NF ‚Äî every determinant is a candidate key. Usually automatic with 3NF.'],
        'hints': ['Updating one value ‚Üí must change multiple rows ‚Üí find which normalization form is violated.', '2NF violations always involve composite PKs. Single-column PK = automatically 2NF.', 'Transitive dependency: A‚ÜíB‚ÜíC where A is PK. Violates 3NF. Fix: move B‚ÜíC to its own table.', 'Denormalization is correct when the trade-off is intentional and documented.'],
        'tasks': ['<strong>Step 1:</strong> Given: orders(order_id, customer_id, customer_email, product_id, product_name, quantity, price). Identify which normal form it violates and which specific columns cause violations.', '<strong>Step 2:</strong> Normalize to 3NF. Draw the schema with 3 tables, their PKs and FKs.', '<strong>Step 3:</strong> Write DDL for all 3 normalized tables with PK and FK constraints.', '<strong>Step 4:</strong> Write the query on the normalized schema that gets total revenue by product category for customer 42. How many JOINs vs the flat table version?'],
        'hard_problem': "Boss Problem (Stripe): Flat table: payments(payment_id, merchant_id, merchant_name, merchant_country, buyer_id, buyer_email, buyer_country, payment_method_id, method_type, method_last4, amount, currency, timestamp). (1) Identify every normalization violation ‚Äî state which NF rule is broken and why. (2) Normalize to 3NF ‚Äî draw the full schema. (3) A business analyst says 'your normalized schema is too slow ‚Äî queries take 2 minutes.' How do you respond and what do you build?",
    },

    # nosql_patterns
    'nosql_patterns': {
        'basics': '<div class="lesson-levels"><div class="level level-1"><div class="level-badge">üü¢ Level 1 ‚Äî Why NoSQL Exists ‚Äî The Mental Model Shift</div><div class="rich"><h4>Why NoSQL Was Invented ‚Äî The 2000s Scaling Crisis</h4><p>In the early 2000s, companies like Google, Amazon, and Facebook hit a fundamental wall with relational databases:</p><ul><li><strong>Scale:</strong> PostgreSQL on one server handles maybe 10TB. Google had petabytes.</li><li><strong>Speed:</strong> JOIN operations on billions of rows are slow regardless of optimization.</li><li><strong>Flexibility:</strong> Rigid schemas could not keep up with fast-changing product requirements.</li></ul><p>NoSQL was born to solve these. But it comes with trade-offs. The most important: you lose the ability to freely JOIN tables and get ACID guarantees simultaneously.</p><h4>The Fundamental Mental Model Shift</h4><p><strong>Relational (SQL):</strong> "Design around entities and relationships. Queries figure themselves out. Normalize first, add indexes later."</p><p><strong>NoSQL:</strong> "Design around ACCESS PATTERNS. Know your most common queries first, then design your data model to serve them in O(1). If you do not know your access patterns, you cannot design a NoSQL model."</p><p>‚úçÔ∏è <strong>Write this down:</strong> This is the most important conceptual shift in NoSQL. SQL is query-flexible but schema-rigid. NoSQL is query-specific but schema-flexible.</p><h4>The 4 Families of NoSQL Databases</h4><table><tr><th>Type</th><td>Examples</td><td>Best For</td></tr><tr><th>Key-Value</th><td>Redis, DynamoDB</td><td>Session storage, caching, simple lookups by ID</td></tr><tr><th>Wide-Column</th><td>Cassandra, HBase</td><td>Time-series, event logs, high write throughput</td></tr><tr><th>Document</th><td>MongoDB, Firestore</td><td>JSON objects, variable structure, nested documents</td></tr><tr><th>Graph</th><td>Neo4j, Amazon Neptune</td><td>Social networks, fraud detection, relationship traversal</td></tr></table><p>The family choice is determined by your access patterns. Use the wrong family and even perfect schema design cannot save query performance.</p></div></div><div class="level level-2"><div class="level-badge">üîµ Level 2 ‚Äî Cassandra: Wide-Column + Three Rules</div><div class="rich"><h4>Cassandra: Wide-Column Store ‚Äî Three Absolute Rules</h4><p>Apache Cassandra is the choice when you need: massive write throughput (millions of writes/second), linear horizontal scalability, and no-single-point-of-failure availability. Used by Apple (Siri storage), Netflix (viewing history), Instagram (feed). But Cassandra has hard rules ‚Äî violating them destroys performance.</p><p>The three rules:</p><ol><li><strong>Rule 1: Queries drive tables.</strong> In SQL you design one table and write any query. In Cassandra, design one table PER query pattern. The table exists to serve exactly one access pattern efficiently.</li><li><strong>Rule 2: No JOINs, no subqueries.</strong> Data must be pre-joined at write time. If you need user data alongside their posts, denormalize: store user data inside the posts table.</li><li><strong>Rule 3: No unbounded queries.</strong> Every query MUST include the partition key in WHERE. You cannot do full table scans or range queries without a partition key ‚Äî Cassandra will reject or time out.</li></ol><h4>Partition Key vs Clustering Key</h4><p>In Cassandra, the PRIMARY KEY has two parts: <code>(partition_key, clustering_key)</code>. The partition key determines which node stores the data. The clustering key determines the sort order of rows WITHIN a partition.</p><pre>-- Netflix watch history: one partition per user\n-- All of one user\'s history on the same node for fast retrieval\nCREATE TABLE watch_history (\n  user_id    UUID,              -- PARTITION KEY: routes to one node\n  watched_at TIMESTAMP,         -- CLUSTERING KEY: sorted within partition\n  show_id    UUID,\n  show_title TEXT,\n  duration_mins INT,\n  PRIMARY KEY (user_id, watched_at)\n) WITH CLUSTERING ORDER BY (watched_at DESC);\n\n-- ‚úÖ Fast: give me user 42\'s last 20 shows\nSELECT * FROM watch_history WHERE user_id=42 LIMIT 20;\n\n-- ‚ùå Rejected: no partition key given ‚Äî would scan entire cluster\nSELECT * FROM watch_history WHERE show_title=\'Stranger Things\';</pre><p>The LIMIT 20 is served from a single node in milliseconds ‚Äî O(1) lookup. The rejected query would require scanning every partition on every node ‚Äî Cassandra refuses it.</p></div></div><div class="level level-3"><div class="level-badge">üü° Level 3 ‚Äî DynamoDB: Single-Table Design</div><div class="rich"><h4>DynamoDB: Single-Table Design</h4><p>Amazon DynamoDB is a managed key-value and document store. The "single-table design" pattern ‚Äî storing multiple entity types in ONE table using a generic partition key (PK) and sort key (SK) ‚Äî is the most important and most misunderstood DynamoDB pattern.</p><p>The motivation: DynamoDB charges for reads/writes per request. If you need Customer + Orders + OrderItems in one API call, querying 3 separate tables costs 3 requests. Single-table design allows all related entities to be co-located under one partition ‚Äî fetched in a single request.</p><pre>-- Single table: "ecommerce_table"\n-- pk=partition key, sk=sort key ‚Äî generic names, specific values\n--\n-- Row type  ‚îÇ pk            ‚îÇ sk              ‚îÇ attributes\n-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n-- Customer  ‚îÇ CUST#42       ‚îÇ METADATA        ‚îÇ name, email, city\n-- Order     ‚îÇ CUST#42       ‚îÇ ORDER#1001      ‚îÇ total, status, date\n-- OrderItem ‚îÇ CUST#42       ‚îÇ ORDER#1001#P01  ‚îÇ qty, price, name\n-- Product   ‚îÇ PRODUCT#P01   ‚îÇ METADATA        ‚îÇ name, category, price\n--\n-- Query: get customer 42 + all their orders in ONE request:\nGET pk=CUST#42            ‚Üí customer metadata + all orders + all items\n\n-- Query: get just orders for customer 42:\nQUERY pk=CUST#42, sk BEGINS_WITH "ORDER#"</pre><h4>The SQL‚ÜíNoSQL Decision Framework</h4><table><tr><th>Requirement</th><td>Choose</td></tr><tr><th>Complex ad-hoc reporting with JOINs</th><td>PostgreSQL / BigQuery (SQL)</td></tr><tr><th>High write throughput (>100K writes/sec), time-series</th><td>Cassandra</td></tr><tr><th>Single-millisecond reads by ID at massive scale</th><td>DynamoDB / Redis</td></tr><tr><th>Flexible/variable document structure</th><td>MongoDB</td></tr><tr><th>Graph traversal (friends-of-friends, fraud rings)</th><td>Neo4j / Neptune</td></tr><tr><th>Caching, session storage (data expires)</th><td>Redis</td></tr></table></div></div><div class="level level-4"><div class="level-badge">üî¥ Level 4 ‚Äî CAP Theorem + Instagram Feed Design</div><div class="rich"><h4>CAP Theorem ‚Äî The Fundamental Distributed Systems Constraint</h4><p>CAP theorem (Brewer, 2000): in a distributed system, you can guarantee at most TWO of three properties simultaneously:</p><ul><li><strong>Consistency (C):</strong> Every read receives the most recent write, or an error. No stale data.</li><li><strong>Availability (A):</strong> Every request receives a response (not guaranteed to be latest).</li><li><strong>Partition Tolerance (P):</strong> The system operates even when network messages are lost between nodes.</li></ul><p>Network partitions are unavoidable in real distributed systems ‚Äî you ALWAYS need P. So the real choice is: sacrifice C (choose AP) or sacrifice A (choose CP).</p><table><tr><th>Database</th><td>CAP Choice</td><td>Trade-off</td></tr><tr><th>Cassandra</th><td>AP (tunable)</td><td>May return stale reads. Uses eventual consistency by default.</td></tr><tr><th>HBase</th><td>CP</td><td>May refuse requests during partition. Strongly consistent.</td></tr><tr><th>DynamoDB</th><td>AP (default) / CP (opt-in)</td><td>Eventually consistent by default, strongly consistent reads available.</td></tr><tr><th>Redis</th><td>AP (single-node) / CP (cluster)</td><td>Depends on replication config and cluster mode.</td></tr><tr><th>PostgreSQL (single)</th><td>CP</td><td>No partition tolerance ‚Äî it is a single node.</td></tr></table><h4>FAANG Interview: Modeling Instagram Feed in Cassandra</h4><p>Interview question: "Design the data model for Instagram user feeds in Cassandra."</p><pre>-- Access pattern: "Get my feed ‚Äî posts from people I follow, newest first"\n-- Option A: Fan-out on Write\n--   When Alice posts ‚Üí immediately write to every follower\'s feed partition\n--   Pro: feed reads are O(1)\n--   Con: if Alice has 10M followers, one post = 10M writes\n\n-- Option B: Fan-out on Read\n--   When user requests feed ‚Üí read from all 300 followees\' timelines, merge\n--   Pro: writes are cheap (one write per post)\n--   Con: feed reads are O(followees) ‚Äî slow for users who follow 1,000 accounts\n\n-- Real Instagram hybrid: fan-out on write for normal users,\n--   fan-out on read for celebrity accounts (10M+ followers)</pre></div></div></div>',
        'key_concepts': ["NoSQL was built to address SQL's scale (storage), speed (join cost), and flexibility (schema rigidity) limits.", 'Fundamental shift: SQL = design around entities, queries are flexible. NoSQL = design around access patterns first.', 'Key-Value (Redis/DynamoDB): O(1) by ID. Wide-Column (Cassandra): time-series, high write throughput.', 'Document (MongoDB): variable/nested structures. Graph (Neo4j): relationship traversal.', 'Cassandra Rule 1: one table per query. Rule 2: no JOINs ‚Äî pre-join at write time. Rule 3: always provide partition key.', 'Partition key = which node stores the data. Clustering key = sort order within that partition.', 'DynamoDB single-table: all entity types in one table with generic PK/SK. Co-located for single-request fetches.', 'CAP theorem: choose 2 of Consistency/Availability/Partition-tolerance. Partitions are unavoidable ‚Üí choose C or A.', 'Cassandra = AP (eventual). HBase = CP (strong). DynamoDB = AP by default, CP opt-in.'],
        'hints': ["Interview: 'When would you use Cassandra?' ‚Üí high write throughput, time-series, queries always by partition key.", 'Cassandra: missing partition key in WHERE = full cluster scan = query rejected. Always include partition key.', 'DynamoDB single-table: use BEGINS_WITH on SK to query subsets (all orders for a customer).', 'CAP: Cassandra AP = may return stale reads. HBase CP = may refuse under partition. Trade-off, not bug.'],
        'tasks': ["<strong>Step 1:</strong> Design a Cassandra table for: 'Get all messages in a chat room, newest first, limit 50.' State partition key, clustering key, and why.", '<strong>Step 2:</strong> Design a DynamoDB single-table for an e-commerce app with customers, orders, and order items. Show 3 rows of sample data.', '<strong>Step 3:</strong> Given the fan-out Instagram feed problem: write the data model for fan-out-on-write. What happens when a celebrity with 10M followers posts?', "<strong>Step 4:</strong> For each of 3 systems (Twitter, Netflix, Uber), state which NoSQL family you'd use and why."],
        'hard_problem': 'Boss Problem (Facebook Messenger): Design the data model for a chat app. Access patterns: (1) get the last 50 messages in a conversation, newest first; (2) get all conversations for a user, sorted by most recent message; (3) get unread message count per conversation. 10B messages/day. Design: SQL, Cassandra, or DynamoDB? Why? What is the partition key for each table? Handle: group chats (1 conversation, many users), message deletion, read receipts.',
    },

    # schema_design_interview
    'schema_design_interview': {
        'basics': '<div class="lesson-levels"><div class="level level-1"><div class="level-badge">üü¢ Level 1 ‚Äî What Schema Design Interviews Test</div><div class="rich"><h4>What Schema Design Interviews Actually Test</h4><p>Schema design interviews are not about knowing the "right answer." They test: (1) how you ask clarifying questions before designing, (2) how you identify access patterns before picking a database, (3) how you handle evolving requirements, (4) how you discuss trade-offs openly instead of presenting one solution as absolute truth.</p><p>The most common failure: jumping into CREATE TABLE statements in the first 30 seconds. Interviewers specifically watch to see whether you ask questions first. An engineer who designs first and asks questions later is dangerous in production.</p><h4>The Framework: 5 Questions Before Any Design</h4><ol><li><strong>"What are the most frequent read queries?"</strong> ‚Äî Access patterns drive everything.</li><li><strong>"What are the most frequent write patterns?"</strong> ‚Äî High write ‚Üí Cassandra. Mostly reads ‚Üí PostgreSQL/DynamoDB.</li><li><strong>"What is the expected data volume?"</strong> ‚Äî 10K rows/day ‚Üí any DB. 10M rows/day ‚Üí need partitioning strategy.</li><li><strong>"How fresh must the data be?"</strong> ‚Äî Real-time ‚Üí strong consistency. Dashboard ‚Üí eventual OK.</li><li><strong>"What is the read-to-write ratio?"</strong> ‚Äî 1000:1 ‚Üí optimize reads. 1:1 ‚Üí optimize both.</li></ol><p>‚úçÔ∏è After asking these 5 questions, you have enough information to make an informed schema decision. Before asking them, you are guessing.</p></div></div><div class="level level-2"><div class="level-badge">üîµ Level 2 ‚Äî Twitter Schema ‚Äî Full Worked Example</div><div class="rich"><h4>Designing Twitter\'s Schema ‚Äî A Worked Example</h4><p><strong>Requirements:</strong> Users post tweets. Users follow other users. Users see a timeline of tweets from accounts they follow. System has 500M users, 500M tweets/day, average user follows 300 accounts.</p><h4>Step 1: Access Patterns</h4><pre>Read patterns (must be fast):\n  R1: Get a user\'s home timeline (latest tweets from followees), newest first\n  R2: Get a user\'s own tweets (profile page)\n  R3: Get a single tweet by ID\n  R4: Get a user\'s follower/following counts\n\nWrite patterns:\n  W1: Post a new tweet\n  W2: Follow / unfollow a user\n  W3: Like / retweet a tweet\n\nVolume: 500M tweets/day = ~5,800 tweets/second. 100B timeline reads/day.</pre><h4>Step 2: Data Entities and SQL Schema</h4><pre>-- Core entities\nCREATE TABLE users (\n  user_id BIGINT PRIMARY KEY,\n  username VARCHAR(50) UNIQUE,\n  display_name VARCHAR(100),\n  bio TEXT,\n  follower_count INT,    -- denormalized counter (avoid COUNT(*) on every read)\n  following_count INT\n);\n\nCREATE TABLE tweets (\n  tweet_id BIGINT PRIMARY KEY,  -- Snowflake ID: encodes timestamp+machine+seq\n  user_id  BIGINT REFERENCES users(user_id),\n  content  VARCHAR(280),\n  created_at TIMESTAMP,\n  like_count INT,         -- denormalized: avoid COUNT per query\n  retweet_count INT\n);\n\nCREATE TABLE follows (\n  follower_id BIGINT REFERENCES users(user_id),\n  followee_id BIGINT REFERENCES users(user_id),\n  followed_at TIMESTAMP,\n  PRIMARY KEY (follower_id, followee_id)\n);\nCREATE INDEX ON follows(followee_id);  -- reverse: who follows this user?</pre><h4>Step 3: The Timeline Problem</h4><p>Naive timeline query: <code>SELECT t.* FROM tweets t JOIN follows f ON t.user_id = f.followee_id WHERE f.follower_id=42 ORDER BY created_at DESC LIMIT 20</code>. This joins 500M tweets √ó 300 followees = runs the follower check on millions of rows. Too slow.</p><p><strong>Solution: Precomputed timeline fan-out.</strong> When Alice posts a tweet, immediately write it to every follower\'s timeline cache (Redis sorted set by timestamp). Timeline reads become O(1). Trade-off: writes are amplified (300 follower writes per tweet from an average user).</p></div></div><div class="level level-3"><div class="level-badge">üü° Level 3 ‚Äî Edge Cases ‚Äî The Interview Differentiator</div><div class="rich"><h4>Handling Edge Cases ‚Äî The Interview Differentiator</h4><p>Strong schema design candidates address edge cases before the interviewer asks. Here are the 5 most common schema edge cases and how to handle each:</p><table><tr><th>Edge Case</th><td>Problem</td><td>Solution</td></tr><tr><th>Soft deletes</th><td>DELETE removes audit trail</td><td>Add is_deleted BOOLEAN + deleted_at TIMESTAMP. Filter WHERE NOT is_deleted.</td></tr><tr><th>Large blobs (images/video)</th><td>Storing files in DB is slow and expensive</td><td>Store in object storage (S3). DB stores only the URL/path.</td></tr><tr><th>Counters (likes, followers)</th><td>COUNT(*) on every request is expensive</td><td>Denormalize: maintain a counter column. Increment on write.</td></tr><tr><th>User-generated content moderation</th><td>Need to hide content without deleting</td><td>Add status enum: ACTIVE / HIDDEN / REMOVED. Filter on status.</td></tr><tr><th>Time zones</th><td>ambiguous timestamps in analytics</td><td>Always store timestamps in UTC. Convert to user local time in the application layer.</td></tr></table><h4>Indexing as Part of Schema Design</h4><p>An incomplete schema interview answer presents only the CREATE TABLE DDL. A complete answer also states: which columns need indexes and why. For every frequent query\'s WHERE and ORDER BY clause, explicitly name the index.</p><pre>-- Twitter schema indexes:\nCREATE INDEX tweets_user_created ON tweets(user_id, created_at DESC);\n-- Supports: WHERE user_id=X ORDER BY created_at DESC LIMIT 20 (profile page)\n\nCREATE INDEX follows_followee ON follows(followee_id);\n-- Supports: WHERE followee_id=X (find all followers of a user)\n\nCREATE INDEX tweets_created ON tweets(created_at DESC);\n-- Supports: global trending (recent tweets across all users)</pre></div></div><div class="level level-4"><div class="level-badge">üî¥ Level 4 ‚Äî Uber Schema + Geospatial Design</div><div class="rich"><h4>Uber/Lyft Schema: The Ride + Driver Matching Problem</h4><p>A more complex FAANG schema problem: design the data model for Uber trips.</p><pre>-- Entities: users (riders), drivers, vehicles, trips, payments\n\nCREATE TABLE drivers (\n  driver_id   BIGINT PRIMARY KEY,\n  user_id     BIGINT UNIQUE REFERENCES users(user_id),\n  license_no  VARCHAR(50),\n  rating      DECIMAL(3,2),\n  status      ENUM(\'ACTIVE\',\'INACTIVE\',\'SUSPENDED\')\n);\n\nCREATE TABLE trips (\n  trip_id         BIGINT PRIMARY KEY,\n  rider_id        BIGINT REFERENCES users(user_id),\n  driver_id       BIGINT REFERENCES drivers(driver_id),\n  vehicle_id      BIGINT REFERENCES vehicles(vehicle_id),\n  status          ENUM(\'REQUESTED\',\'ACCEPTED\',\'IN_PROGRESS\',\'COMPLETED\',\'CANCELLED\'),\n  requested_at    TIMESTAMP,\n  pickup_lat      DECIMAL(9,6),\n  pickup_lng      DECIMAL(9,6),\n  dropoff_lat     DECIMAL(9,6),\n  dropoff_lng     DECIMAL(9,6),\n  fare_amount     DECIMAL(10,2),\n  distance_miles  DECIMAL(8,2),\n  duration_mins   INT\n);\n\n-- For matching: drivers need to be queried by location\n-- Use geospatial index (PostGIS) on current lat/lng\nCREATE INDEX idx_driver_location ON driver_locations\nUSING GIST (location);  -- enables radius queries: WHERE ST_Distance(location, point) < 2km</pre><p><strong>Key design decisions to state in an interview:</strong></p><ul><li>Driver real-time location goes in a SEPARATE table (high write frequency, different retention)</li><li>Geospatial queries require GIST index (not B-Tree)</li><li>Trip status uses ENUM ‚Äî adding a new state requires ALTER TABLE (discuss migration strategy)</li><li>Fare calculation stored as result (denormalized) ‚Äî never recompute from distance/time on every read</li><li>Soft deletes on trips (audit trail, disputes) ‚Äî never hard DELETE trip records</li></ul></div></div></div>',
        'key_concepts': ['Schema design interviews test process (ask first, design second) more than encyclopedic knowledge.', '5 questions before any design: access patterns, write patterns, volume, freshness requirement, read/write ratio.', 'Twitter timeline problem: naive JOIN on followees is O(followees) ‚Äî precomputed feed fan-out is O(1).', 'Denormalized counters (like_count, follower_count): maintain on write to avoid COUNT(*) on every read.', 'Always include index design in your schema answer ‚Äî incomplete without it.', 'Snowflake IDs (tweet_id): encode timestamp + machine ID + sequence. Sortable by time, no central coordinator.', 'Geospatial queries require GIST index (not B-Tree). Always mention this for location-based schemas.', 'Soft deletes: is_deleted + deleted_at. Never hard DELETE records that may be needed for disputes/audits.'],
        'hints': ['Never jump to CREATE TABLE before asking clarifying questions ‚Äî it is a red flag in schema interviews.', 'Counter columns (follower_count): faster than COUNT(*) per request. Trade-off: counts can drift if not careful.', 'Status as ENUM vs VARCHAR: ENUM is validated at DB level (safer). VARCHAR is more flexible for new states.', "Always state your indexes ‚Äî 'I would add an index on user_id + created_at DESC for the profile page query.'"],
        'tasks': ['<strong>Step 1:</strong> Design the schema for a URL shortener (bit.ly). What are the access patterns? What is the primary table? What index makes redirect O(1)?', '<strong>Step 2:</strong> Design the schema for a parking app (SpotHero). Users book parking spots by location and time. Handle: spots have complex pricing rules, bookings can be cancelled.', '<strong>Step 3:</strong> Take your Twitter schema from Level 2. Add: (1) soft deletes on tweets, (2) a hashtags table with many-to-many to tweets, (3) a media_attachments table for images.', "<strong>Step 4:</strong> Mock: design YouTube's schema in 20 minutes. Ask yourself the 5 questions first. Then design. Then check: did you state all indexes? Did you mention edge cases?"],
        'hard_problem': "Boss Problem (Airbnb): Design Airbnb's complete data model: properties (photos, amenities, room types), hosts, guests, bookings (no double-booking), reviews (bidirectional: host reviews guest AND guest reviews property), pricing (variable by date, seasonality). The analytics team needs a star schema for: revenue by city/month, occupancy rate by property type, host performance. Design: (1) operational schema (OLTP), (2) DW schema (star schema), (3) which data flows from OLTP to DW via ETL.",
    },

    # week3_rest
    'week3_rest': {
        'basics': '<div class="lesson-levels"><div class="level level-1"><div class="level-badge">üü¢ Level 1 ‚Äî Consolidation ‚Äî What to Focus On</div><div class="rich"><h4>Week 3 Was Dense ‚Äî Here\'s How to Consolidate It</h4><p>This week covered: dimensional modeling (star schemas, grain, surrogate keys), slowly changing dimensions (Types 1/2/3, bi-temporal, late-arriving facts), normalization (1NF/2NF/3NF, denormalization trade-offs), NoSQL modeling (Cassandra rules, DynamoDB single-table, CAP theorem), and schema design interviews (5-question framework, edge cases).</p><p>That is 5 major bodies of knowledge in 5 days. Today\'s goal: surface the knowledge you\'ve acquired so it becomes retrievable under pressure ‚Äî not just familiar when re-read.</p><h4>The Most Commonly Confused Concepts This Week</h4><ul><li><strong>Star vs Snowflake:</strong> star = flat dims = simpler queries. Most people say "snowflake is better because it\'s more normalized" ‚Äî wrong for analytics.</li><li><strong>SCD Type 1 vs 2:</strong> Type 1 loses history. Type 2 preserves history. You almost always want Type 2.</li><li><strong>2NF vs 3NF:</strong> 2NF is about partial dependencies on composite keys. 3NF is about column-to-column (transitive) dependencies.</li><li><strong>Cassandra partition key vs clustering key:</strong> partition key routes to a node, clustering key sorts within that partition.</li><li><strong>CAP theorem choices:</strong> Cassandra = AP (eventually consistent). HBase = CP (strongly consistent). Not interchangeable.</li></ul></div></div><div class="level level-2"><div class="level-badge">üîµ Level 2 ‚Äî Active Recall Quiz</div><div class="rich"><h4>Active Recall ‚Äî No Notes Allowed</h4><pre>Dimensional Modeling:\n‚ñ° What is a fact table? What is a dimension table? Name 3 examples of each.\n‚ñ° What is grain? Give an example of a mixed-grain table and why it breaks analytics.\n‚ñ° Why are surrogate keys used instead of natural keys?\n‚ñ° What columns does a date dimension always have?\n\nSCDs:\n‚ñ° Draw SCD Type 2 ‚Äî what columns are required? What happens when an attribute changes?\n‚ñ° Write the UPDATE + INSERT pattern from memory for closing/opening a Type 2 row.\n‚ñ° What is the point-in-time query pattern? Write the WHERE clause.\n\nNormalization:\n‚ñ° Explain 2NF in one sentence without jargon.\n‚ñ° Give an example of a 3NF violation and how to fix it.\n‚ñ° When is denormalization correct? When is it wrong?\n\nNoSQL:\n‚ñ° What are Cassandra\'s 3 rules? Why can\'t you query without a partition key?\n‚ñ° Explain DynamoDB single-table design in 2 sentences.\n‚ñ° CAP theorem: what does Cassandra sacrifice? What does HBase sacrifice?\n\nScore: 14+/16 = interview ready | <10 = revisit weakest day</pre></div></div><div class="level level-3"><div class="level-badge">üü° Level 3 ‚Äî Spaced Repetition Schedule</div><div class="rich"><h4>Spaced Repetition Schedule</h4><table><tr><th>Topic</th><td>Review in 3 days</td><td>Review in 7 days</td><td>Review in 30 days</td></tr><tr><th>Dimensional Modeling</th><td>Redesign a star schema from scratch</td><td>Add SCD handling to your design</td><td>Design a complete DW: Amazon or Netflix</td></tr><tr><th>SCDs</th><td>Write the SCD Type 2 ETL MERGE</td><td>Point-in-time query on real data</td><td>Explain bi-temporal to a colleague</td></tr><tr><th>Normalization</th><td>Normalize a 10-column flat table</td><td>Identify violations in real schemas</td><td>Explain 3NF violation to a non-engineer</td></tr><tr><th>NoSQL</th><td>Design a Cassandra table for an access pattern</td><td>When Cassandra vs DynamoDB</td><td>CAP theorem applied to 3 real systems</td></tr><tr><th>Schema Design</th><td>Mock interview: design Twitter schema</td><td>Add indexes to your design</td><td>Review a real production schema at work</td></tr></table><p>‚úçÔ∏è <strong>The most effective review:</strong> schedule a 45-minute mock interview with yourself or the chatbot. Pick one of this week\'s Boss Problems and work through it completely from scratch, out loud, under time pressure.</p></div></div><div class="level level-4"><div class="level-badge">üî¥ Level 4 ‚Äî 45-Minute Mock Interview</div><div class="rich"><h4>45-Minute Mock Interview ‚Äî Data Modeling Exam</h4><pre>Timer: 45 minutes. No notes. Speak out loud as you design.\n\nProblem: Design the data model for Airbnb.\n\nRequirements:\n  - Hosts list properties (each has multiple rooms/beds, photos, amenities, price rules)\n  - Guests search for properties (by location, dates, price, amenities)\n  - Guests book properties (one booking per property per date range)\n  - Both hosts and guests can leave reviews\n  - Analytics team needs: revenue by city and month, occupancy rates, top-rated hosts\n\nYou will be scored on:\n  ‚ñ° Did you ask clarifying questions before designing?\n  ‚ñ° Did you identify the grain of each table before writing DDL?\n  ‚ñ° Did you choose SQL vs NoSQL and justify it?\n  ‚ñ° Did you handle: property price changes over time (SCD!), soft deletes, review integrity\n  ‚ñ° Did you design for the analytics queries (star schema for DW layer)?\n  ‚ñ° Did you state which indexes are needed and why?\n  ‚ñ° Did you mention at least one edge case unprompted?</pre><h4>Self-Evaluation Rubric</h4><pre>5/7 checklist items = Strong Pass ‚úÖ\n3-4/7 = Pass with coaching\n<3/7 = Revisit weakest area before interviewing\n\nMost common gaps:\n  - Forgetting to state the grain before writing DDL\n  - Not mentioning indexes (always mention them)\n  - Not asking clarifying questions at the start\n  - Picking SQL without considering access pattern volume</pre></div></div></div>',
        'key_concepts': ['Week 3 topics: dimensional modeling, SCDs, normalization, NoSQL, schema design interviews.', 'Most confused: star vs snowflake, SCD Type 1 vs 2, 2NF vs 3NF, Cassandra partition vs clustering key.', 'Active recall is 3x more effective than re-reading ‚Äî test yourself before reviewing notes.'],
        'hints': ["Mock interview with chatbot: 'Design Airbnb's schema. Ask me clarifying questions then build it step by step.'", 'Write CREATE TABLE DDL on paper without autocomplete ‚Äî builds syntax recall for actual interviews.', "Any recall gap: go directly back to that specific day's Level 1 content and re-read the mental model section."],
        'tasks': ['<strong>Active Recall:</strong> From memory, draw a star schema for a ride-sharing app. Name all facts, all dimensions, and state the grain.', '<strong>SCD Practice:</strong> Write the SCD Type 2 UPDATE + INSERT pattern from memory. Include all 6 required columns.', '<strong>Normalization:</strong> Given a 10-column flat table, identify violations and normalize to 3NF. Time yourself: 15 minutes.', '<strong>Mock Interview:</strong> 45-minute Airbnb schema design. Record yourself speaking through the design.'],
        'hard_problem': "Connect-the-dots: A senior data engineer says: 'We have a 5TB PostgreSQL OLTP database (fully normalized, 3NF). Analysts are complaining that their revenue-by-region-by-month queries take 40 minutes. The database is also getting slower for application writes because of too many indexes.' Walk through the complete architectural solution: (1) what data warehouse design would you build? (2) what ETL would populate it? (3) how would you handle slowly changing dimensions in the pipeline? (4) which NoSQL component (if any) would you add and for what purpose?",
    },

}
