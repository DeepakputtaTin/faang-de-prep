[
    {
        "Week": 1,
        "Day": "Monday",
        "Phase": "1. Filter (Coding)",
        "Theme": "SQL Analytics",
        "SpecificTopic": "Window Function Basics",
        "ActionItem_Deliverable": "Solve LeetCode 178 (Rank Scores) using DENSE_RANK",
        "LeetCodeProblem": "<strong>LC 178 \u2013 Rank Scores</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": "Done",
        "Notes": null,
        "Warmup": "No previous topics yet \u2014 focus on today's core concepts!",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 The Problem Window Functions Solve</div>\n<div class=\"rich\">\n<h4>Start Here \u2014 Before Any Code</h4>\n<p>You're a data analyst at Spotify. Your manager asks: <em>\"Show me each employee's salary AND the average salary for their department \u2014 on the same row.\"</em></p>\n<p>Your first instinct: <code>GROUP BY department</code>. Problem: <strong>GROUP BY collapses rows</strong>. You get one row per department, losing individual employee detail. You can't have both on the same row \u2014 unless you use a slow self-join. This is exactly the gap <strong>Window Functions</strong> fill: they compute aggregates across related rows while <strong>keeping every row in the output</strong>.</p>\n<h4>The Mental Model: A Sliding Frame of Glass</h4>\n<p>Imagine placing a sheet of glass over a spreadsheet. For each row, the database looks through that glass at a \"window\" of rows, performs a calculation, and writes the result back into a new column \u2014 without removing the original row.</p>\n<pre>GROUP BY result:              Window Function result:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 dept        \u2502 avg_sal \u2502    \u2502 name  \u2502 dept        \u2502 salary \u2502 dept_avg \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Engineering \u2502  90,000 \u2502    \u2502 Alice \u2502 Engineering \u2502 95,000 \u2502  90,000  \u2502\n\u2502 Marketing   \u2502  72,500 \u2502    \u2502 Bob   \u2502 Engineering \u2502 85,000 \u2502  90,000  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 Eve   \u2502 Engineering \u2502 90,000 \u2502  90,000  \u2502\n2 rows \u2014 detail lost!         \u2502 Carol \u2502 Marketing   \u2502 70,000 \u2502  72,500  \u2502\n                              \u2502 Dave  \u2502 Marketing   \u2502 75,000 \u2502  72,500  \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              5 rows \u2014 ALL detail preserved \u2705</pre>\n<p>\u270d\ufe0f <strong>Write this down:</strong> Window functions NEVER reduce row count. They only ADD new computed columns. The <code>OVER()</code> keyword is the signal that tells the database \"this is a window function.\" Without OVER(), AVG() collapses rows. With OVER(), it keeps all rows.</p>\n<h4>The Syntax Structure</h4>\n<pre>FUNCTION_NAME() OVER (\n    PARTITION BY column   -- which rows form the window for each row\n    ORDER BY column       -- sort order within the window\n    ROWS BETWEEN ...      -- optional: how many surrounding rows to include\n)</pre>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 ROW_NUMBER \u2014 Step by Step with Full Explanation</div>\n<div class=\"rich\">\n<h4>What ROW_NUMBER() Does</h4>\n<p><code>ROW_NUMBER()</code> assigns a unique sequential integer to each row within a window, sorted as you specify. It ALWAYS gives unique numbers \u2014 even if two rows are completely identical. This makes it perfect for deduplication: you can always pick row #1 per group and discard the rest.</p>\n<h4>Step 1 \u2014 Create and populate the table</h4>\n<pre>CREATE TABLE employees (\n  emp_id   INT,\n  emp_name VARCHAR(50),\n  dept     VARCHAR(50),\n  salary   INT\n);\nINSERT INTO employees VALUES\n  (1, 'Alice', 'Engineering', 95000),\n  (2, 'Bob',   'Engineering', 85000),\n  (3, 'Carol', 'Marketing',   70000),\n  (4, 'Dave',  'Marketing',   75000),\n  (5, 'Eve',   'Engineering', 90000);</pre>\n<h4>Step 2 \u2014 Run your first window function</h4>\n<pre>SELECT\n  emp_name, dept, salary,\n  ROW_NUMBER() OVER (\n    PARTITION BY dept      -- restart numbering for each department\n    ORDER BY salary DESC   -- highest salary = rank 1\n  ) AS rank_in_dept\nFROM employees;</pre>\n<h4>Step 3 \u2014 Trace the output and understand WHY</h4>\n<pre>emp_name \u2502 dept        \u2502 salary \u2502 rank_in_dept\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nAlice    \u2502 Engineering \u2502  95000 \u2502      1    \u2190 highest in Eng \u2192 rank 1\nEve      \u2502 Engineering \u2502  90000 \u2502      2    \u2190 2nd in Eng\nBob      \u2502 Engineering \u2502  85000 \u2502      3    \u2190 3rd in Eng\nDave     \u2502 Marketing   \u2502  75000 \u2502      1    \u2190 RESTARTS! Highest in Marketing\nCarol    \u2502 Marketing   \u2502  70000 \u2502      2    \u2190 2nd in Marketing</pre>\n<p>Dave gets rank 1 even though he earns less than Bob \u2014 because PARTITION BY dept created a completely separate window for Marketing. Dave is simply the top earner within his own department's window.</p>\n<h4>What the Engine Does Internally</h4>\n<ol>\n  <li>Read all 5 rows from the table</li>\n  <li>Split into partitions: Engineering (3 rows), Marketing (2 rows)</li>\n  <li>Sort each partition by salary DESC</li>\n  <li>Assign row numbers 1,2,3... within each sorted partition</li>\n  <li>Attach numbers back to original rows and return all 5 rows</li>\n</ol>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 RANK vs DENSE_RANK \u2014 Why Ties Change Everything</div>\n<div class=\"rich\">\n<h4>The Problem with Ties in Real Data</h4>\n<p>In practice, duplicate values are common: two products with the same rating, two employees with the same salary, two cities with equal population. The three ranking functions handle ties differently, and choosing the wrong one produces wrong business results.</p>\n<pre>-- Add Frank \u2014 same salary as Eve\nINSERT INTO employees VALUES (6, 'Frank', 'Engineering', 90000);\n\nSELECT emp_name, salary,\n  ROW_NUMBER() OVER (ORDER BY salary DESC) AS row_num,\n  RANK()       OVER (ORDER BY salary DESC) AS rnk,\n  DENSE_RANK() OVER (ORDER BY salary DESC) AS dense_rnk\nFROM employees WHERE dept = 'Engineering';</pre>\n<pre>emp_name \u2502 salary \u2502 row_num \u2502 rnk \u2502 dense_rnk\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nAlice    \u2502  95000 \u2502    1    \u2502  1  \u2502     1\nEve      \u2502  90000 \u2502    2    \u2502  2  \u2502     2    \u2190 tied with Frank\nFrank    \u2502  90000 \u2502    3    \u2502  2  \u2502     2    \u2190 tied with Eve\nBob      \u2502  85000 \u2502    4    \u2502  4  \u2502     3    \u2190 RANK skips 3, DENSE_RANK doesn't</pre>\n<p>Bob gets RANK 4 because ranks 1, 2, 2 are taken \u2014 the number 3 is \"skipped\" to reflect that two people occupy the 2nd position. DENSE_RANK gives Bob a 3 with no gap.</p>\n<h4>Choosing the Right Function</h4>\n<table>\n<tr><th>Need exactly one result per group (dedup)?</th><td>ROW_NUMBER()</td></tr>\n<tr><th>Gap after tie is meaningful (sports: no 3rd if two share 2nd)?</th><td>RANK()</td></tr>\n<tr><th>Sequential tiers without gaps (medals, grade bands)?</th><td>DENSE_RANK()</td></tr>\n</table>\n<p>\u270d\ufe0f <strong>Interview trap (LeetCode 185):</strong> \"Top 3 salary values per department\" \u2014 ties must BOTH appear. Use DENSE_RANK not ROW_NUMBER. ROW_NUMBER eliminates tied rows arbitrarily \u2014 wrong answer.</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 FAANG Pattern: Top-N Per Group + Scale Secrets</div>\n<div class=\"rich\">\n<h4>The Most Common FAANG Window Function Pattern</h4>\n<p>Top-N per group appears in virtually every FAANG interview. The key constraint: you CANNOT filter on a window function in the same SELECT \u2014 window functions run in step 5 of SQL's execution order, but WHERE runs in step 2. The solution is always a CTE.</p>\n<pre>-- Top 3 earners per department\nWITH ranked AS (\n  SELECT emp_name, dept, salary,\n    DENSE_RANK() OVER (PARTITION BY dept ORDER BY salary DESC) AS rk\n    -- DENSE_RANK: if 2 people tie for 2nd, both appear in top 3\n  FROM employees\n)\nSELECT dept, emp_name, salary\nFROM ranked\nWHERE rk &lt;= 3          -- NOW we can filter because rk is a real column\nORDER BY dept, rk;</pre>\n<h4>SQL Execution Order \u2014 Memorize This</h4>\n<pre>1. FROM       \u2190 identify tables\n2. WHERE      \u2190 filter rows (window functions do NOT exist here yet!)\n3. GROUP BY   \u2190 aggregate\n4. HAVING     \u2190 filter aggregates\n5. SELECT     \u2190 compute expressions \u2014 window functions are evaluated HERE\n6. ORDER BY   \u2190 sort final result\n7. LIMIT      \u2190 restrict rows\n\nThis is why you need a CTE: the outer WHERE sees the CTE's rk column\nbecause by then, the CTE's step 5 has already been executed.</pre>\n<h4>Scale: Why PARTITION BY Key Choice Matters</h4>\n<p>On billions of rows: good PARTITION BY keys (user_id, device_id) spread data across thousands of nodes \u2014 parallel and fast. Bad keys (is_active: TRUE/FALSE) dump 99% of data on one node \u2014 memory crash. Always ask: \"does my partition key distribute data evenly?\"</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Window functions compute over related rows WITHOUT collapsing output \u2014 unlike GROUP BY.",
            "OVER() transforms any aggregate into a window function. Without OVER: GROUP BY. With OVER: window.",
            "PARTITION BY = independent windows per group. Rankings restart per partition.",
            "ORDER BY inside OVER = sort order within partition. Determines which row gets rank 1.",
            "ROW_NUMBER(): always unique. Use for dedup, pagination, keeping exactly 1 row per group.",
            "RANK(): ties share same number, next is SKIPPED (1,2,2,4). Use for sports/competitions.",
            "DENSE_RANK(): ties share same number, no skip (1,2,2,3). Use for tiered rankings. Use for LeetCode 185.",
            "SQL execution order: FROM\u2192WHERE\u2192GROUP BY\u2192HAVING\u2192SELECT(windows here)\u2192ORDER BY\u2192LIMIT.",
            "Cannot use window function result in WHERE. Always wrap in CTE first."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create employees table and insert 5 rows. Run the ROW_NUMBER query. Explain in writing why Dave gets rank 1.",
            "<strong>Step 2:</strong> Add Frank (Engineering, 90000) and run all 3 ranking functions. Write the difference you see in Bob's row.",
            "<strong>Step 3:</strong> Write the DENSE_RANK top-3-per-department query from scratch without looking. Test it shows both Eve and Frank when tied.",
            "<strong>Step 4:</strong> Try adding WHERE rank_in_dept &lt;= 3 directly in the window query (no CTE). Note the error. Then fix it with a CTE."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Solve LeetCode 178 (Rank Scores) using DENSE_RANK",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Solve LeetCode 178 (Rank Scores) using DENSE_RANK \u2014 trace through 2-3 edge cases before writing any code.",
                "'Per group' or 'within each category' \u2192 your signal to use PARTITION BY.",
                "LeetCode 185 trap: 'top 3 salaries' not 'top 3 employees' \u2192 DENSE_RANK so ties both show.",
                "Can't use window function in WHERE? Wrap in CTE \u2014 this is always the fix.",
                "High-cardinality PARTITION BY (user_id) = good parallelism. Low-cardinality (boolean) = hotspot."
            ]
        },
        "HardProblem": "Boss Problem (Meta): 5-billion row table: user_events(user_id, event_type, revenue, event_ts). (1) Rank each user's events by revenue using DENSE_RANK. (2) Return only top 3 per user. (3) Add running total revenue per user. Explain: Which PARTITION BY key? What happens memory-wise with no PARTITION BY on 5B rows? How would you run this in Spark?"
    },
    {
        "Week": 1,
        "Day": "Tuesday",
        "Phase": "1. Filter (Coding)",
        "Theme": "SQL Analytics",
        "SpecificTopic": "Rolling Windows",
        "ActionItem_Deliverable": "Solve LeetCode 1321 (Restaurant Growth) using ROWS BETWEEN",
        "LeetCodeProblem": "<strong>LC 1321 \u2013 Restaurant Growth</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": "Done",
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Window Function Basics</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 What Rolling Windows Solve and Why</div>\n<div class=\"rich\">\n<h4>The Business Problem</h4>\n<p>You're a data analyst at Netflix. Daily streaming hours spike on Saturdays and drop on Mondays. Your VP doesn't want spiky daily numbers \u2014 they want to see the <em>trend</em>. \"Is engagement growing or shrinking?\" The answer is a <strong>7-day rolling average</strong>: instead of just today's number, average the last 7 days. As each new day arrives, the oldest day drops off \u2014 the window \"rolls\" forward smoothly.</p>\n<pre>Day data:   100   80   120   90   110   95   105\n3-day rolling averages:\n  Day 1: [100]             \u2192 avg = 100.0   (partial window \u2014 only 1 row)\n  Day 2: [100, 80]         \u2192 avg = 90.0    (partial \u2014 only 2 rows)\n  Day 3: [100, 80, 120]    \u2192 avg = 100.0   (full 3-day window)\n  Day 4: [     80, 120, 90]\u2192 avg = 96.7    (100 dropped off!)\n  Day 5: [         120, 90, 110] \u2192 avg = 106.7</pre>\n<p>The first N-1 rows have \"partial windows\" \u2014 there aren't enough prior rows yet. SQL handles this automatically, averaging whatever rows ARE available. This is normal and expected behavior.</p>\n<h4>Where Rolling Windows Appear at FAANG</h4>\n<ul>\n  <li><strong>Facebook:</strong> 28-day rolling DAU/MAU ratio \u2014 their core engagement metric</li>\n  <li><strong>Netflix:</strong> 7-day rolling content hours to smooth weekend effects</li>\n  <li><strong>Uber:</strong> 30-day rolling driver earnings to assess compensation fairness</li>\n  <li><strong>Finance:</strong> 200-day moving average used in stock trading algorithms</li>\n</ul>\n<p>\u270d\ufe0f <strong>Formula to memorize:</strong> For N-day rolling window \u2192 ROWS BETWEEN (N-1) PRECEDING AND CURRENT ROW. 7-day = 6 PRECEDING. 30-day = 29 PRECEDING.</p>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Building Rolling Windows \u2014 Every Clause Explained</div>\n<div class=\"rich\">\n<h4>The ROWS BETWEEN Clause</h4>\n<p>By default, a window function without ROWS BETWEEN includes all rows from partition start to current row. ROWS BETWEEN lets you control exactly which surrounding rows to include. Think of it as telling the DB: \"I'm on row N, look at rows N-6 through N \u2014 those are my 7 rows.\"</p>\n<pre>CREATE TABLE daily_revenue (\n  rev_date DATE,\n  revenue  DECIMAL(10,2)\n);\nINSERT INTO daily_revenue VALUES\n  ('2024-01-01', 100), ('2024-01-02', 150),\n  ('2024-01-03', 120), ('2024-01-04', 200),\n  ('2024-01-05', 180), ('2024-01-06', 250), ('2024-01-07', 300);\n\nSELECT\n  rev_date,\n  revenue,\n  -- Running total: from the very first row to today \u2014 never resets\n  SUM(revenue) OVER (\n    ORDER BY rev_date\n    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n  ) AS running_total,\n  -- 3-day rolling avg: only the 3 most recent rows\n  ROUND(AVG(revenue) OVER (\n    ORDER BY rev_date\n    ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n  ), 2) AS rolling_3d_avg\nFROM daily_revenue;</pre>\n<pre>rev_date   \u2502 revenue \u2502 running_total \u2502 rolling_3d_avg\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n2024-01-01 \u2502   100   \u2502      100      \u2502  100.00  \u2190 partial: [100]\n2024-01-02 \u2502   150   \u2502      250      \u2502  125.00  \u2190 partial: [100,150]\n2024-01-03 \u2502   120   \u2502      370      \u2502  123.33  \u2190 full: [100,150,120]\n2024-01-04 \u2502   200   \u2502      570      \u2502  156.67  \u2190 [150,120,200] \u2014 100 dropped!\n2024-01-05 \u2502   180   \u2502      750      \u2502  166.67  \u2190 [120,200,180]</pre>\n<p>The running_total grows forever (never resets \u2014 UNBOUNDED PRECEDING brings all history). The rolling_3d_avg only ever sees 3 rows and forgets older data.</p>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 ROWS vs RANGE \u2014 The Hidden Trap</div>\n<div class=\"rich\">\n<h4>Two Frame Types That Look Identical But Aren't</h4>\n<p><strong>ROWS BETWEEN:</strong> counts physical rows. \"2 PRECEDING\" = exactly 2 rows above in sorted order, regardless of their values. Always predictable.</p>\n<p><strong>RANGE BETWEEN:</strong> counts by value. All rows with the SAME ORDER BY value as the current row are treated as peers and included in the frame. This can include far more rows than you expect.</p>\n<pre>-- If you have two rows both with rev_date='2024-01-03':\nROWS BETWEEN 1 PRECEDING AND CURRENT ROW\n  \u2192 exactly 2 rows always (1 before + current row)\n\nRANGE BETWEEN 1 PRECEDING AND CURRENT ROW\n  \u2192 includes BOTH Jan-3 rows in each Jan-3 row's frame\n  \u2192 result: BOTH rows show the same running total value\n  \u2192 can be very confusing!</pre>\n<p><strong>Default:</strong> Writing just <code>OVER (ORDER BY col)</code> uses RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW. This means duplicate ORDER BY values all show the same cumulative total \u2014 sometimes desired, often surprising. Use ROWS BETWEEN explicitly for predictable behavior.</p>\n<h4>Frame Clause Reference</h4>\n<table>\n<tr><th>What you want</th><th>Frame clause</th></tr>\n<tr><td>Running total</td><td>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</td></tr>\n<tr><td>7-day rolling avg</td><td>ROWS BETWEEN 6 PRECEDING AND CURRENT ROW</td></tr>\n<tr><td>Centered avg (before+after)</td><td>ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING</td></tr>\n<tr><td>% of grand total</td><td>ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING</td></tr>\n</table>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 FAANG: Anomaly Detection with Z-Score Rolling Baseline</div>\n<div class=\"rich\">\n<h4>How Netflix Detects Pipeline Failures Automatically</h4>\n<p>In production, rolling windows power anomaly detection: when a daily metric drops more than 2 standard deviations below its prior 30-day average, alert the on-call engineer. This runs automatically via scheduled SQL or Spark jobs \u2014 no human manually reviews every metric.</p>\n<p>The Z-score formula: <code>(today - rolling_avg) / rolling_stddev</code>. Values outside \u00b12 are flagged. We use ROWS BETWEEN 29 PRECEDING AND <strong>1 PRECEDING</strong> (not CURRENT ROW) to create a baseline from <em>before today</em> \u2014 otherwise today's anomaly would bias its own baseline.</p>\n<pre>WITH rolling_stats AS (\n  SELECT rev_date, revenue,\n    AVG(revenue)    OVER (ORDER BY rev_date ROWS BETWEEN 29 PRECEDING AND 1 PRECEDING) AS avg_30d,\n    STDDEV(revenue) OVER (ORDER BY rev_date ROWS BETWEEN 29 PRECEDING AND 1 PRECEDING) AS std_30d\n  FROM daily_revenue\n)\nSELECT rev_date, revenue,\n  ROUND((revenue - avg_30d) / NULLIF(std_30d, 0), 2) AS z_score,\n  CASE\n    WHEN (revenue - avg_30d) / NULLIF(std_30d, 0) &lt; -2 THEN '\ud83d\udd34 ANOMALY'\n    WHEN (revenue - avg_30d) / NULLIF(std_30d, 0) &gt;  2 THEN '\ud83d\udfe1 SPIKE'\n    ELSE '\u2705 Normal'\n  END AS status\nFROM rolling_stats\nWHERE avg_30d IS NOT NULL  -- skip first 30 rows: no baseline yet\nORDER BY rev_date;</pre>\n<p><code>NULLIF(std_30d, 0)</code> prevents division by zero when all 30 days had identical revenue \u2014 returns NULL instead of crashing. Always include this when dividing by a window aggregate.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Rolling window = sliding subset of rows that moves with each row. Window slides forward, oldest row drops off.",
            "N-day rolling window \u2192 ROWS BETWEEN (N-1) PRECEDING AND CURRENT ROW.",
            "ROWS counts physical rows (predictable). RANGE counts by value (can include many more rows with duplicate values).",
            "UNBOUNDED PRECEDING = from partition start. Use for running totals that never reset.",
            "Partial windows: first N-1 rows have fewer than N rows available. SQL averages what exists \u2014 this is correct behavior.",
            "NULLIF(denominator, 0): prevents division by zero in Z-score and percentage calculations.",
            "Anomaly baseline: use 1 PRECEDING not CURRENT ROW to exclude today from its own baseline."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create daily_revenue table with 7 rows. Run Level 2 query. Manually verify Jan 4 rolling_3d_avg = (150+120+200)/3 = 156.67.",
            "<strong>Step 2:</strong> Change to 5-day rolling. How many rows still have partial windows? What does Jan 2 become?",
            "<strong>Step 3:</strong> Write a query showing each day's revenue as % of the grand total. What frame gives you the grand total in the denominator?",
            "<strong>Step 4 (write from memory):</strong> Without looking, write the Z-score anomaly detection query. Test: insert an extreme value (revenue=10000 on Jan 8) and verify it gets flagged."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Solve LeetCode 1321 (Restaurant Growth) using ROWS BETWEEN",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Solve LeetCode 1321 (Restaurant Growth) using ROWS BETWEEN \u2014 trace through 2-3 edge cases before writing any code.",
                "7-day rolling: ROWS BETWEEN 6 PRECEDING AND CURRENT ROW. Always N-1 PRECEDING for N-day.",
                "Use ROWS not RANGE for moving averages \u2014 RANGE surprises you with duplicate ORDER BY values.",
                "Rolling % of total: OVER(ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) gives grand total.",
                "In anomaly detection, always exclude today from the baseline window (1 PRECEDING not CURRENT ROW)."
            ]
        },
        "HardProblem": "Boss Problem (Airbnb): Table booking_events(booking_date, property_id, revenue). For each property show: daily revenue, 30-day rolling avg, % change vs same day last week (LAG), flag 'DECLINING' if 7-day rolling avg dropped >20% vs prior 7-day period. Edge case: some properties have zero bookings on certain dates \u2014 how do you fill the missing dates so your 7-day window doesn't skip them?"
    },
    {
        "Week": 1,
        "Day": "Wednesday",
        "Phase": "1. Filter (Coding)",
        "Theme": "SQL Analytics",
        "SpecificTopic": "Lead/Lag Pattern",
        "ActionItem_Deliverable": "Solve LeetCode 180 (Consecutive Numbers)",
        "LeetCodeProblem": "<strong>LC 180 \u2013 Consecutive Numbers</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": "Done",
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Window Function Basics</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Comparing Adjacent Rows \u2014 The Problem</div>\n<div class=\"rich\">\n<h4>The Problem: Row-to-Row Comparison</h4>\n<p>You have a table of daily stock prices. Your manager asks: <em>\"How much did Apple's stock move from yesterday to today?\"</em> Each row only knows its own date and price \u2014 it doesn't automatically \"see\" the previous row.</p>\n<p>The naive solution is a self-join: join the table to itself where today's date equals yesterday's date plus one day. But self-joins on large tables are slow, complex to write, and break when dates have gaps (weekends, holidays). <code>LAG</code> and <code>LEAD</code> solve this cleanly in one pass.</p>\n<ul>\n  <li><strong>LAG(col, n, default)</strong> \u2014 looks BACKWARDS n rows from the current row. Returns the value of col from n rows ago, or the default if no such row exists.</li>\n  <li><strong>LEAD(col, n, default)</strong> \u2014 looks FORWARDS n rows. Returns the value from n rows ahead.</li>\n</ul>\n<pre>Sorted by date:\n  Row 1 (Jan 1): price=185  \u2192 LAG returns NULL (no prior row), LEAD returns 188.50\n  Row 2 (Jan 2): price=188.50 \u2192 LAG returns 185, LEAD returns 182\n  Row 3 (Jan 3): price=182  \u2192 LAG returns 188.50, LEAD returns 191\n  Row 4 (Jan 4): price=191  \u2192 LAG returns 182, LEAD returns NULL (no next row)</pre>\n<p>\u270d\ufe0f LAG requires ORDER BY in the OVER clause \u2014 without it, \"previous\" has no meaning. Always include PARTITION BY when you want the lookback to reset at group boundaries (e.g., per stock symbol).</p>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 LAG in Action \u2014 Day-over-Day Price Change</div>\n<div class=\"rich\">\n<pre>CREATE TABLE stock_prices (\n  trade_date  DATE,\n  symbol      VARCHAR(10),\n  close_price DECIMAL(10,2)\n);\nINSERT INTO stock_prices VALUES\n  ('2024-01-01', 'AAPL', 185.00),\n  ('2024-01-02', 'AAPL', 188.50),\n  ('2024-01-03', 'AAPL', 182.00),\n  ('2024-01-04', 'AAPL', 191.00),\n  ('2024-01-05', 'AAPL', 189.00);\n\nSELECT\n  trade_date, close_price,\n  -- LAG(column, offset, default_if_null)\n  LAG(close_price, 1, 0) OVER (\n    PARTITION BY symbol   -- reset for each stock symbol\n    ORDER BY trade_date   -- \"previous\" means previous trading day\n  ) AS prev_close,\n  -- Dollar change: today minus yesterday\n  close_price - LAG(close_price) OVER (PARTITION BY symbol ORDER BY trade_date) AS dollar_change,\n  -- % change: (today-yesterday)/yesterday * 100\n  ROUND(100.0 * (close_price - LAG(close_price) OVER (PARTITION BY symbol ORDER BY trade_date))\n    / NULLIF(LAG(close_price) OVER (PARTITION BY symbol ORDER BY trade_date), 0), 2) AS pct_change\nFROM stock_prices;</pre>\n<pre>trade_date \u2502 close \u2502 prev_close \u2502 dollar_change \u2502 pct_change\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n2024-01-01 \u2502 185.0 \u2502      0.00  \u2502     NULL      \u2502   NULL    \u2190 no prior row\n2024-01-02 \u2502 188.5 \u2502    185.00  \u2502     3.50      \u2502   1.89\n2024-01-03 \u2502 182.0 \u2502    188.50  \u2502    -6.50      \u2502  -3.44   \u2190 price dropped\n2024-01-04 \u2502 191.0 \u2502    182.00  \u2502     9.00      \u2502   4.95\n2024-01-05 \u2502 189.0 \u2502    191.00  \u2502    -2.00      \u2502  -1.05</pre>\n<p>The third argument to LAG (the 0 in <code>LAG(close_price, 1, 0)</code>) is the default value returned when there is no prior row (the first row). Without it, you get NULL, which can crash downstream percentage calculations.</p>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Session Detection with LAG \u2014 Uber Pattern</div>\n<div class=\"rich\">\n<h4>Finding Time Gaps Between Events</h4>\n<p>A \"session\" in product analytics is a group of consecutive events with no long gap between them. If a user makes 5 clicks within 10 minutes, that's one session. If they then come back 2 hours later, that's a new session. LAG lets you detect the boundary: wherever the time gap between two consecutive events exceeds the threshold, a new session starts.</p>\n<pre>-- Step 1: compute the gap between each event and the one before it\nWITH event_gaps AS (\n  SELECT\n    user_id, event_type, event_ts,\n    LAG(event_ts) OVER (PARTITION BY user_id ORDER BY event_ts) AS prev_ts,\n    TIMESTAMPDIFF(MINUTE,\n      LAG(event_ts) OVER (PARTITION BY user_id ORDER BY event_ts),\n      event_ts\n    ) AS gap_minutes\n  FROM user_events\n),\n-- Step 2: mark each row as a new-session-start if gap > 30 min or it's the first event\nsession_markers AS (\n  SELECT *,\n    SUM(CASE WHEN gap_minutes > 30 OR gap_minutes IS NULL THEN 1 ELSE 0 END)\n      OVER (PARTITION BY user_id ORDER BY event_ts) AS session_id\n    -- SUM of 1s: every time a new session starts, the running count increments\n    -- This gives a unique session_id per session per user\n  FROM event_gaps\n)\n-- Step 3: aggregate each session\nSELECT\n  user_id, session_id,\n  MIN(event_ts) AS session_start,\n  MAX(event_ts) AS session_end,\n  COUNT(*)      AS events_in_session,\n  TIMESTAMPDIFF(MINUTE, MIN(event_ts), MAX(event_ts)) AS session_duration_mins\nFROM session_markers\nGROUP BY user_id, session_id\nORDER BY user_id, session_start;</pre>\n<p>The key insight: <code>SUM(CASE WHEN new_session THEN 1 ELSE 0 END) OVER (ORDER BY ts)</code> creates a running count of how many sessions have started so far \u2014 which becomes a unique session number per user.</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Month-over-Month and Year-over-Year with LAG</div>\n<div class=\"rich\">\n<h4>The Most Common Reporting Pattern in Business Intelligence</h4>\n<p>Every business dashboard needs month-over-month and year-over-year comparisons. LAG with offset=1 gives you last month; LAG with offset=12 gives you the same month last year. The key trick: pre-aggregate to monthly first (GROUP BY), then apply LAG in a CTE or subquery on the monthly totals.</p>\n<pre>WITH monthly_revenue AS (\n  SELECT\n    DATE_TRUNC('month', order_date) AS month,\n    SUM(revenue) AS monthly_rev\n  FROM orders\n  GROUP BY 1\n),\ncomparisons AS (\n  SELECT\n    month, monthly_rev,\n    LAG(monthly_rev, 1) OVER (ORDER BY month) AS prev_month,\n    LAG(monthly_rev, 12) OVER (ORDER BY month) AS same_month_last_year\n  FROM monthly_revenue\n)\nSELECT\n  month, monthly_rev,\n  -- MoM change: (this month - last month) / last month * 100\n  ROUND(100.0 * (monthly_rev - prev_month) / NULLIF(prev_month, 0), 1) AS mom_pct,\n  -- YoY change: (this month - same month last year) / same month last year * 100\n  ROUND(100.0 * (monthly_rev - same_month_last_year) / NULLIF(same_month_last_year, 0), 1) AS yoy_pct\nFROM comparisons\nORDER BY month;</pre>\n<p>Always wrap the LAG value in NULLIF(value, 0) before dividing \u2014 if last month's revenue was zero (new product, seasonal lull), division by zero crashes the query or produces infinity.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "LAG(col, n, default): look back n rows. Returns default (not NULL) when no prior row exists.",
            "LEAD(col, n, default): look forward n rows. Returns default when no next row exists.",
            "Both require ORDER BY inside OVER \u2014 without it, 'previous' and 'next' are undefined.",
            "PARTITION BY resets LAG/LEAD boundaries per group (e.g., per stock symbol, per user).",
            "LAG(col, 12) = same value 12 rows back = same month last year (when data is monthly).",
            "Session detection: SUM of new-session flags OVER (ORDER BY time) creates unique session IDs.",
            "NULLIF(denominator, 0): always use when dividing by a LAG value to prevent division-by-zero."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create stock_prices and run the Level 2 query. Verify Jan 3 shows dollar_change = -6.50.",
            "<strong>Step 2:</strong> Add the pct_change column. What happens to Jan 1 pct_change without NULLIF? Add NULLIF and confirm it becomes NULL instead of an error.",
            "<strong>Step 3:</strong> Use LEAD to add a 'tomorrow_price' column. For the last row (Jan 5), what does LEAD return?",
            "<strong>Step 4:</strong> Identify all consecutive 2-day price increases using LAG twice: WHERE today > yesterday AND yesterday > day-before-yesterday."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Solve LeetCode 180 (Consecutive Numbers)",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Solve LeetCode 180 (Consecutive Numbers) \u2014 trace through 2-3 edge cases before writing any code.",
                "First row returns NULL from LAG with no default \u2014 this crashes % change calculations. Always add default 0.",
                "% change formula: (new - old) / NULLIF(old, 0) * 100. Always NULLIF the denominator.",
                "Session detection: gap > threshold OR gap IS NULL (IS NULL = the first event per user, always a new session).",
                "LeetCode 180 'Consecutive Numbers': uses LAG twice to look at previous two rows. Practice this."
            ]
        },
        "HardProblem": "Boss Problem (LinkedIn): Table post_views(post_id, view_date, view_count). Write a query: (1) daily view count, (2) 7-day % change using LAG(7), (3) classify each row as 'VIRAL' (>20% gain), 'DECLINING' (>20% loss), or 'STABLE'. Optimize for 1 billion rows: what index would you add? What partition strategy in Spark?"
    },
    {
        "Week": 1,
        "Day": "Thursday",
        "Phase": "1. Filter (Coding)",
        "Theme": "SQL Analytics",
        "SpecificTopic": "Gaps & Islands (Logic)",
        "ActionItem_Deliverable": "Write query to find users with 3+ consecutive login days",
        "LeetCodeProblem": "<strong>LC 601 \u2013 Human Traffic (preview)</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": "Done",
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Lead/Lag Pattern</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Finding Breaks in Sequences \u2014 The Intuition</div>\n<div class=\"rich\">\n<h4>The Problem: SQL Doesn't Understand 'Consecutive'</h4>\n<p>You're a data engineer at Duolingo. You have a table of dates when users studied. The product team wants to know: \"What are each user's study streaks \u2014 continuous blocks of consecutive study days?\" A streak is a run of consecutive days with no gap.</p>\n<pre>User 42's study dates:\nJan 1, Jan 2, Jan 3, [gap: Jan 4 missing], Jan 5, Jan 6, [gap], Jan 10\n                                                                      \u2191\nIslands (continuous streaks):    Gaps (missing days):\n  Island 1: Jan 1 \u2192 Jan 3         Gap 1: Jan 4\n  Island 2: Jan 5 \u2192 Jan 6         Gap 2: Jan 7, 8, 9\n  Island 3: Jan 10</pre>\n<p>SQL can sort rows and count them, but it has no built-in concept of \"these rows are consecutive.\" We need a trick to group consecutive dates together. This is the <strong>Gaps & Islands</strong> problem \u2014 one of the most elegant SQL puzzles and heavily tested at FAANG.</p>\n<h4>The Core Insight \u2014 date minus row_number \u270d\ufe0f</h4>\n<p>For consecutive dates, if you subtract the row's sequential number from the date itself, you get the SAME constant value for every date in the same island. When a gap occurs, the constant changes. This is the key: use that constant as a GROUP BY key to aggregate each island.</p>\n<pre>date    rn    date - rn\nJan 1    1    Dec 31    \u2190\u2510 same value = same island\nJan 2    2    Dec 31    \u2190\u2518\nJan 3    3    Dec 31    \u2190\u2518 all three in Island 1\nJan 5    4    Jan 1     \u2190 DIFFERENT value = new island!\nJan 6    5    Jan 1     \u2190\u2500 Island 2\nJan 10   6    Jan 4     \u2190 another new island</pre>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Step-by-Step Implementation</div>\n<div class=\"rich\">\n<pre>CREATE TABLE user_logins (user_id INT, login_date DATE);\nINSERT INTO user_logins VALUES\n  (1,'2024-01-01'),(1,'2024-01-02'),(1,'2024-01-03'),\n  -- gap: Jan 4 missing --\n  (1,'2024-01-05'),(1,'2024-01-06'),\n  -- gap: Jan 7,8,9 --\n  (1,'2024-01-10');</pre>\n\n<h4>Step 1 \u2014 Assign row numbers and compute the group key</h4>\n<pre>SELECT\n  user_id, login_date,\n  ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY login_date) AS rn,\n  login_date - INTERVAL (\n    ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY login_date)\n  ) DAY AS group_key   -- this is constant for consecutive dates!\nFROM user_logins;</pre>\n<pre>Output:\nuser_id \u2502 login_date  \u2502 rn \u2502 group_key\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   1    \u2502 2024-01-01  \u2502  1 \u2502 2023-12-31  \u2190 Island 1\n   1    \u2502 2024-01-02  \u2502  2 \u2502 2023-12-31  \u2190 same!\n   1    \u2502 2024-01-03  \u2502  3 \u2502 2023-12-31  \u2190 same!\n   1    \u2502 2024-01-05  \u2502  4 \u2502 2024-01-01  \u2190 Island 2\n   1    \u2502 2024-01-06  \u2502  5 \u2502 2024-01-01  \u2190 same!\n   1    \u2502 2024-01-10  \u2502  6 \u2502 2024-01-04  \u2190 Island 3</pre>\n\n<h4>Step 2 \u2014 GROUP BY the group key to get each island's bounds</h4>\n<pre>WITH numbered AS (\n  SELECT user_id, login_date,\n    login_date - INTERVAL (ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY login_date)) DAY AS grp\n  FROM user_logins\n)\nSELECT\n  user_id,\n  MIN(login_date) AS streak_start,\n  MAX(login_date) AS streak_end,\n  COUNT(*)        AS streak_days\nFROM numbered\nGROUP BY user_id, grp\nORDER BY user_id, streak_start;</pre>\n<pre>user_id \u2502 streak_start \u2502 streak_end  \u2502 streak_days\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   1    \u2502 2024-01-01   \u2502 2024-01-03  \u2502      3\n   1    \u2502 2024-01-05   \u2502 2024-01-06  \u2502      2\n   1    \u2502 2024-01-10   \u2502 2024-01-10  \u2502      1</pre>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Finding the Gaps (Missing Dates)</div>\n<div class=\"rich\">\n<h4>From Islands to Gaps \u2014 What's Missing?</h4>\n<p>Once you have island boundaries (start and end of each streak), finding the gaps is straightforward: a gap starts the day AFTER an island ends and finishes the day BEFORE the next island starts. Use LEAD to look at the next island's start.</p>\n<pre>WITH numbered AS (\n  SELECT user_id, login_date,\n    login_date - INTERVAL (ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY login_date)) DAY AS grp\n  FROM user_logins\n),\nislands AS (\n  SELECT user_id,\n    MIN(login_date) AS island_start,\n    MAX(login_date) AS island_end\n  FROM numbered\n  GROUP BY user_id, grp\n),\ngaps AS (\n  SELECT\n    user_id,\n    island_end + INTERVAL 1 DAY AS gap_start,\n    LEAD(island_start) OVER (PARTITION BY user_id ORDER BY island_start)\n      - INTERVAL 1 DAY AS gap_end,\n    DATEDIFF(\n      LEAD(island_start) OVER (PARTITION BY user_id ORDER BY island_start),\n      island_end\n    ) - 1 AS gap_days\n  FROM islands\n)\nSELECT * FROM gaps WHERE gap_days > 0;</pre>\n<pre>user_id \u2502 gap_start   \u2502 gap_end     \u2502 gap_days\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   1    \u2502 2024-01-04  \u2502 2024-01-04  \u2502    1     \u2190 Jan 4 missing\n   1    \u2502 2024-01-07  \u2502 2024-01-09  \u2502    3     \u2190 Jan 7,8,9 missing</pre>\n<p>FAANG uses this exact pattern for: billing gaps (subscription payment missing), SLA violations (service down for N hours), outage windows (server status logs), and content recommendation dead zones (user didn't watch for N days).</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Session Detection and Advanced Variants</div>\n<div class=\"rich\">\n<h4>Gaps & Islands with Timestamps (Meta Interview Question)</h4>\n<p>The same pattern works for timestamps. Define a \"session\" as: consecutive events where no gap exceeds 30 minutes. Instead of subtracting a row number from a date, you use a different grouping technique: a running SUM of \"did this event start a new session?\" flags.</p>\n<pre>WITH diffs AS (\n  SELECT *,\n    TIMESTAMPDIFF(MINUTE,\n      LAG(event_ts) OVER (PARTITION BY user_id ORDER BY event_ts),\n      event_ts\n    ) AS gap_minutes\n  FROM user_events\n),\nsessions AS (\n  SELECT *,\n    -- Every time gap > 30 OR no prior event (NULL), increment session counter\n    SUM(CASE WHEN gap_minutes > 30 OR gap_minutes IS NULL THEN 1 ELSE 0 END)\n      OVER (PARTITION BY user_id ORDER BY event_ts) AS session_id\n  FROM diffs\n)\nSELECT user_id, session_id,\n  MIN(event_ts) AS session_start,\n  MAX(event_ts) AS session_end,\n  COUNT(*)      AS event_count\nFROM sessions\nGROUP BY user_id, session_id\nORDER BY user_id, session_start;</pre>\n<p>Why does the SUM trick work? Every time a new session starts, we add 1 to the running sum. The running sum at any point = the total number of sessions started so far = a unique session ID. On average, this pattern appears in every 3rd FAANG product analytics interview.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Gaps & Islands: group consecutive sequences (dates, IDs, timestamps) into islands (runs without gaps).",
            "Core trick: for consecutive dates, (date - ROW_NUMBER()) produces the SAME constant value per island.",
            "The constant changes at each gap \u2014 use it as a GROUP BY key to aggregate each island separately.",
            "Finding gaps: use LEAD to get next island's start, compute: gap_start = island_end+1, gap_end = next_start-1.",
            "Session detection: SUM(CASE WHEN new_session_start THEN 1 ELSE 0 END) OVER (ORDER BY ts) = session ID.",
            "The date arithmetic: in MySQL use DATE_ADD(date, INTERVAL -rn DAY). In Postgres use date - rn::integer.",
            "Always deduplicate (DISTINCT) before applying gaps & islands \u2014 duplicate dates break the row number math."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create the user_logins table with 3 islands (Jan 1-3, Jan 5-6, Jan 10). Print the intermediate table showing date - rn. Verify same-island rows share the same group_key.",
            "<strong>Step 2:</strong> Run the complete Gaps & Islands query. Verify output: 3 islands with streak_days = 3, 2, 1.",
            "<strong>Step 3:</strong> Run the gap-finding query. Verify it finds Jan 4 (1 day) and Jan 7-9 (3 days).",
            "<strong>Step 4 \u2014 from scratch:</strong> Write the session detection (timestamp version) for a user_events table with 30-minute threshold. Test with events that have a 45-minute gap."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Write query to find users with 3+ consecutive login days",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Write query to find users with 3+ consecutive login days \u2014 trace through 2-3 edge cases before writing any code.",
                "Always print the intermediate table (with date - rn) to verify consecutive dates share the same group_key.",
                "For integers instead of dates: id - ROW_NUMBER() \u2014 same trick, even simpler math.",
                "New session flag: gap IS NULL (first event per user) should ALSO trigger a new session.",
                "Longest streak: after finding all islands, SELECT MAX(streak_days) per user."
            ]
        },
        "HardProblem": "Boss Problem (Duolingo): Table user_sessions(user_id, session_date, minutes_studied). A valid streak day = at least 10 minutes studied. Find: (1) each user's current streak length, (2) their all-time longest streak and its exact start/end dates, (3) users whose streak broke within the last 7 days (to trigger a 'restart your streak' notification). Handle: a user who studied 5 minutes today (breaks streak) but had 30 minutes yesterday."
    },
    {
        "Week": 1,
        "Day": "Friday",
        "Phase": "1. Filter (Coding)",
        "Theme": "SQL Analytics",
        "SpecificTopic": "Gaps & Islands (Complex)",
        "ActionItem_Deliverable": "Solve LeetCode 601 (Human Traffic of Stadium)",
        "LeetCodeProblem": "<strong>LC 601 \u2013 Human Traffic of Stadium</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": "Done",
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Gaps & Islands (Logic)</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\">\n<div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 The Concept</div><div class=\"rich\">\n<h4>What Is Gaps & Islands (Complex)?</h4>\n<p>Data Engineering at FAANG scale requires mastering <strong>Gaps & Islands (Complex)</strong> from first principles. Before writing any code, understand: <em>what problem does this solve? Why does it exist?</em></p>\n<p>Think of it like this: every tool in data engineering exists because someone hit a real-world scaling or correctness problem. Understanding <strong>the problem</strong> first makes the solution obvious.</p>\n<h4>The Core Questions to Answer</h4>\n<ul>\n  <li>What problem does Gaps & Islands (Complex) solve that simpler approaches can't?</li>\n  <li>What are the 2-3 most important properties or guarantees it provides?</li>\n  <li>Where does it fit in a typical DE pipeline?</li>\n</ul>\n</div></div>\n<div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 First Example</div><div class=\"rich\">\n<h4>Hello World: Gaps & Islands (Complex)</h4>\n<p>Start with the simplest possible working example. No optimization yet \u2014 just get it working and understand the output.</p>\n<pre># Start here: implement the most basic version of Gaps & Islands (Complex)\n# Then run it, verify the output, and understand each line.\n# Only then move to Level 3 complexity.</pre>\n<h4>Key Syntax to Know</h4>\n<ul>\n  <li>The most common command or pattern for Gaps & Islands (Complex)</li>\n  <li>The most important configuration parameter</li>\n  <li>How to verify it's working correctly</li>\n</ul>\n</div></div>\n<div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Real-World Application</div><div class=\"rich\">\n<h4>Production-Grade Gaps & Islands (Complex)</h4>\n<p>At scale, Gaps & Islands (Complex) requires understanding tradeoffs: what breaks at 10\u00d7 load? What's the failure mode?</p>\n<h4>Critical Tradeoffs</h4>\n<ul>\n  <li><strong>Speed vs Correctness:</strong> Can you sacrifice some accuracy for throughput?</li>\n  <li><strong>Memory vs CPU:</strong> What's the bottleneck at 100M rows?</li>\n  <li><strong>Complexity vs Simplicity:</strong> Is the advanced approach worth it at your scale?</li>\n</ul>\n<pre># Real-world pattern:\n# Step 1: Validate input data quality\n# Step 2: Process in batches (not all at once \u2192 OOM risk)\n# Step 3: Handle failures gracefully (retry, DLQ)\n# Step 4: Emit metrics for observability</pre>\n</div></div>\n<div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 FAANG Scale</div><div class=\"rich\">\n<h4>FAANG Production Patterns for Gaps & Islands (Complex)</h4>\n<ul>\n  <li><strong>Scale:</strong> What breaks at 1 billion rows? What does your solution handle at that scale?</li>\n  <li><strong>Reliability:</strong> Is your implementation idempotent? What's your retry strategy?</li>\n  <li><strong>Observability:</strong> Structured logs, row-count metrics, p99 latency SLO, error rate alerts</li>\n  <li><strong>Cost:</strong> Estimate storage and compute cost \u2014 FAANG engineers always think about $$$</li>\n</ul>\n<pre># FAANG interview answer template:\n# \"My approach for Gaps & Islands (Complex):\n# 1. For correctness: [describe your idempotency/consistency strategy]\n# 2. For scale: [describe distributed/partitioned approach]\n# 3. For reliability: [describe retry logic and failure handling]\n# 4. For monitoring: [describe metrics and alerting]\"\n</pre>\n</div></div>\n</div>",
        "KeyConcepts": [
            "Gaps & Islands (Complex): understand the core abstraction before memorizing syntax.",
            "Scalability: what breaks at 10\u00d7 load \u2014 CPU, memory, network, or disk?",
            "Idempotency: a pipeline run twice must produce the same result as run once.",
            "Observability: metrics, structured logs, and SLOs before shipping to production.",
            "Tradeoffs: every design choice sacrifices something \u2014 name both sides explicitly.",
            "Failure recovery: retries, DLQs, circuit breakers, and partial failure handling.",
            "Cost: estimate storage and compute cost for the expected data volume."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Define the problem in one sentence and identify the primary constraint (throughput? latency? cost?).",
            "<strong>Step 2:</strong> Sketch the architecture \u2014 source, processing, storage, serving.",
            "<strong>Step 3:</strong> Identify 3 failure modes and describe how you handle each.",
            "<strong>Step 4:</strong> Estimate the data volume and compute requirements using back-of-envelope math."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Solve LeetCode 601 (Human Traffic of Stadium)",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Solve LeetCode 601 (Human Traffic of Stadium) \u2014 trace through 2-3 edge cases before writing any code.",
                "Clarify the scale: how many rows? How many events/second? What's the target latency?",
                "Start with the simplest correct solution, then optimize \u2014 avoid premature optimization.",
                "Mention the failure scenario even if the interviewer doesn't ask \u2014 it signals production experience.",
                "Name the specific technology you'd use (Spark, Kafka, Airflow) and justify the choice."
            ]
        },
        "HardProblem": "Boss Problem: Design a complete production-ready system for Gaps & Islands (Complex) that handles 100M events/day. Include architecture, technology choices with justifications, idempotency strategy, monitoring, and a cost estimate."
    },
    {
        "Week": 1,
        "Day": "Saturday",
        "Phase": "1. Filter (Coding)",
        "Theme": "SQL Analytics",
        "SpecificTopic": "Mock Assessment",
        "ActionItem_Deliverable": "Pick 3 Medium SQL problems (45 mins timer)",
        "LeetCodeProblem": "<strong>LC 1174 \u2013 Immediate Food Delivery II</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": "Done",
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Gaps & Islands (Logic)</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 The Problem Window Functions Solve</div>\n<div class=\"rich\">\n<h4>Start Here \u2014 Before Any Code</h4>\n<p>You're a data analyst at Spotify. Your manager asks: <em>\"Show me each employee's salary AND the average salary for their department \u2014 on the same row.\"</em></p>\n<p>Your first instinct: <code>GROUP BY department</code>. Problem: <strong>GROUP BY collapses rows</strong>. You get one row per department, losing individual employee detail. You can't have both on the same row \u2014 unless you use a slow self-join. This is exactly the gap <strong>Window Functions</strong> fill: they compute aggregates across related rows while <strong>keeping every row in the output</strong>.</p>\n<h4>The Mental Model: A Sliding Frame of Glass</h4>\n<p>Imagine placing a sheet of glass over a spreadsheet. For each row, the database looks through that glass at a \"window\" of rows, performs a calculation, and writes the result back into a new column \u2014 without removing the original row.</p>\n<pre>GROUP BY result:              Window Function result:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 dept        \u2502 avg_sal \u2502    \u2502 name  \u2502 dept        \u2502 salary \u2502 dept_avg \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Engineering \u2502  90,000 \u2502    \u2502 Alice \u2502 Engineering \u2502 95,000 \u2502  90,000  \u2502\n\u2502 Marketing   \u2502  72,500 \u2502    \u2502 Bob   \u2502 Engineering \u2502 85,000 \u2502  90,000  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 Eve   \u2502 Engineering \u2502 90,000 \u2502  90,000  \u2502\n2 rows \u2014 detail lost!         \u2502 Carol \u2502 Marketing   \u2502 70,000 \u2502  72,500  \u2502\n                              \u2502 Dave  \u2502 Marketing   \u2502 75,000 \u2502  72,500  \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              5 rows \u2014 ALL detail preserved \u2705</pre>\n<p>\u270d\ufe0f <strong>Write this down:</strong> Window functions NEVER reduce row count. They only ADD new computed columns. The <code>OVER()</code> keyword is the signal that tells the database \"this is a window function.\" Without OVER(), AVG() collapses rows. With OVER(), it keeps all rows.</p>\n<h4>The Syntax Structure</h4>\n<pre>FUNCTION_NAME() OVER (\n    PARTITION BY column   -- which rows form the window for each row\n    ORDER BY column       -- sort order within the window\n    ROWS BETWEEN ...      -- optional: how many surrounding rows to include\n)</pre>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 ROW_NUMBER \u2014 Step by Step with Full Explanation</div>\n<div class=\"rich\">\n<h4>What ROW_NUMBER() Does</h4>\n<p><code>ROW_NUMBER()</code> assigns a unique sequential integer to each row within a window, sorted as you specify. It ALWAYS gives unique numbers \u2014 even if two rows are completely identical. This makes it perfect for deduplication: you can always pick row #1 per group and discard the rest.</p>\n<h4>Step 1 \u2014 Create and populate the table</h4>\n<pre>CREATE TABLE employees (\n  emp_id   INT,\n  emp_name VARCHAR(50),\n  dept     VARCHAR(50),\n  salary   INT\n);\nINSERT INTO employees VALUES\n  (1, 'Alice', 'Engineering', 95000),\n  (2, 'Bob',   'Engineering', 85000),\n  (3, 'Carol', 'Marketing',   70000),\n  (4, 'Dave',  'Marketing',   75000),\n  (5, 'Eve',   'Engineering', 90000);</pre>\n<h4>Step 2 \u2014 Run your first window function</h4>\n<pre>SELECT\n  emp_name, dept, salary,\n  ROW_NUMBER() OVER (\n    PARTITION BY dept      -- restart numbering for each department\n    ORDER BY salary DESC   -- highest salary = rank 1\n  ) AS rank_in_dept\nFROM employees;</pre>\n<h4>Step 3 \u2014 Trace the output and understand WHY</h4>\n<pre>emp_name \u2502 dept        \u2502 salary \u2502 rank_in_dept\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nAlice    \u2502 Engineering \u2502  95000 \u2502      1    \u2190 highest in Eng \u2192 rank 1\nEve      \u2502 Engineering \u2502  90000 \u2502      2    \u2190 2nd in Eng\nBob      \u2502 Engineering \u2502  85000 \u2502      3    \u2190 3rd in Eng\nDave     \u2502 Marketing   \u2502  75000 \u2502      1    \u2190 RESTARTS! Highest in Marketing\nCarol    \u2502 Marketing   \u2502  70000 \u2502      2    \u2190 2nd in Marketing</pre>\n<p>Dave gets rank 1 even though he earns less than Bob \u2014 because PARTITION BY dept created a completely separate window for Marketing. Dave is simply the top earner within his own department's window.</p>\n<h4>What the Engine Does Internally</h4>\n<ol>\n  <li>Read all 5 rows from the table</li>\n  <li>Split into partitions: Engineering (3 rows), Marketing (2 rows)</li>\n  <li>Sort each partition by salary DESC</li>\n  <li>Assign row numbers 1,2,3... within each sorted partition</li>\n  <li>Attach numbers back to original rows and return all 5 rows</li>\n</ol>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 RANK vs DENSE_RANK \u2014 Why Ties Change Everything</div>\n<div class=\"rich\">\n<h4>The Problem with Ties in Real Data</h4>\n<p>In practice, duplicate values are common: two products with the same rating, two employees with the same salary, two cities with equal population. The three ranking functions handle ties differently, and choosing the wrong one produces wrong business results.</p>\n<pre>-- Add Frank \u2014 same salary as Eve\nINSERT INTO employees VALUES (6, 'Frank', 'Engineering', 90000);\n\nSELECT emp_name, salary,\n  ROW_NUMBER() OVER (ORDER BY salary DESC) AS row_num,\n  RANK()       OVER (ORDER BY salary DESC) AS rnk,\n  DENSE_RANK() OVER (ORDER BY salary DESC) AS dense_rnk\nFROM employees WHERE dept = 'Engineering';</pre>\n<pre>emp_name \u2502 salary \u2502 row_num \u2502 rnk \u2502 dense_rnk\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nAlice    \u2502  95000 \u2502    1    \u2502  1  \u2502     1\nEve      \u2502  90000 \u2502    2    \u2502  2  \u2502     2    \u2190 tied with Frank\nFrank    \u2502  90000 \u2502    3    \u2502  2  \u2502     2    \u2190 tied with Eve\nBob      \u2502  85000 \u2502    4    \u2502  4  \u2502     3    \u2190 RANK skips 3, DENSE_RANK doesn't</pre>\n<p>Bob gets RANK 4 because ranks 1, 2, 2 are taken \u2014 the number 3 is \"skipped\" to reflect that two people occupy the 2nd position. DENSE_RANK gives Bob a 3 with no gap.</p>\n<h4>Choosing the Right Function</h4>\n<table>\n<tr><th>Need exactly one result per group (dedup)?</th><td>ROW_NUMBER()</td></tr>\n<tr><th>Gap after tie is meaningful (sports: no 3rd if two share 2nd)?</th><td>RANK()</td></tr>\n<tr><th>Sequential tiers without gaps (medals, grade bands)?</th><td>DENSE_RANK()</td></tr>\n</table>\n<p>\u270d\ufe0f <strong>Interview trap (LeetCode 185):</strong> \"Top 3 salary values per department\" \u2014 ties must BOTH appear. Use DENSE_RANK not ROW_NUMBER. ROW_NUMBER eliminates tied rows arbitrarily \u2014 wrong answer.</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 FAANG Pattern: Top-N Per Group + Scale Secrets</div>\n<div class=\"rich\">\n<h4>The Most Common FAANG Window Function Pattern</h4>\n<p>Top-N per group appears in virtually every FAANG interview. The key constraint: you CANNOT filter on a window function in the same SELECT \u2014 window functions run in step 5 of SQL's execution order, but WHERE runs in step 2. The solution is always a CTE.</p>\n<pre>-- Top 3 earners per department\nWITH ranked AS (\n  SELECT emp_name, dept, salary,\n    DENSE_RANK() OVER (PARTITION BY dept ORDER BY salary DESC) AS rk\n    -- DENSE_RANK: if 2 people tie for 2nd, both appear in top 3\n  FROM employees\n)\nSELECT dept, emp_name, salary\nFROM ranked\nWHERE rk &lt;= 3          -- NOW we can filter because rk is a real column\nORDER BY dept, rk;</pre>\n<h4>SQL Execution Order \u2014 Memorize This</h4>\n<pre>1. FROM       \u2190 identify tables\n2. WHERE      \u2190 filter rows (window functions do NOT exist here yet!)\n3. GROUP BY   \u2190 aggregate\n4. HAVING     \u2190 filter aggregates\n5. SELECT     \u2190 compute expressions \u2014 window functions are evaluated HERE\n6. ORDER BY   \u2190 sort final result\n7. LIMIT      \u2190 restrict rows\n\nThis is why you need a CTE: the outer WHERE sees the CTE's rk column\nbecause by then, the CTE's step 5 has already been executed.</pre>\n<h4>Scale: Why PARTITION BY Key Choice Matters</h4>\n<p>On billions of rows: good PARTITION BY keys (user_id, device_id) spread data across thousands of nodes \u2014 parallel and fast. Bad keys (is_active: TRUE/FALSE) dump 99% of data on one node \u2014 memory crash. Always ask: \"does my partition key distribute data evenly?\"</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Window functions compute over related rows WITHOUT collapsing output \u2014 unlike GROUP BY.",
            "OVER() transforms any aggregate into a window function. Without OVER: GROUP BY. With OVER: window.",
            "PARTITION BY = independent windows per group. Rankings restart per partition.",
            "ORDER BY inside OVER = sort order within partition. Determines which row gets rank 1.",
            "ROW_NUMBER(): always unique. Use for dedup, pagination, keeping exactly 1 row per group.",
            "RANK(): ties share same number, next is SKIPPED (1,2,2,4). Use for sports/competitions.",
            "DENSE_RANK(): ties share same number, no skip (1,2,2,3). Use for tiered rankings. Use for LeetCode 185.",
            "SQL execution order: FROM\u2192WHERE\u2192GROUP BY\u2192HAVING\u2192SELECT(windows here)\u2192ORDER BY\u2192LIMIT.",
            "Cannot use window function result in WHERE. Always wrap in CTE first."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create employees table and insert 5 rows. Run the ROW_NUMBER query. Explain in writing why Dave gets rank 1.",
            "<strong>Step 2:</strong> Add Frank (Engineering, 90000) and run all 3 ranking functions. Write the difference you see in Bob's row.",
            "<strong>Step 3:</strong> Write the DENSE_RANK top-3-per-department query from scratch without looking. Test it shows both Eve and Frank when tied.",
            "<strong>Step 4:</strong> Try adding WHERE rank_in_dept &lt;= 3 directly in the window query (no CTE). Note the error. Then fix it with a CTE."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Pick 3 Medium SQL problems (45 mins timer)",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Pick 3 Medium SQL problems (45 mins timer) \u2014 trace through 2-3 edge cases before writing any code.",
                "'Per group' or 'within each category' \u2192 your signal to use PARTITION BY.",
                "LeetCode 185 trap: 'top 3 salaries' not 'top 3 employees' \u2192 DENSE_RANK so ties both show.",
                "Can't use window function in WHERE? Wrap in CTE \u2014 this is always the fix.",
                "High-cardinality PARTITION BY (user_id) = good parallelism. Low-cardinality (boolean) = hotspot."
            ]
        },
        "HardProblem": "Boss Problem (Meta): 5-billion row table: user_events(user_id, event_type, revenue, event_ts). (1) Rank each user's events by revenue using DENSE_RANK. (2) Return only top 3 per user. (3) Add running total revenue per user. Explain: Which PARTITION BY key? What happens memory-wise with no PARTITION BY on 5B rows? How would you run this in Spark?"
    },
    {
        "Week": 1,
        "Day": "Sunday",
        "Phase": "1. Filter (Coding)",
        "Theme": "Rest",
        "SpecificTopic": "Rest & Review",
        "ActionItem_Deliverable": "Review notes on Window Functions",
        "LeetCodeProblem": null,
        "Status": "Done",
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Window Function Basics</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Active Rest \u2014 How Memory Consolidates</div>\n<div class=\"rich\">\n<h4>The Neuroscience of Spaced Repetition</h4>\n<p>Rest days are not wasted days. During rest, your brain replays what it learned \u2014 a process called <strong>memory consolidation</strong>. Research consistently shows that <em>attempting to recall from memory</em> is 3\u00d7 more effective at building durable knowledge than re-reading notes. The act of struggling to remember \u2014 even failing \u2014 strengthens the neural pathway more than passive review.</p>\n<p>Think of it like muscle recovery: a weightlifter who trains 7 days straight without rest gets weaker, not stronger. The growth happens during recovery. Your brain is the same.</p>\n<h4>Today's Protocol (4 hours, structured)</h4>\n<ol>\n  <li><strong>Active Recall (60 min):</strong> Close everything. On a blank page, write every concept, formula, and pattern from this week from memory. Then check.</li>\n  <li><strong>Error Review (60 min):</strong> Find the hardest problem you struggled with. Attempt it again completely from scratch \u2014 no notes.</li>\n  <li><strong>Teach It (45 min):</strong> Open the AI chatbot and explain Gaps & Islands to it as if it's a junior engineer. If you stumble, that's your gap.</li>\n  <li><strong>Next Week Preview (15 min):</strong> Skim next Monday's topic at headline level only \u2014 priming improves retention when you study it deeply.</li>\n</ol>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Active Recall Quiz \u2014 No Notes Allowed</div>\n<div class=\"rich\">\n<h4>Answer These From Memory. Then Check.</h4>\n<pre>Window Functions:\n\u25a1 What is the difference between ROW_NUMBER, RANK, and DENSE_RANK?\n  (Write the output for a table with two tied rows)\n\u25a1 Write the syntax for a 7-day rolling average from memory\n\u25a1 What does PARTITION BY do differently from GROUP BY?\n\u25a1 Write the complete Gaps & Islands CTE from memory\n\u25a1 What is the difference between ROWS BETWEEN and RANGE BETWEEN?\n\u25a1 LAG(col, 1, 0) vs LAG(col, 1) \u2014 what is different?\n\u25a1 In what SQL execution order step do window functions run?\n\u25a1 Why can't you write WHERE rank &lt;= 3 in the same query as ROW_NUMBER?\n\nScore yourself:\n8/8 = \ud83c\udfc6 Mastery level \u2014 you're ready for interviews on this\n5-7/8 = \ud83d\udfe1 Good \u2014 review the 1-2 concepts you missed\n&lt; 5/8 = \ud83d\udd34 Go back to the specific day's content, re-read Level 1-2</pre>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Spaced Repetition Schedule</div>\n<div class=\"rich\">\n<h4>When to Review This Week's Material</h4>\n<table>\n<tr><th>Review this content</th><th>When</th><th>How</th></tr>\n<tr><td>This week</td><td>Today</td><td>Active recall quiz above</td></tr>\n<tr><td>This week</td><td>In 3 days</td><td>Re-solve the hard problem from Day 4</td></tr>\n<tr><td>This week</td><td>In 7 days</td><td>LeetCode: solve problems 185, 180, 196</td></tr>\n<tr><td>This week</td><td>In 30 days</td><td>Explain all 4 concepts to someone else</td></tr>\n</table>\n<h4>The Feynman Technique</h4>\n<p>After studying any concept, close your notes and explain it as if you're teaching a newcomer. Use only simple words. Avoid jargon. If you stumble, that's exactly where your understanding breaks down \u2014 return to just that part. This technique built Richard Feynman's legendary ability to explain complex physics simply.</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Interview Readiness Self-Assessment</div>\n<div class=\"rich\">\n<pre>Rate yourself 1\u20135 on each topic (honest self-assessment):\n  1 = \"I need to re-read from scratch\"\n  3 = \"I understand but need to look at notes\"\n  5 = \"I can code this under pressure from memory\"\n\nROW_NUMBER / RANK / DENSE_RANK:        ___/5\nRolling windows (ROWS BETWEEN):        ___/5\nLAG/LEAD (adjacent row comparison):    ___/5\nGaps & Islands (date streaks):         ___/5\nSession detection (timestamp version): ___/5\n\nAction:\n  Score &lt;3: revisit that day's Level 1 and Level 2 content tomorrow\n  Score 3:  do one LeetCode problem on that specific topic\n  Score 5:  you're interview-ready \u2705 on this topic</pre>\n<h4>FAANG Interview Reality Check</h4>\n<p>At meta, Amazon, and Google data engineering interviews, you'll be asked to write SQL live on a shared screen without autocomplete or documentation. The most common reason candidates fail: they know the concept but freeze on the syntax under pressure. The fix: write every pattern from memory at least 5 times this week. Muscle memory is more reliable than conscious recall under stress.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Memory consolidation happens during rest \u2014 the brain replays and stabilizes learned patterns during downtime.",
            "Active recall (testing yourself) builds 3x stronger memory than re-reading notes.",
            "Spaced repetition: review at day 1, day 3, day 7, day 30 for maximum retention.",
            "The Feynman Technique: if you can't explain it simply, you don't fully understand it yet.",
            "Error analysis: the problems you got WRONG are your highest-value review items \u2014 spend the most time there."
        ],
        "Tasks": [
            "<strong>Active Recall:</strong> Write the Gaps & Islands CTE from memory on paper. Check it against Day 4.",
            "<strong>Re-attempt:</strong> Pick the hardest problem from this week and solve it in 20 minutes from scratch.",
            "<strong>Teach it:</strong> Explain RANK vs DENSE_RANK to the AI chatbot as if it's a junior. Get it right without notes.",
            "<strong>Preview:</strong> Skim next week's Monday topic for 10 minutes \u2014 just headline concepts."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Review notes on Window Functions",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Review notes on Window Functions \u2014 trace through 2-3 edge cases before writing any code.",
                "Use the AI chatbot to quiz yourself \u2014 ask it 'Give me a SQL window function interview question.'",
                "Write SQL from memory in a plain text editor \u2014 no autocomplete. This builds true recall.",
                "The hardest concept this week: spend 30 minutes writing a full explanation in your own words."
            ]
        },
        "HardProblem": "Connect-the-dots: Write 2-3 paragraphs explaining how ROW_NUMBER, rolling averages, LAG, and Gaps & Islands could ALL be used in a single production pipeline for a ride-sharing company. What problem does each solve? How do they complement each other? What would break if you removed any one of them?"
    },
    {
        "Week": 2,
        "Day": "Monday",
        "Phase": "1. Filter (Coding)",
        "Theme": "SQL Optimization",
        "SpecificTopic": "Recursive CTEs",
        "ActionItem_Deliverable": "Solve LeetCode 1270 (All People Report to Manager)",
        "LeetCodeProblem": "<strong>LC 1270 \u2013 All People Report to Manager</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": "Done",
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Gaps & Islands (Logic)</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 What Are CTEs and Why Do Recursive CTEs Exist?</div>\n<div class=\"rich\">\n<h4>First \u2014 What Is a CTE?</h4>\n<p>A <strong>Common Table Expression (CTE)</strong> is a named, temporary result set defined at the top of a query using <code>WITH</code>. Think of it as giving a name to a subquery so you can reference it later \u2014 like naming a variable in programming. It makes complex queries readable by breaking them into clear, named steps.</p>\n<pre>-- Without CTE: nested, hard to read\nSELECT dept, avg_sal FROM (SELECT dept, AVG(salary) AS avg_sal FROM employees GROUP BY dept) sub;\n\n-- With CTE: reads like steps of a story\nWITH dept_averages AS (\n  SELECT dept, AVG(salary) AS avg_sal FROM employees GROUP BY dept\n)\nSELECT dept, avg_sal FROM dept_averages;   -- clean!</pre>\n<h4>Now \u2014 Why Recursive CTEs?</h4>\n<p>Some data is naturally hierarchical: an employee reports to a manager who reports to a VP who reports to a CEO. Or a product category has subcategories which have sub-subcategories. Or a network of roads connects city A \u2192 B \u2192 C \u2192 D with no fixed depth.</p>\n<p>Standard SQL can't traverse this structure because it doesn't know how many levels deep to go \u2014 the depth is unknown until you actually walk the hierarchy. <strong>Recursive CTEs</strong> solve this by defining a query that calls itself, going one level deeper on each iteration, until it reaches the bottom (a row with no more children).</p>\n<h4>The Mental Model: A Snowball Rolling Downhill</h4>\n<p>Imagine rolling a snowball from the top of a hill. It starts small (the CEO, the root node). As it rolls down, it picks up more snow (each level of employees). Each iteration, it grows by one layer, until it reaches the bottom of the hill (leaf employees with no reports). The recursive CTE is the rule that says \"keep rolling, picking up one layer at a time, until there's nothing left to pick up.\"</p>\n<pre>Iteration 1 (Anchor): seed_rows = [CEO]\nIteration 2: rows where manager = CEO \u2192 [VP1, VP2]\nIteration 3: rows where manager = VP1 or VP2 \u2192 [Dir1, Dir2, Dir3]\nIteration 4: rows where manager = Dir1,2,3 \u2192 [Emp1, Emp2 ... Emp8]\nIteration 5: no more rows \u2192 STOP</pre>\n<p>\u270d\ufe0f <strong>Write this down:</strong> A recursive CTE has two mandatory parts \u2014 the <strong>anchor member</strong> (the starting point, non-recursive) and the <strong>recursive member</strong> (the part that references the CTE itself, going one level deeper). They are joined with <code>UNION ALL</code>.</p>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Step-by-Step: Traversing an Employee Hierarchy</div>\n<div class=\"rich\">\n<h4>The Data: A Classic Manager-Employee Tree</h4>\n<pre>CREATE TABLE employees (\n  emp_id   INT PRIMARY KEY,\n  emp_name VARCHAR(50),\n  manager_id INT    -- NULL means this person is the CEO (top of tree)\n);\nINSERT INTO employees VALUES\n  (1, 'Alice (CEO)',   NULL),   -- root\n  (2, 'Bob (VP Eng)',    1),\n  (3, 'Carol (VP Mkt)',  1),\n  (4, 'Dave (Dir)',      2),\n  (5, 'Eve (Dir)',       2),\n  (6, 'Frank (Eng)',     4),\n  (7, 'Grace (Eng)',     4),\n  (8, 'Heidi (Mkt)',     3);</pre>\n<h4>The Recursive CTE \u2014 Every Line Explained</h4>\n<pre>WITH RECURSIVE org_tree AS (\n\n  -- \u2460 ANCHOR MEMBER: the starting point\n  -- This runs ONCE. It seeds the recursion with the root row(s).\n  SELECT\n    emp_id, emp_name, manager_id,\n    0 AS level,                      -- CEO is at depth 0\n    emp_name AS path                 -- start the full path string\n  FROM employees\n  WHERE manager_id IS NULL           -- \"give me the top of the tree\"\n\n  UNION ALL                          -- stack anchor rows + recursive rows\n\n  -- \u2461 RECURSIVE MEMBER: runs once per level, references org_tree\n  -- Each iteration, it finds direct reports of the PREVIOUS iteration's results\n  SELECT\n    e.emp_id, e.emp_name, e.manager_id,\n    t.level + 1,                     -- go one level deeper\n    t.path || ' \u2192 ' || e.emp_name    -- extend the path string\n  FROM employees e\n  INNER JOIN org_tree t              -- join employees to LAST iteration's results\n    ON e.manager_id = t.emp_id      -- \"whose manager is one of the rows already found?\"\n)\nSELECT * FROM org_tree ORDER BY level, emp_id;</pre>\n<pre>emp_name          \u2502 level \u2502 path\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nAlice (CEO)       \u2502   0   \u2502 Alice (CEO)\nBob (VP Eng)      \u2502   1   \u2502 Alice (CEO) \u2192 Bob (VP Eng)\nCarol (VP Mkt)    \u2502   1   \u2502 Alice (CEO) \u2192 Carol (VP Mkt)\nDave (Dir)        \u2502   2   \u2502 Alice (CEO) \u2192 Bob (VP Eng) \u2192 Dave (Dir)\nEve (Dir)         \u2502   2   \u2502 Alice (CEO) \u2192 Bob (VP Eng) \u2192 Eve (Dir)\nFrank (Eng)       \u2502   3   \u2502 Alice (CEO) \u2192 Bob \u2192 Dave \u2192 Frank (Eng)\nGrace (Eng)       \u2502   3   \u2502 Alice (CEO) \u2192 Bob \u2192 Dave \u2192 Grace (Eng)\nHeidi (Mkt)       \u2502   2   \u2502 Alice (CEO) \u2192 Carol (VP Mkt) \u2192 Heidi (Mkt)</pre>\n<p>The level column gives you depth. The path column builds a breadcrumb trail. Both are built incrementally: each iteration adds 1 to level and appends \u2192 emp_name to path.</p>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Number Generation, Fibonacci, and Graph Traversal</div>\n<div class=\"rich\">\n<h4>Beyond Hierarchies: Three More Recursive CTE Patterns</h4>\n<p><strong>Pattern 1: Generating a number sequence without a numbers table.</strong> Instead of having a static table of integers for JOIN tricks, generate them on the fly:</p>\n<pre>WITH RECURSIVE nums AS (\n  SELECT 1 AS n          -- anchor: start at 1\n  UNION ALL\n  SELECT n + 1 FROM nums -- recursive: add 1 each time\n  WHERE n &lt; 100          -- termination condition: stop at 100!\n)\nSELECT n FROM nums;      -- gives you 1, 2, 3, ... 100</pre>\n<p>\u26a0\ufe0f <strong>Always include a termination condition</strong> (the WHERE clause in the recursive member). Without it, the query runs forever \u2014 an infinite loop that will kill your database connection or exhaust memory.</p>\n<p><strong>Pattern 2: Date sequence generation.</strong> Fill in missing dates between two endpoints:</p>\n<pre>WITH RECURSIVE date_range AS (\n  SELECT '2024-01-01'::DATE AS d        -- anchor: start date\n  UNION ALL\n  SELECT d + INTERVAL 1 DAY FROM date_range\n  WHERE d &lt; '2024-01-31'               -- stop at end date\n)\nSELECT d AS all_dates FROM date_range;   -- all 31 dates, even if no data</pre>\n<p>Use case: LEFT JOIN this date_range to your events table to fill zeros on days with no activity \u2014 essential for rolling window calculations that would otherwise skip silent days.</p>\n<p><strong>Pattern 3: Graph shortest path.</strong> Given a roads table (city_from, city_to, distance), find all cities reachable from London and their total distance:</p>\n<pre>WITH RECURSIVE reachable AS (\n  SELECT 'London' AS city, 0 AS total_dist\n  UNION ALL\n  SELECT r.city_to, rch.total_dist + r.distance\n  FROM roads r JOIN reachable rch ON r.city_from = rch.city\n  WHERE rch.total_dist &lt; 500            -- stop searching beyond 500 miles\n)\nSELECT DISTINCT city, MIN(total_dist) AS shortest FROM reachable GROUP BY city;</pre>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 FAANG Interview: Bill of Materials + Cycle Detection</div>\n<div class=\"rich\">\n<h4>The Bill of Materials Problem (Amazon Interview Staple)</h4>\n<p>A bill of materials (BOM) asks: \"What are ALL the components needed to build product X, including components of components?\" A car needs an engine which needs a cylinder block which needs bolts... the depth is unknown. This is recursive CTE territory.</p>\n<pre>WITH RECURSIVE bom AS (\n  SELECT component_id, component_name, parent_id, quantity, 1 AS depth\n  FROM components WHERE parent_id IS NULL  -- top-level products\n  UNION ALL\n  SELECT c.component_id, c.component_name, c.parent_id, c.quantity, b.depth + 1\n  FROM components c JOIN bom b ON c.parent_id = b.component_id\n  WHERE b.depth &lt; 10   -- safety valve: never go deeper than 10 levels\n)\nSELECT * FROM bom ORDER BY depth;</pre>\n<h4>Cycle Detection \u2014 The Critical Safety Check</h4>\n<p>In graph data (not tree data), cycles can exist: A \u2192 B \u2192 C \u2192 A. Without protection, the recursive CTE loops forever. The defense: track the path as an array and stop if the next node is already in the array.</p>\n<pre>WITH RECURSIVE traverse AS (\n  SELECT node_id, ARRAY[node_id] AS visited_path\n  FROM graph WHERE node_id = 1\n  UNION ALL\n  SELECT e.to_node, t.visited_path || e.to_node\n  FROM edges e JOIN traverse t ON e.from_node = t.node_id\n  WHERE NOT (e.to_node = ANY(t.visited_path))  -- stop if cycle detected!\n)\nSELECT * FROM traverse;</pre>\n<p>Interview tip: always mention cycle detection when discussing recursive CTEs on graph data. Interviewers specifically check whether you know this failure mode \u2014 it's one of the most common causes of runaway queries in production.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "CTE = named subquery using WITH. Makes complex queries readable. Evaluated once per reference in most DBs.",
            "Recursive CTE = two parts joined by UNION ALL: anchor (seeds the recursion) + recursive member (references itself).",
            "The recursive member runs repeatedly until it produces zero new rows \u2014 that's the automatic stop condition.",
            "Always include an explicit termination condition (WHERE depth &lt; N) as a safety valve against infinite loops.",
            "level column: add 1 per iteration to track depth. path column: append names per iteration to build breadcrumbs.",
            "Cycle detection: track visited nodes in an array; stop if next node is already in the array (graph data only).",
            "Date sequence trick: generate all dates between two endpoints using recursive CTE to fill 'silent day' gaps."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create the employee hierarchy table. Run the recursive CTE. Verify Frank and Grace appear at level 3.",
            "<strong>Step 2:</strong> Modify the query to also show how many direct reports each employee has (add a subquery count).",
            "<strong>Step 3:</strong> Write a number generator from 1 to 50. Extend it: show only even numbers.",
            "<strong>Step 4 \u2014 Write from scratch:</strong> Generate all dates in January 2024. LEFT JOIN to a sales table to show $0 for days with no sales."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Solve LeetCode 1270 (All People Report to Manager)",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Solve LeetCode 1270 (All People Report to Manager) \u2014 trace through 2-3 edge cases before writing any code.",
                "Infinite loop? You forgot the WHERE termination condition in the recursive member. Always add WHERE depth &lt; N.",
                "UNION vs UNION ALL in recursive CTEs: always use UNION ALL \u2014 UNION deduplicates on every iteration which is very slow.",
                "For manager hierarchy queries, always include a 'max depth' guard: even clean data can have accidental cycles.",
                "Date generation with recursive CTE is cleaner than a calendar table for ad-hoc ranges."
            ]
        },
        "HardProblem": "Boss Problem (LinkedIn): You have a table: connections(user_id, connected_to, connection_date). Write a query that finds all users reachable from user #1 within 3 degrees of connection (like LinkedIn's '3rd connection'). Return: user_id, degree, and the path of user IDs from user 1 to them. Handle cycles (mutual connections A\u2194B). Explain how performance degrades as the graph grows and what index you'd add."
    },
    {
        "Week": 2,
        "Day": "Tuesday",
        "Phase": "1. Filter (Coding)",
        "Theme": "SQL Optimization",
        "SpecificTopic": "Execution Plans",
        "ActionItem_Deliverable": "Watch Postgres Explain Visualized & read EXPLAIN output",
        "LeetCodeProblem": "<strong>LC 570 \u2013 Managers with 5 Direct Reports</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": "Done",
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Window Function Basics</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 How the Database Actually Executes Your Query</div>\n<div class=\"rich\">\n<h4>The Hidden Machinery: Query Execution Pipeline</h4>\n<p>When you press Enter on a SQL query, you think the database just \"runs\" it. In reality, there are 4 distinct steps, and understanding them is the key to knowing WHY some queries are fast and others crawl on the same data:</p>\n<ol>\n  <li><strong>Parsing:</strong> The SQL text is tokenized and parsed into a syntax tree. If you have a typo, the error happens here \u2014 before touching any data.</li>\n  <li><strong>Optimization:</strong> The <em>query optimizer</em> \u2014 one of the most sophisticated pieces of software in a database \u2014 rewrites your query into the most efficient physical execution plan. It considers: which index to use, which table to scan first, how to order JOINs, whether to hash or sort. It evaluates thousands of possible plans and picks the cheapest based on cost estimates.</li>\n  <li><strong>Execution:</strong> The plan is carried out on actual data, reading pages from disk or buffer cache.</li>\n  <li><strong>Result return:</strong> Rows flow back to the client.</li>\n</ol>\n<p>You only write step 1 (the SQL text). Steps 2 and 3 are entirely the database's decision. But you can influence step 2 enormously by understanding what the optimizer looks for.</p>\n<h4>The Most Important Tool: EXPLAIN / EXPLAIN ANALYZE</h4>\n<p><code>EXPLAIN</code> shows what plan the optimizer chose <em>without running the query</em>. <code>EXPLAIN ANALYZE</code> runs the query AND shows actual vs estimated row counts \u2014 the discrepancy between those two numbers is where you find bugs in the optimizer's cost model.</p>\n<pre>EXPLAIN ANALYZE\nSELECT * FROM orders WHERE customer_id = 42;\n-- Output shows: Seq Scan or Index Scan, rows estimated vs actual, time\n-- \"Seq Scan\" = reading EVERY row = slow on large tables\n-- \"Index Scan\" = jumping directly using an index = fast</pre>\n<p>\u270d\ufe0f <strong>Rule 1:</strong> Anytime a query is slow, your first action is always EXPLAIN ANALYZE. Everything else is guessing.</p>\n<h4>What \"Slow Query\" Actually Means</h4>\n<p>A query that processes 1 million rows where it only needed to look at 100 is slow because of <em>unnecessary I/O</em>. Every row the database reads from disk costs time. The optimizer's #1 job is to minimize the rows read. Your job is to write SQL that gives the optimizer a chance to do that \u2014 by using indexed columns, avoiding function wrappers on columns, and joining in the right order.</p>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 The Rules of Sargability \u2014 What the Optimizer Can and Can't Use</div>\n<div class=\"rich\">\n<h4>SARGable: Search ARGument able</h4>\n<p>A query condition is <strong>sargable</strong> if the database engine can use an index to satisfy it. Non-sargable conditions force a full table scan \u2014 reading every single row even if only 1 matches. Learning to spot non-sargable patterns and fix them is worth hours of optimization effort.</p>\n<pre>-- \u274c NON-SARGABLE: function wrapped around the column\n-- The DB cannot use an index on order_date because it must first\n-- call YEAR() on EVERY row to evaluate the condition.\nSELECT * FROM orders WHERE YEAR(order_date) = 2024;\n\n-- \u2705 SARGABLE: rewrite as a RANGE on the raw column\n-- The DB can jump directly to the first Jan 1 entry in the index.\nSELECT * FROM orders WHERE order_date &gt;= '2024-01-01' AND order_date &lt; '2025-01-01';\n\n-- \u274c NON-SARGABLE: arithmetic on the indexed column\nSELECT * FROM orders WHERE order_amount * 1.1 &gt; 1000;\n\n-- \u2705 SARGABLE: move the math to the right side\nSELECT * FROM orders WHERE order_amount &gt; 1000 / 1.1;   -- 909.09\n\n-- \u274c NON-SARGABLE: LIKE with leading wildcard\n-- '%ion' means \"ends with ion\" \u2014 no way to use alphabetical index\nSELECT * FROM products WHERE name LIKE '%ion';\n\n-- \u2705 SARGABLE: leading-fixed LIKE\n-- 'ion%' means \"starts with ion\" \u2014 index can seek to first 'ion' entry\nSELECT * FROM products WHERE name LIKE 'ion%';</pre>\n<p>The fix pattern is always the same: <em>never wrap the indexed column in a function or formula</em>. Instead, rewrite the condition so the column stands alone on one side of the operator and all transformations happen on the constant (literal value) side.</p>\n<h4>The Optimizer's Cost Estimate \u2014 Why It Sometimes Gets It Wrong</h4>\n<p>The optimizer doesn't \"see\" the data \u2014 it uses statistics (histograms of column value distributions) collected by the last ANALYZE command. If statistics are stale (your table grew by 10x since last ANALYZE), the optimizer may think a column has 1,000 distinct values when it actually has 1 billion \u2014 and choose a terrible plan. The fix: run <code>ANALYZE table_name</code> to refresh statistics on large tables after bulk loads.</p>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Join Order, Predicate Pushdown, and CTE Optimization</div>\n<div class=\"rich\">\n<h4>Join Order Matters More Than You Think</h4>\n<p>When you join 3 tables, there are 6 possible join orders (3! = 6). The optimizer tries all of them and picks the cheapest. But on 10+ tables, this becomes 3.6 million possibilities \u2014 optimizers use heuristics instead, which can be wrong. The general rule: <strong>filter early, join small tables first</strong>. Bring large tables in late after they've been pre-filtered.</p>\n<pre>-- \u274c Slow: JOIN all three tables first, THEN filter by high-value\nSELECT o.order_id, c.name, p.category\nFROM orders o\nJOIN customers c ON o.customer_id = c.customer_id\nJOIN products p ON o.product_id = p.product_id\nWHERE o.revenue &gt; 10000;\n\n-- \u2705 Fast: filter orders FIRST (to a small set), then join\nWITH high_value_orders AS (\n  SELECT * FROM orders WHERE revenue &gt; 10000  -- reduce from 1B to 50K rows\n)\nSELECT h.order_id, c.name, p.category\nFROM high_value_orders h\nJOIN customers c ON h.customer_id = c.customer_id\nJOIN products p ON h.product_id = p.product_id;</pre>\n<h4>Predicate Pushdown</h4>\n<p>Modern optimizers automatically \"push\" WHERE conditions as deep into the query plan as possible \u2014 applying filters before joins rather than after. But this has limits: conditions on computed columns, window functions, and some subqueries don't get pushed down automatically. Always apply your most selective filters as early as possible in your CTE chain \u2014 don't rely on the optimizer to do it.</p>\n<h4>CTE Materialization \u2014 A Critical Database Difference</h4>\n<p>In <strong>PostgreSQL 12+</strong>: CTEs are lazily evaluated \u2014 the optimizer can inline them and push predicates through. In <strong>older PostgreSQL and SQL Server</strong>: CTEs are \"optimization fences\" \u2014 they're evaluated once, results materialized into a temp table, optimizer cannot push outer WHERE into them. This makes some CTEs on old systems unexpectedly slow. Fix: use subqueries or WITH MATERIALIZED/NOT MATERIALIZED hints.</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 FAANG-Scale: Reading EXPLAIN Plans and Common Anti-Patterns</div>\n<div class=\"rich\">\n<h4>Reading an EXPLAIN ANALYZE Output</h4>\n<pre>EXPLAIN ANALYZE SELECT * FROM orders WHERE customer_id = 42;\n\n-- Sample output:\n-- Index Scan using orders_customer_idx on orders (cost=0.56..8.58 rows=3 width=120)\n--   (actual time=0.042..0.044 rows=3 loops=1)\n--   Index Cond: (customer_id = 42)\n-- Planning Time: 0.3 ms\n-- Execution Time: 0.1 ms\n</pre>\n<p><strong>What to look for:</strong></p>\n<ul>\n  <li><code>Seq Scan</code> on a large table = \ud83d\udd34 missing index or optimizer chose wrong plan</li>\n  <li><code>rows=3</code> estimated vs <code>rows=3 million</code> actual = \ud83d\udd34 stale statistics, run ANALYZE</li>\n  <li><code>Hash Join</code> with large hash table = \ud83d\udfe1 may spill to disk if work_mem is too small</li>\n  <li><code>Nested Loop</code> on large tables = \ud83d\udd34 usually means wrong join order or missing index</li>\n  <li><code>Sort</code> before a window function without ORDER BY index = \ud83d\udfe1 add index on window ORDER BY column</li>\n</ul>\n<h4>Top 5 Query Anti-Patterns at FAANG Scale</h4>\n<table>\n<tr><th>Anti-Pattern</th><th>Why It's Slow</th><th>Fix</th></tr>\n<tr><td>SELECT *</td><td>Reads all columns even if you need 2. Breaks column pruning in columnar stores.</td><td>List only needed columns</td></tr>\n<tr><td>Subquery in WHERE per row</td><td>Runs the subquery once per outer row = N\u00b2 scans</td><td>Convert to JOIN or EXISTS</td></tr>\n<tr><td>DISTINCT to fix bad JOINs</td><td>Sorts/hashes entire result to deduplicate. Hides the real problem.</td><td>Fix the JOIN cardinality instead</td></tr>\n<tr><td>OR on different columns</td><td>Cannot use index on either column \u2014 full scan</td><td>UNION ALL of two indexed queries</td></tr>\n<tr><td>Implicit type cast</td><td>WHERE int_col = '42' casts every row to string \u2014 loses index</td><td>Match types: WHERE int_col = 42</td></tr>\n</table>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Query execution: Parse \u2192 Optimize (choose plan) \u2192 Execute (read data) \u2192 Return. You influence step 2 via SQL structure.",
            "EXPLAIN ANALYZE: always your first debugging tool. Shows actual vs estimated rows \u2014 discrepancy = stale stats or bad plan.",
            "Sargable: condition the optimizer can use an index for. Non-sargable = full table scan.",
            "Never apply functions to indexed columns in WHERE. Rewrite: move transformations to the literal side.",
            "LIKE '%word' is non-sargable (leading wildcard). LIKE 'word%' is sargable (trailing wildcard only).",
            "Join order: filter early (small result sets), join large tables last. Most selective filter first.",
            "CTE materialization: PostgreSQL 12+ inlines CTEs. Older versions materialize them \u2014 can cause slowness.",
            "Stale optimizer statistics cause bad plans. Run ANALYZE after bulk loads on large tables."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write a query on orders with YEAR(order_date) = 2024. Run EXPLAIN. Note 'Seq Scan'. Then rewrite as a range condition and compare EXPLAIN output.",
            "<strong>Step 2:</strong> Write a query with a correlated subquery in WHERE (SELECT MAX(x) FROM other WHERE other.id = outer.id). Rewrite as a JOIN. Compare explain plans.",
            "<strong>Step 3:</strong> Write SELECT * from a 3-table join. Then rewrite selecting only 3 necessary columns. Is there a difference in execution time?",
            "<strong>Step 4:</strong> Create a table with 100,000 rows. Run EXPLAIN on a WHERE condition. Then run ANALYZE. Run EXPLAIN again. Did the estimated row count change?"
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Watch Postgres Explain Visualized & read EXPLAIN output",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Watch Postgres Explain Visualized & read EXPLAIN output \u2014 trace through 2-3 edge cases before writing any code.",
                "Slow query? EXPLAIN ANALYZE first, always. Never guess at optimization without the execution plan.",
                "Seq Scan on a large table = missing index OR non-sargable WHERE condition. Check both.",
                "SELECT * in production = red flag. Always list specific columns especially in columnar stores (BigQuery, Redshift).",
                "OR across different columns can't use indexes. Rewrite as UNION ALL of two queries, each using its own index."
            ]
        },
        "HardProblem": "Boss Problem (Google): A table user_events(user_id, event_type, event_ts, session_id) has 10 billion rows. Query: count distinct users per event_type for last 24 hours, ordered by count DESC. The current query takes 45 minutes. Walk through: (1) What does EXPLAIN ANALYZE show? (2) What indexes would you add? (3) How would you rewrite the query? (4) Would you partition the table? On what column? (5) In BigQuery, why does SELECT * hurt this query more than in PostgreSQL?"
    },
    {
        "Week": 2,
        "Day": "Wednesday",
        "Phase": "1. Filter (Coding)",
        "Theme": "SQL Optimization",
        "SpecificTopic": "Indexing Strategies",
        "ActionItem_Deliverable": "Explain B-Tree vs Hash Index on paper",
        "LeetCodeProblem": "<strong>LC 184 \u2013 Dept Highest Salary</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": "Done",
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Gaps & Islands (Complex)</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Why Indexes Exist \u2014 The Library Analogy</div>\n<div class=\"rich\">\n<h4>The Problem: Finding a Needle in a 10-Billion-Row Haystack</h4>\n<p>Imagine a library with 10 million books, organized in no particular order. You want \"The Great Gatsby.\" Without a catalog, you'd have to walk through every single shelf, checking each book one by one. That's a <strong>sequential scan</strong> \u2014 exactly what the database does on an un-indexed column.</p>\n<p>Now the librarian hands you a catalog \u2014 an alphabetical index of every book title mapped to its shelf location. You flip to G, find \"Great Gatsby,\" see \"Shelf 4B, position 22,\" walk directly there. Done. That's an <strong>index scan</strong> \u2014 the database equivalent.</p>\n<p>On a 10-billion-row table, a sequential scan might read every row off disk over 10 minutes. An index scan jumps directly to the matching rows in 0.1 seconds. <strong>The index is often the difference between a query that works and one that doesn't.</strong></p>\n<h4>What an Index Actually Is (Under the Hood)</h4>\n<p>Most database indexes are <strong>B-Trees</strong> (Balanced Trees) \u2014 a sorted, tree-structured file stored separately from the main table data. The tree has a root node at the top, internal nodes that guide navigation, and leaf nodes at the bottom that contain the indexed column values plus pointers (physical addresses) to the actual table rows.</p>\n<pre>B-Tree Index on salary column:\n                    [50000]\n                   /                  [30000]           [80000]\n           /     \\           /            [10k,20k] [40k]  [60k,70k] [90k,95k]\n         \u2193         \u2193       \u2193          \u2193\n     (row pointers to actual table pages)</pre>\n<p>To find salary = 90000: start at root, go right (90000 > 50000), go right again (90000 > 80000), read the leaf \u2014 find the row pointer \u2014 jump to that exact table page. 3 operations instead of 10 billion.</p>\n<p>\u270d\ufe0f <strong>Key trade-off:</strong> Every index speeds up reads but slows down writes (INSERT/UPDATE/DELETE must update the index too). You never add indexes blindly \u2014 only on columns that appear in WHERE, JOIN ON, or ORDER BY of frequent, slow queries.</p>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 B-Tree, Hash, and Composite Indexes \u2014 When to Use Each</div>\n<div class=\"rich\">\n<h4>The 4 Main Index Types</h4>\n<p><strong>1. B-Tree Index (default)</strong> \u2014 works for equality, ranges, BETWEEN, LIKE 'prefix%', and ORDER BY. This is the index you use 90% of the time.</p>\n<pre>CREATE INDEX idx_orders_date ON orders(order_date);\n-- Now fast: WHERE order_date = '2024-01-15'\n-- Also fast: WHERE order_date BETWEEN '2024-01-01' AND '2024-01-31'\n-- Also fast: ORDER BY order_date (sorted already!)</pre>\n<p><strong>2. Hash Index</strong> \u2014 works ONLY for equality checks (=). Faster than B-Tree for pure equality, but useless for ranges, sorting, or LIKE. Rare in practice.</p>\n<pre>CREATE INDEX idx_users_email_hash ON users USING HASH (email);\n-- Fast: WHERE email = 'alice@example.com'\n-- \u274c Cannot use for: WHERE email LIKE 'alice%' or ORDER BY email</pre>\n<p><strong>3. Composite Index (multi-column)</strong> \u2014 indexes multiple columns together. Column order matters critically.</p>\n<pre>-- Composite index on (customer_id, order_date)\nCREATE INDEX idx_orders_customer_date ON orders(customer_id, order_date);\n\n-- \u2705 Uses the index (matches leftmost prefix: customer_id)\nSELECT * FROM orders WHERE customer_id = 42;\n\n-- \u2705 Uses the index (both columns in order)\nSELECT * FROM orders WHERE customer_id = 42 AND order_date > '2024-01-01';\n\n-- \u274c Does NOT use the index (skipped the first column!)\nSELECT * FROM orders WHERE order_date > '2024-01-01';</pre>\n<p><strong>The Leftmost Prefix Rule:</strong> A composite index (A, B, C) can be used for queries on: A alone, A+B together, or A+B+C together. It CANNOT be used for B alone or C alone or B+C without A. The index is like a phone book sorted by last name, then first name \u2014 you can look up by last name, or last+first, but not by first name alone.</p>\n<p><strong>4. Covering Index (Partial Index)</strong> \u2014 if the index contains ALL columns the query needs, the DB never touches the main table at all (index-only scan).</p>\n<pre>-- Query only needs customer_id and total_amount\nCREATE INDEX idx_orders_covering ON orders(customer_id, total_amount);\n-- Now this query reads the index only \u2014 never visits the table!\nSELECT customer_id, SUM(total_amount) FROM orders WHERE customer_id = 42;</pre>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Index Selectivity, Bloat, and the Cases to AVOID Indexes</div>\n<div class=\"rich\">\n<h4>Selectivity \u2014 The Most Important Index Concept You're Never Taught</h4>\n<p><strong>Selectivity</strong> is the ratio of distinct values to total rows. High selectivity = many distinct values relative to total rows = the index is useful. Low selectivity = few distinct values = the index is nearly worthless.</p>\n<pre>Table: 10 million orders\n\nColumn: order_id (10M distinct values)\n  Selectivity: 10M/10M = 100% \u2192 perfect index\n  Finding one order: 3 B-Tree hops \u2192 1 row returned\n\nColumn: status ('pending', 'shipped', 'delivered', 'cancelled')\n  Selectivity: 4/10M = 0.00004% \u2192 terrible index!\n  \"WHERE status = 'shipped'\" returns 3 million rows\n  The DB will IGNORE the index and do a full table scan anyway \u2014\n  it's faster to scan sequentially than do 3M random I/O jumps</pre>\n<p>Rule of thumb: if a WHERE condition returns more than ~5-10% of the table, the DB will ignore the index. Focus indexes on high-cardinality columns.</p>\n<h4>Index Bloat \u2014 When Indexes Hurt Performance</h4>\n<p>Every UPDATE and DELETE on indexed columns leaves \"dead\" entries in the B-Tree that aren't immediately removed. Over time, this bloat grows. A table with 1 million live rows might have an index 3x the size it should be \u2014 all dead entries. This wastes memory (buffer cache fills with dead pages) and slows scans. Fix: run <code>VACUUM ANALYZE</code> in PostgreSQL or <code>REBUILD INDEX</code> in SQL Server periodically.</p>\n<h4>When NOT to Add an Index</h4>\n<table>\n<tr><th>Scenario</th><th>Reason to skip the index</th></tr>\n<tr><td>Column with few distinct values (gender, boolean)</td><td>Low selectivity \u2014 DB will ignore it</td></tr>\n<tr><td>Small tables (&lt; ~10,000 rows)</td><td>Full scan is faster than B-Tree overhead</td></tr>\n<tr><td>Write-heavy tables (logs, events)</td><td>Every INSERT must update all indexes \u2014 can be 5x slower writes</td></tr>\n<tr><td>Columns never in WHERE/JOIN/ORDER BY</td><td>Index never used \u2014 pure overhead</td></tr>\n</table>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 FAANG Patterns: Partial Indexes, Function Indexes, Index-Only Scans</div>\n<div class=\"rich\">\n<h4>3 Advanced Index Patterns Used in Production at FAANG</h4>\n<p><strong>1. Partial Index</strong> \u2014 index only a subset of rows matching a condition. Dramatically smaller and faster than a full index when queries almost always filter on a fixed condition.</p>\n<pre>-- 99% of queries only look at active users\n-- Full index on user_id: 100M entries\n-- Partial index: only 2M active user entries \u2014 50x smaller!\nCREATE INDEX idx_active_users ON users(user_id) WHERE status = 'active';\n-- This index is only used for: WHERE user_id = X AND status = 'active'</pre>\n<p><strong>2. Function-Based Index</strong> \u2014 index the result of a function on a column. Solves the non-sargable LOWER() problem.</p>\n<pre>-- Queries always do case-insensitive email search\nWHERE LOWER(email) = LOWER('Alice@Example.com')\n\n-- Solution: index the lowercased value\nCREATE INDEX idx_users_email_lower ON users(LOWER(email));\n-- Now LOWER(email) = '...' uses the index!</pre>\n<p><strong>3. Index-Only Scan (Covering Index)</strong> \u2014 the most powerful optimization: structure the index so all needed columns are IN the index, eliminating table heap access entirely.</p>\n<pre>-- Query: monthly revenue per customer\nSELECT customer_id, DATE_TRUNC('month', order_date), SUM(revenue)\nFROM orders\nGROUP BY 1, 2;\n\n-- Create covering index with ALL 3 needed columns\nCREATE INDEX idx_orders_cust_date_rev ON orders(customer_id, order_date, revenue);\n-- Result: Seq scan on index only \u2014 never touches the table at all</pre>\n<p>At petabyte scale (BigQuery, Redshift), covering indexes are replaced by <strong>column clustering</strong>: physically sorting the table by frequently used filter columns so range scans read contiguous pages. The concept is identical \u2014 reduce pages read for common access patterns.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Sequential scan = read every row. Index scan = jump directly using B-Tree navigation.",
            "B-Tree: default index. Works for =, ranges, BETWEEN, ORDER BY, LIKE 'prefix%'. Not LIKE '%suffix'.",
            "Hash index: only for equality (=). Never for ranges or ordering.",
            "Composite index leftmost prefix rule: index(A,B,C) can be used on A, A+B, A+B+C \u2014 NOT B alone.",
            "Selectivity: high cardinality = useful index. Low cardinality (boolean, status) = index often ignored.",
            "Covering index: all queried columns in the index \u2192 index-only scan, never touches table heap.",
            "Index trade-off: speeds reads, slows writes. Every INSERT/UPDATE/DELETE must maintain all indexes.",
            "Partial index: index only rows matching a condition. Smaller, faster for queries that always filter on that condition."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create an orders table with 50,000 rows and no index. Run EXPLAIN on a WHERE order_date query. Note 'Seq Scan'.",
            "<strong>Step 2:</strong> Add a B-Tree index on order_date. Re-run the same EXPLAIN. Note the plan change to 'Index Scan'.",
            "<strong>Step 3:</strong> Create a composite index on (customer_id, order_date). Test which of these 3 queries use it: (1) WHERE customer_id=42, (2) WHERE order_date>'2024-01-01', (3) WHERE customer_id=42 AND order_date>'2024-01-01'.",
            "<strong>Step 4:</strong> Design a covering index for: SELECT customer_id, SUM(revenue) FROM orders WHERE customer_id=42 GROUP BY customer_id."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Explain B-Tree vs Hash Index on paper",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Explain B-Tree vs Hash Index on paper \u2014 trace through 2-3 edge cases before writing any code.",
                "Add indexes on: WHERE columns, JOIN ON columns, ORDER BY columns of frequent slow queries.",
                "Low-cardinality columns (gender, boolean) get bad indexes. Check selectivity before indexing.",
                "Composite index: put the most-selective column first (highest cardinality) to prune the tree fastest.",
                "Missing covering index? Add the remaining SELECT columns as the trailing part of the index."
            ]
        },
        "HardProblem": "Boss Problem (Uber): You have a rides table: rides(ride_id, driver_id, rider_id, start_ts, end_ts, ride_status, city, fare). The following 3 queries are all slow on 5 billion rows: (Q1) WHERE city='NYC' AND ride_status='completed' AND start_ts > NOW()-30d; (Q2) WHERE driver_id=X ORDER BY start_ts DESC LIMIT 10; (Q3) SELECT city, AVG(fare) GROUP BY city WHERE ride_status='completed'. Design the optimal index for each query. Can one composite index serve all three? What are the trade-offs?"
    },
    {
        "Week": 2,
        "Day": "Thursday",
        "Phase": "1. Filter (Coding)",
        "Theme": "SQL Optimization",
        "SpecificTopic": "Joins Deep Dive",
        "ActionItem_Deliverable": "Draw diagram of Broadcast vs Shuffle Hash Join",
        "LeetCodeProblem": "<strong>LC 185 \u2013 Dept Top 3 Salaries</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": "Done",
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Mock Assessment</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 How Joins Actually Work \u2014 Not Just Syntax</div>\n<div class=\"rich\">\n<h4>What a JOIN Really Does</h4>\n<p>Most engineers think of a JOIN as \"connecting two tables.\" That's accurate but incomplete. Precisely: a JOIN produces every combination of rows from two tables that satisfies a matching condition. The database doesn't \"link\" rows \u2014 it tests each row from the left table against each row from the right table and keeps the pairs that match. Understanding this process explains why joins on large tables are expensive and how different join types produce different results.</p>\n<h4>The 3 Physical Join Algorithms (What the Engine Does)</h4>\n<p><strong>1. Nested Loop Join:</strong> For each row in the outer table, scan the inner table for matches. O(N\u00d7M) complexity. Fast when outer table is tiny and inner table is indexed. Terrible on two large un-indexed tables \u2014 1M \u00d7 1M = 1 trillion comparisons.</p>\n<p><strong>2. Hash Join:</strong> Build a hash table from the smaller table (using the join key). Then scan the larger table, looking up each row in the hash table. O(N+M) complexity. The go-to algorithm for large tables. Requires enough memory to hold the hash table \u2014 can spill to disk if too large.</p>\n<p><strong>3. Sort-Merge Join:</strong> Sort both tables by the join key, then merge-scan them in parallel (like merging two sorted arrays). O(N log N + M log M). Excellent when both tables are already sorted (e.g., both clustered by the join key). Very common in data warehouse query engines.</p>\n<pre>Optimizer choice rule of thumb:\n  Small + Large table with index on join key \u2192 Nested Loop\n  Two large tables (unsorted) \u2192 Hash Join\n  Two large tables (already sorted / clustered) \u2192 Sort-Merge</pre>\n<h4>INNER vs LEFT vs FULL OUTER \u2014 When Each Row Appears</h4>\n<pre>Table A:                   Table B:\nid \u2502 name               id \u2502 score\n\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500              \u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\n 1 \u2502 Alice               1 \u2502  90\n 2 \u2502 Bob                 3 \u2502  75     \u2190 note: no id=2 in B, no id=4 in A\n 4 \u2502 Dave                5 \u2502  80     \u2190 note: no id=5 in A\n\nINNER JOIN (id match required in BOTH):\n  Alice(1) + 90    \u2190 match\n  [Bob(2) dropped \u2014 no score]\n  [Dave(4) dropped \u2014 no score]\n  [score(5) dropped \u2014 no name]\n\nLEFT JOIN (ALL left rows kept, NULLs if no match):\n  Alice(1) + 90\n  Bob(2)   + NULL   \u2190 kept! Bob has no score yet\n  Dave(4)  + NULL\n\nFULL OUTER JOIN (ALL rows from BOTH sides):\n  Alice(1) + 90\n  Bob(2)   + NULL\n  Dave(4)  + NULL\n  NULL     + 80     \u2190 score for id=5, no matching person!</pre>\n<p>\u270d\ufe0f <strong>Interview insight:</strong> LEFT JOIN is the most misunderstood join. Interviewers frequently ask \"find all users who have NOT made a purchase\" \u2014 the correct pattern is LEFT JOIN + WHERE right_table.id IS NULL, not a subquery.</p>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 NULL \u2014 The Three-Valued Logic Problem</div>\n<div class=\"rich\">\n<h4>NULL is Not Zero, Not Empty String, Not False \u2014 It's Unknown</h4>\n<p>NULL represents \"the value is unknown or does not exist.\" This creates a <strong>three-valued logic system</strong> in SQL: TRUE, FALSE, and UNKNOWN. Any comparison with NULL returns UNKNOWN \u2014 not TRUE or FALSE. UNKNOWN in a WHERE clause means the row is silently dropped. This is the #1 source of subtle bugs in SQL.</p>\n<pre>-- All of these return UNKNOWN (not TRUE, not FALSE!):\nNULL = NULL     \u2192 UNKNOWN   -- two unknowns are not equal\nNULL &lt;&gt; NULL    \u2192 UNKNOWN\nNULL + 5        \u2192 NULL      -- any arithmetic with NULL = NULL\n'text' || NULL  \u2192 NULL      -- any concatenation with NULL = NULL\nNULL = 0        \u2192 UNKNOWN\nNULL = ''       \u2192 UNKNOWN\n\n-- Only these work correctly for NULL:\nNULL IS NULL    \u2192 TRUE\nNULL IS NOT NULL \u2192 FALSE\nCOALESCE(NULL, 42) \u2192 42     -- return first non-NULL value</pre>\n<h4>The Anti-Pattern That Loses Data Silently</h4>\n<pre>-- \u274c WRONG: this query returns 0 rows if ANY salary is NULL\nSELECT emp_name FROM employees WHERE salary &lt;&gt; 90000;\n-- reason: WHERE salary &lt;&gt; 90000 is UNKNOWN for NULL rows \u2192 row dropped\n\n-- \u2705 CORRECT: explicitly handle NULLs\nSELECT emp_name FROM employees\nWHERE salary &lt;&gt; 90000 OR salary IS NULL;\n\n-- \u274c WRONG: COUNT(*) vs COUNT(col) confusion\nSELECT COUNT(*),       -- counts every row including NULLs\n       COUNT(salary)   -- counts only non-NULL salary rows\nFROM employees;        -- these two numbers will differ if salary has NULLs!</pre>\n<h4>COALESCE and NULLIF \u2014 Your NULL Safety Tools</h4>\n<pre>-- COALESCE: return first non-NULL value (chain of fallbacks)\nSELECT COALESCE(phone, mobile, 'No contact') AS best_contact FROM users;\n\n-- NULLIF: return NULL if two values are equal\n-- Use to prevent division by zero:\nSELECT revenue / NULLIF(sessions, 0) AS revenue_per_session FROM daily_stats;\n-- If sessions=0, NULLIF returns NULL \u2192 division by NULL \u2192 NULL (not error)</pre>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Advanced: ANTI JOIN, CROSS JOIN, SELF JOIN</div>\n<div class=\"rich\">\n<h4>Three Underused but Important Join Patterns</h4>\n<p><strong>ANTI JOIN</strong> \u2014 find rows in A with NO match in B. The go-to pattern for \"unmatched\" questions:</p>\n<pre>-- Find customers who have never placed an order\n-- Pattern 1: LEFT JOIN + NULL check (most common)\nSELECT c.customer_id, c.name\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id\nWHERE o.customer_id IS NULL;   -- \u2190 NULL means no matching order existed\n\n-- Pattern 2: NOT EXISTS (sometimes faster \u2014 stops at first match found)\nSELECT c.customer_id, c.name FROM customers c\nWHERE NOT EXISTS (SELECT 1 FROM orders o WHERE o.customer_id = c.customer_id);\n\n-- Pattern 3: NOT IN (\u26a0\ufe0f dangerous with NULLs!)\n-- If ANY order has customer_id = NULL, NOT IN returns ZERO rows!\nSELECT customer_id FROM customers\nWHERE customer_id NOT IN (SELECT customer_id FROM orders);  -- risky!</pre>\n<p><strong>SELF JOIN</strong> \u2014 join a table to itself. Classic use: find employee-manager pairs from a single employees table:</p>\n<pre>SELECT e.emp_name AS employee, m.emp_name AS manager\nFROM employees e\nLEFT JOIN employees m ON e.manager_id = m.emp_id;\n-- Same table used twice with different aliases</pre>\n<p><strong>CROSS JOIN</strong> \u2014 every combination of rows (Cartesian product). Useful for generating test data or all pair combinations. Use with extreme caution \u2014 1,000 \u00d7 1,000 = 1,000,000 rows:</p>\n<pre>-- Generate all size \u00d7 color combinations for a product catalog\nSELECT s.size, c.color FROM sizes s CROSS JOIN colors c;\n-- 5 sizes \u00d7 8 colors = 40 combinations</pre>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 FAANG Interview: The Duplicate Join & Fanout Problem</div>\n<div class=\"rich\">\n<h4>The Silent Data Explosion \u2014 Many-to-Many Joins</h4>\n<p>One of the most common FAANG interview mistakes: joining two tables where the join key is NOT unique in both, producing unexpected row multiplication. This is called <strong>join fanout</strong>.</p>\n<pre>-- orders: one row per order (order_id is unique)\n-- order_items: multiple rows per order (each item is a row)\n-- Mistake: SUM on the wrong side causes double-counting!\n\nSELECT o.order_id, SUM(o.order_total) AS wrong_total\nFROM orders o\nJOIN order_items oi ON o.order_id = oi.order_id\nGROUP BY o.order_id;\n-- \u274c If an order has 3 items, order_total is summed 3 times!\n\n-- Fix 1: aggregate items first, then join\nWITH item_counts AS (\n  SELECT order_id, COUNT(*) AS item_count, SUM(item_price) AS items_total\n  FROM order_items GROUP BY order_id\n)\nSELECT o.order_id, o.order_total, ic.item_count\nFROM orders o JOIN item_counts ic ON o.order_id = ic.order_id;\n\n-- Fix 2: detect unexpected fanout with COUNT check\nSELECT order_id, COUNT(*) AS row_count\nFROM (SELECT o.*, oi.item_id FROM orders o JOIN order_items oi USING (order_id)) t\nGROUP BY order_id HAVING COUNT(*) &gt; 1;\n-- Any order_id with count &gt; 1 means join is multiplying rows</pre>\n<p><strong>Interview rule:</strong> Before writing any JOIN query in an interview, state out loud: \"Let me verify the cardinality of both sides of the join key. Is order_id unique in the orders table? Could there be multiple rows in order_items per order_id?\" Mentioning this thought process alone impresses interviewers.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Nested Loop: O(N\u00d7M), fast when outer is small + inner is indexed. Hash Join: O(N+M), best for large tables.",
            "INNER JOIN: only rows with matches in BOTH tables. LEFT JOIN: ALL left rows, NULLs for unmatched right.",
            "FULL OUTER JOIN: ALL rows from both sides, NULLs where no match on either side.",
            "NULL = three-valued logic: TRUE / FALSE / UNKNOWN. Any comparison with NULL = UNKNOWN = row dropped.",
            "Use IS NULL / IS NOT NULL, never = NULL or &lt;&gt; NULL. COALESCE for defaults. NULLIF for zero-division safety.",
            "ANTI JOIN pattern: LEFT JOIN + WHERE right.id IS NULL. Use NOT EXISTS for performance on large tables.",
            "NOT IN with NULLs: dangerous. If subquery returns ANY NULL, NOT IN returns zero rows \u2014 silent bug.",
            "Join fanout: if join key is not unique on both sides, rows multiply. Always verify cardinality before joining."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create customers (5 rows) and orders (3 rows, 2 customers unordered). Write LEFT JOIN + WHERE IS NULL to find unordered customers. Verify 2 results.",
            "<strong>Step 2:</strong> Test NULL logic: SELECT NULL = NULL, NULL &lt;&gt; 5, COALESCE(NULL, NULL, 42). Verify the outputs.",
            "<strong>Step 3:</strong> Create a many-to-many join scenario (orders \u00d7 order_items). Write the broken SUM query, verify it double-counts. Fix it with a pre-aggregated CTE.",
            "<strong>Step 4:</strong> Write a self-join to show each employee and their manager's name from a single employees table."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Draw diagram of Broadcast vs Shuffle Hash Join",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Draw diagram of Broadcast vs Shuffle Hash Join \u2014 trace through 2-3 edge cases before writing any code.",
                "'Find users who never did X': LEFT JOIN + WHERE right.key IS NULL. Don't use NOT IN if NULLs possible.",
                "COUNT(*) counts all rows including NULLs. COUNT(col) counts only non-NULL values. Know the difference.",
                "Join producing too many rows? Check if the join key is unique in both tables. Aggregate smaller side first.",
                "COALESCE(a, b, c): returns the first non-NULL. Perfect for fallback chains and report formatting."
            ]
        },
        "HardProblem": "Boss Problem (Stripe): Table: transactions(txn_id, user_id, amount, status, created_at). Another table: refunds(refund_id, txn_id, refund_amount, created_at). (1) Find all transactions that were partially refunded (refund_amount < transaction amount). (2) Find transactions with NO refund. (3) Find users who had more than 3 refunds in the last 30 days \u2014 flag them as fraud risk. Handle: a transaction can have multiple refunds. A refund can reference a NULL txn_id (orphaned refund). Does NULL in txn_id affect your NOT IN query?"
    },
    {
        "Week": 2,
        "Day": "Friday",
        "Phase": "1. Filter (Coding)",
        "Theme": "SQL Optimization",
        "SpecificTopic": "NULL Handling",
        "ActionItem_Deliverable": "Fix a query where NULL=NULL causes errors (IS DISTINCT FROM)",
        "LeetCodeProblem": "<strong>LC 183 \u2013 Customers Who Never Order</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": "Done",
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Execution Plans</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 How Joins Actually Work \u2014 Not Just Syntax</div>\n<div class=\"rich\">\n<h4>What a JOIN Really Does</h4>\n<p>Most engineers think of a JOIN as \"connecting two tables.\" That's accurate but incomplete. Precisely: a JOIN produces every combination of rows from two tables that satisfies a matching condition. The database doesn't \"link\" rows \u2014 it tests each row from the left table against each row from the right table and keeps the pairs that match. Understanding this process explains why joins on large tables are expensive and how different join types produce different results.</p>\n<h4>The 3 Physical Join Algorithms (What the Engine Does)</h4>\n<p><strong>1. Nested Loop Join:</strong> For each row in the outer table, scan the inner table for matches. O(N\u00d7M) complexity. Fast when outer table is tiny and inner table is indexed. Terrible on two large un-indexed tables \u2014 1M \u00d7 1M = 1 trillion comparisons.</p>\n<p><strong>2. Hash Join:</strong> Build a hash table from the smaller table (using the join key). Then scan the larger table, looking up each row in the hash table. O(N+M) complexity. The go-to algorithm for large tables. Requires enough memory to hold the hash table \u2014 can spill to disk if too large.</p>\n<p><strong>3. Sort-Merge Join:</strong> Sort both tables by the join key, then merge-scan them in parallel (like merging two sorted arrays). O(N log N + M log M). Excellent when both tables are already sorted (e.g., both clustered by the join key). Very common in data warehouse query engines.</p>\n<pre>Optimizer choice rule of thumb:\n  Small + Large table with index on join key \u2192 Nested Loop\n  Two large tables (unsorted) \u2192 Hash Join\n  Two large tables (already sorted / clustered) \u2192 Sort-Merge</pre>\n<h4>INNER vs LEFT vs FULL OUTER \u2014 When Each Row Appears</h4>\n<pre>Table A:                   Table B:\nid \u2502 name               id \u2502 score\n\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500              \u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\n 1 \u2502 Alice               1 \u2502  90\n 2 \u2502 Bob                 3 \u2502  75     \u2190 note: no id=2 in B, no id=4 in A\n 4 \u2502 Dave                5 \u2502  80     \u2190 note: no id=5 in A\n\nINNER JOIN (id match required in BOTH):\n  Alice(1) + 90    \u2190 match\n  [Bob(2) dropped \u2014 no score]\n  [Dave(4) dropped \u2014 no score]\n  [score(5) dropped \u2014 no name]\n\nLEFT JOIN (ALL left rows kept, NULLs if no match):\n  Alice(1) + 90\n  Bob(2)   + NULL   \u2190 kept! Bob has no score yet\n  Dave(4)  + NULL\n\nFULL OUTER JOIN (ALL rows from BOTH sides):\n  Alice(1) + 90\n  Bob(2)   + NULL\n  Dave(4)  + NULL\n  NULL     + 80     \u2190 score for id=5, no matching person!</pre>\n<p>\u270d\ufe0f <strong>Interview insight:</strong> LEFT JOIN is the most misunderstood join. Interviewers frequently ask \"find all users who have NOT made a purchase\" \u2014 the correct pattern is LEFT JOIN + WHERE right_table.id IS NULL, not a subquery.</p>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 NULL \u2014 The Three-Valued Logic Problem</div>\n<div class=\"rich\">\n<h4>NULL is Not Zero, Not Empty String, Not False \u2014 It's Unknown</h4>\n<p>NULL represents \"the value is unknown or does not exist.\" This creates a <strong>three-valued logic system</strong> in SQL: TRUE, FALSE, and UNKNOWN. Any comparison with NULL returns UNKNOWN \u2014 not TRUE or FALSE. UNKNOWN in a WHERE clause means the row is silently dropped. This is the #1 source of subtle bugs in SQL.</p>\n<pre>-- All of these return UNKNOWN (not TRUE, not FALSE!):\nNULL = NULL     \u2192 UNKNOWN   -- two unknowns are not equal\nNULL &lt;&gt; NULL    \u2192 UNKNOWN\nNULL + 5        \u2192 NULL      -- any arithmetic with NULL = NULL\n'text' || NULL  \u2192 NULL      -- any concatenation with NULL = NULL\nNULL = 0        \u2192 UNKNOWN\nNULL = ''       \u2192 UNKNOWN\n\n-- Only these work correctly for NULL:\nNULL IS NULL    \u2192 TRUE\nNULL IS NOT NULL \u2192 FALSE\nCOALESCE(NULL, 42) \u2192 42     -- return first non-NULL value</pre>\n<h4>The Anti-Pattern That Loses Data Silently</h4>\n<pre>-- \u274c WRONG: this query returns 0 rows if ANY salary is NULL\nSELECT emp_name FROM employees WHERE salary &lt;&gt; 90000;\n-- reason: WHERE salary &lt;&gt; 90000 is UNKNOWN for NULL rows \u2192 row dropped\n\n-- \u2705 CORRECT: explicitly handle NULLs\nSELECT emp_name FROM employees\nWHERE salary &lt;&gt; 90000 OR salary IS NULL;\n\n-- \u274c WRONG: COUNT(*) vs COUNT(col) confusion\nSELECT COUNT(*),       -- counts every row including NULLs\n       COUNT(salary)   -- counts only non-NULL salary rows\nFROM employees;        -- these two numbers will differ if salary has NULLs!</pre>\n<h4>COALESCE and NULLIF \u2014 Your NULL Safety Tools</h4>\n<pre>-- COALESCE: return first non-NULL value (chain of fallbacks)\nSELECT COALESCE(phone, mobile, 'No contact') AS best_contact FROM users;\n\n-- NULLIF: return NULL if two values are equal\n-- Use to prevent division by zero:\nSELECT revenue / NULLIF(sessions, 0) AS revenue_per_session FROM daily_stats;\n-- If sessions=0, NULLIF returns NULL \u2192 division by NULL \u2192 NULL (not error)</pre>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Advanced: ANTI JOIN, CROSS JOIN, SELF JOIN</div>\n<div class=\"rich\">\n<h4>Three Underused but Important Join Patterns</h4>\n<p><strong>ANTI JOIN</strong> \u2014 find rows in A with NO match in B. The go-to pattern for \"unmatched\" questions:</p>\n<pre>-- Find customers who have never placed an order\n-- Pattern 1: LEFT JOIN + NULL check (most common)\nSELECT c.customer_id, c.name\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id\nWHERE o.customer_id IS NULL;   -- \u2190 NULL means no matching order existed\n\n-- Pattern 2: NOT EXISTS (sometimes faster \u2014 stops at first match found)\nSELECT c.customer_id, c.name FROM customers c\nWHERE NOT EXISTS (SELECT 1 FROM orders o WHERE o.customer_id = c.customer_id);\n\n-- Pattern 3: NOT IN (\u26a0\ufe0f dangerous with NULLs!)\n-- If ANY order has customer_id = NULL, NOT IN returns ZERO rows!\nSELECT customer_id FROM customers\nWHERE customer_id NOT IN (SELECT customer_id FROM orders);  -- risky!</pre>\n<p><strong>SELF JOIN</strong> \u2014 join a table to itself. Classic use: find employee-manager pairs from a single employees table:</p>\n<pre>SELECT e.emp_name AS employee, m.emp_name AS manager\nFROM employees e\nLEFT JOIN employees m ON e.manager_id = m.emp_id;\n-- Same table used twice with different aliases</pre>\n<p><strong>CROSS JOIN</strong> \u2014 every combination of rows (Cartesian product). Useful for generating test data or all pair combinations. Use with extreme caution \u2014 1,000 \u00d7 1,000 = 1,000,000 rows:</p>\n<pre>-- Generate all size \u00d7 color combinations for a product catalog\nSELECT s.size, c.color FROM sizes s CROSS JOIN colors c;\n-- 5 sizes \u00d7 8 colors = 40 combinations</pre>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 FAANG Interview: The Duplicate Join & Fanout Problem</div>\n<div class=\"rich\">\n<h4>The Silent Data Explosion \u2014 Many-to-Many Joins</h4>\n<p>One of the most common FAANG interview mistakes: joining two tables where the join key is NOT unique in both, producing unexpected row multiplication. This is called <strong>join fanout</strong>.</p>\n<pre>-- orders: one row per order (order_id is unique)\n-- order_items: multiple rows per order (each item is a row)\n-- Mistake: SUM on the wrong side causes double-counting!\n\nSELECT o.order_id, SUM(o.order_total) AS wrong_total\nFROM orders o\nJOIN order_items oi ON o.order_id = oi.order_id\nGROUP BY o.order_id;\n-- \u274c If an order has 3 items, order_total is summed 3 times!\n\n-- Fix 1: aggregate items first, then join\nWITH item_counts AS (\n  SELECT order_id, COUNT(*) AS item_count, SUM(item_price) AS items_total\n  FROM order_items GROUP BY order_id\n)\nSELECT o.order_id, o.order_total, ic.item_count\nFROM orders o JOIN item_counts ic ON o.order_id = ic.order_id;\n\n-- Fix 2: detect unexpected fanout with COUNT check\nSELECT order_id, COUNT(*) AS row_count\nFROM (SELECT o.*, oi.item_id FROM orders o JOIN order_items oi USING (order_id)) t\nGROUP BY order_id HAVING COUNT(*) &gt; 1;\n-- Any order_id with count &gt; 1 means join is multiplying rows</pre>\n<p><strong>Interview rule:</strong> Before writing any JOIN query in an interview, state out loud: \"Let me verify the cardinality of both sides of the join key. Is order_id unique in the orders table? Could there be multiple rows in order_items per order_id?\" Mentioning this thought process alone impresses interviewers.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Nested Loop: O(N\u00d7M), fast when outer is small + inner is indexed. Hash Join: O(N+M), best for large tables.",
            "INNER JOIN: only rows with matches in BOTH tables. LEFT JOIN: ALL left rows, NULLs for unmatched right.",
            "FULL OUTER JOIN: ALL rows from both sides, NULLs where no match on either side.",
            "NULL = three-valued logic: TRUE / FALSE / UNKNOWN. Any comparison with NULL = UNKNOWN = row dropped.",
            "Use IS NULL / IS NOT NULL, never = NULL or &lt;&gt; NULL. COALESCE for defaults. NULLIF for zero-division safety.",
            "ANTI JOIN pattern: LEFT JOIN + WHERE right.id IS NULL. Use NOT EXISTS for performance on large tables.",
            "NOT IN with NULLs: dangerous. If subquery returns ANY NULL, NOT IN returns zero rows \u2014 silent bug.",
            "Join fanout: if join key is not unique on both sides, rows multiply. Always verify cardinality before joining."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create customers (5 rows) and orders (3 rows, 2 customers unordered). Write LEFT JOIN + WHERE IS NULL to find unordered customers. Verify 2 results.",
            "<strong>Step 2:</strong> Test NULL logic: SELECT NULL = NULL, NULL &lt;&gt; 5, COALESCE(NULL, NULL, 42). Verify the outputs.",
            "<strong>Step 3:</strong> Create a many-to-many join scenario (orders \u00d7 order_items). Write the broken SUM query, verify it double-counts. Fix it with a pre-aggregated CTE.",
            "<strong>Step 4:</strong> Write a self-join to show each employee and their manager's name from a single employees table."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Fix a query where NULL=NULL causes errors (IS DISTINCT FROM)",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Fix a query where NULL=NULL causes errors (IS DISTINCT FROM) \u2014 trace through 2-3 edge cases before writing any code.",
                "'Find users who never did X': LEFT JOIN + WHERE right.key IS NULL. Don't use NOT IN if NULLs possible.",
                "COUNT(*) counts all rows including NULLs. COUNT(col) counts only non-NULL values. Know the difference.",
                "Join producing too many rows? Check if the join key is unique in both tables. Aggregate smaller side first.",
                "COALESCE(a, b, c): returns the first non-NULL. Perfect for fallback chains and report formatting."
            ]
        },
        "HardProblem": "Boss Problem (Stripe): Table: transactions(txn_id, user_id, amount, status, created_at). Another table: refunds(refund_id, txn_id, refund_amount, created_at). (1) Find all transactions that were partially refunded (refund_amount < transaction amount). (2) Find transactions with NO refund. (3) Find users who had more than 3 refunds in the last 30 days \u2014 flag them as fraud risk. Handle: a transaction can have multiple refunds. A refund can reference a NULL txn_id (orphaned refund). Does NULL in txn_id affect your NOT IN query?"
    },
    {
        "Week": 2,
        "Day": "Saturday",
        "Phase": "1. Filter (Coding)",
        "Theme": "SQL Optimization",
        "SpecificTopic": "Mock Assessment",
        "ActionItem_Deliverable": "2 Hard SQL Problems (60 mins timer)",
        "LeetCodeProblem": "<strong>LC 262 \u2013 Trips and Users</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Lead/Lag Pattern</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 The Problem Window Functions Solve</div>\n<div class=\"rich\">\n<h4>Start Here \u2014 Before Any Code</h4>\n<p>You're a data analyst at Spotify. Your manager asks: <em>\"Show me each employee's salary AND the average salary for their department \u2014 on the same row.\"</em></p>\n<p>Your first instinct: <code>GROUP BY department</code>. Problem: <strong>GROUP BY collapses rows</strong>. You get one row per department, losing individual employee detail. You can't have both on the same row \u2014 unless you use a slow self-join. This is exactly the gap <strong>Window Functions</strong> fill: they compute aggregates across related rows while <strong>keeping every row in the output</strong>.</p>\n<h4>The Mental Model: A Sliding Frame of Glass</h4>\n<p>Imagine placing a sheet of glass over a spreadsheet. For each row, the database looks through that glass at a \"window\" of rows, performs a calculation, and writes the result back into a new column \u2014 without removing the original row.</p>\n<pre>GROUP BY result:              Window Function result:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 dept        \u2502 avg_sal \u2502    \u2502 name  \u2502 dept        \u2502 salary \u2502 dept_avg \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Engineering \u2502  90,000 \u2502    \u2502 Alice \u2502 Engineering \u2502 95,000 \u2502  90,000  \u2502\n\u2502 Marketing   \u2502  72,500 \u2502    \u2502 Bob   \u2502 Engineering \u2502 85,000 \u2502  90,000  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 Eve   \u2502 Engineering \u2502 90,000 \u2502  90,000  \u2502\n2 rows \u2014 detail lost!         \u2502 Carol \u2502 Marketing   \u2502 70,000 \u2502  72,500  \u2502\n                              \u2502 Dave  \u2502 Marketing   \u2502 75,000 \u2502  72,500  \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              5 rows \u2014 ALL detail preserved \u2705</pre>\n<p>\u270d\ufe0f <strong>Write this down:</strong> Window functions NEVER reduce row count. They only ADD new computed columns. The <code>OVER()</code> keyword is the signal that tells the database \"this is a window function.\" Without OVER(), AVG() collapses rows. With OVER(), it keeps all rows.</p>\n<h4>The Syntax Structure</h4>\n<pre>FUNCTION_NAME() OVER (\n    PARTITION BY column   -- which rows form the window for each row\n    ORDER BY column       -- sort order within the window\n    ROWS BETWEEN ...      -- optional: how many surrounding rows to include\n)</pre>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 ROW_NUMBER \u2014 Step by Step with Full Explanation</div>\n<div class=\"rich\">\n<h4>What ROW_NUMBER() Does</h4>\n<p><code>ROW_NUMBER()</code> assigns a unique sequential integer to each row within a window, sorted as you specify. It ALWAYS gives unique numbers \u2014 even if two rows are completely identical. This makes it perfect for deduplication: you can always pick row #1 per group and discard the rest.</p>\n<h4>Step 1 \u2014 Create and populate the table</h4>\n<pre>CREATE TABLE employees (\n  emp_id   INT,\n  emp_name VARCHAR(50),\n  dept     VARCHAR(50),\n  salary   INT\n);\nINSERT INTO employees VALUES\n  (1, 'Alice', 'Engineering', 95000),\n  (2, 'Bob',   'Engineering', 85000),\n  (3, 'Carol', 'Marketing',   70000),\n  (4, 'Dave',  'Marketing',   75000),\n  (5, 'Eve',   'Engineering', 90000);</pre>\n<h4>Step 2 \u2014 Run your first window function</h4>\n<pre>SELECT\n  emp_name, dept, salary,\n  ROW_NUMBER() OVER (\n    PARTITION BY dept      -- restart numbering for each department\n    ORDER BY salary DESC   -- highest salary = rank 1\n  ) AS rank_in_dept\nFROM employees;</pre>\n<h4>Step 3 \u2014 Trace the output and understand WHY</h4>\n<pre>emp_name \u2502 dept        \u2502 salary \u2502 rank_in_dept\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nAlice    \u2502 Engineering \u2502  95000 \u2502      1    \u2190 highest in Eng \u2192 rank 1\nEve      \u2502 Engineering \u2502  90000 \u2502      2    \u2190 2nd in Eng\nBob      \u2502 Engineering \u2502  85000 \u2502      3    \u2190 3rd in Eng\nDave     \u2502 Marketing   \u2502  75000 \u2502      1    \u2190 RESTARTS! Highest in Marketing\nCarol    \u2502 Marketing   \u2502  70000 \u2502      2    \u2190 2nd in Marketing</pre>\n<p>Dave gets rank 1 even though he earns less than Bob \u2014 because PARTITION BY dept created a completely separate window for Marketing. Dave is simply the top earner within his own department's window.</p>\n<h4>What the Engine Does Internally</h4>\n<ol>\n  <li>Read all 5 rows from the table</li>\n  <li>Split into partitions: Engineering (3 rows), Marketing (2 rows)</li>\n  <li>Sort each partition by salary DESC</li>\n  <li>Assign row numbers 1,2,3... within each sorted partition</li>\n  <li>Attach numbers back to original rows and return all 5 rows</li>\n</ol>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 RANK vs DENSE_RANK \u2014 Why Ties Change Everything</div>\n<div class=\"rich\">\n<h4>The Problem with Ties in Real Data</h4>\n<p>In practice, duplicate values are common: two products with the same rating, two employees with the same salary, two cities with equal population. The three ranking functions handle ties differently, and choosing the wrong one produces wrong business results.</p>\n<pre>-- Add Frank \u2014 same salary as Eve\nINSERT INTO employees VALUES (6, 'Frank', 'Engineering', 90000);\n\nSELECT emp_name, salary,\n  ROW_NUMBER() OVER (ORDER BY salary DESC) AS row_num,\n  RANK()       OVER (ORDER BY salary DESC) AS rnk,\n  DENSE_RANK() OVER (ORDER BY salary DESC) AS dense_rnk\nFROM employees WHERE dept = 'Engineering';</pre>\n<pre>emp_name \u2502 salary \u2502 row_num \u2502 rnk \u2502 dense_rnk\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nAlice    \u2502  95000 \u2502    1    \u2502  1  \u2502     1\nEve      \u2502  90000 \u2502    2    \u2502  2  \u2502     2    \u2190 tied with Frank\nFrank    \u2502  90000 \u2502    3    \u2502  2  \u2502     2    \u2190 tied with Eve\nBob      \u2502  85000 \u2502    4    \u2502  4  \u2502     3    \u2190 RANK skips 3, DENSE_RANK doesn't</pre>\n<p>Bob gets RANK 4 because ranks 1, 2, 2 are taken \u2014 the number 3 is \"skipped\" to reflect that two people occupy the 2nd position. DENSE_RANK gives Bob a 3 with no gap.</p>\n<h4>Choosing the Right Function</h4>\n<table>\n<tr><th>Need exactly one result per group (dedup)?</th><td>ROW_NUMBER()</td></tr>\n<tr><th>Gap after tie is meaningful (sports: no 3rd if two share 2nd)?</th><td>RANK()</td></tr>\n<tr><th>Sequential tiers without gaps (medals, grade bands)?</th><td>DENSE_RANK()</td></tr>\n</table>\n<p>\u270d\ufe0f <strong>Interview trap (LeetCode 185):</strong> \"Top 3 salary values per department\" \u2014 ties must BOTH appear. Use DENSE_RANK not ROW_NUMBER. ROW_NUMBER eliminates tied rows arbitrarily \u2014 wrong answer.</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 FAANG Pattern: Top-N Per Group + Scale Secrets</div>\n<div class=\"rich\">\n<h4>The Most Common FAANG Window Function Pattern</h4>\n<p>Top-N per group appears in virtually every FAANG interview. The key constraint: you CANNOT filter on a window function in the same SELECT \u2014 window functions run in step 5 of SQL's execution order, but WHERE runs in step 2. The solution is always a CTE.</p>\n<pre>-- Top 3 earners per department\nWITH ranked AS (\n  SELECT emp_name, dept, salary,\n    DENSE_RANK() OVER (PARTITION BY dept ORDER BY salary DESC) AS rk\n    -- DENSE_RANK: if 2 people tie for 2nd, both appear in top 3\n  FROM employees\n)\nSELECT dept, emp_name, salary\nFROM ranked\nWHERE rk &lt;= 3          -- NOW we can filter because rk is a real column\nORDER BY dept, rk;</pre>\n<h4>SQL Execution Order \u2014 Memorize This</h4>\n<pre>1. FROM       \u2190 identify tables\n2. WHERE      \u2190 filter rows (window functions do NOT exist here yet!)\n3. GROUP BY   \u2190 aggregate\n4. HAVING     \u2190 filter aggregates\n5. SELECT     \u2190 compute expressions \u2014 window functions are evaluated HERE\n6. ORDER BY   \u2190 sort final result\n7. LIMIT      \u2190 restrict rows\n\nThis is why you need a CTE: the outer WHERE sees the CTE's rk column\nbecause by then, the CTE's step 5 has already been executed.</pre>\n<h4>Scale: Why PARTITION BY Key Choice Matters</h4>\n<p>On billions of rows: good PARTITION BY keys (user_id, device_id) spread data across thousands of nodes \u2014 parallel and fast. Bad keys (is_active: TRUE/FALSE) dump 99% of data on one node \u2014 memory crash. Always ask: \"does my partition key distribute data evenly?\"</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Window functions compute over related rows WITHOUT collapsing output \u2014 unlike GROUP BY.",
            "OVER() transforms any aggregate into a window function. Without OVER: GROUP BY. With OVER: window.",
            "PARTITION BY = independent windows per group. Rankings restart per partition.",
            "ORDER BY inside OVER = sort order within partition. Determines which row gets rank 1.",
            "ROW_NUMBER(): always unique. Use for dedup, pagination, keeping exactly 1 row per group.",
            "RANK(): ties share same number, next is SKIPPED (1,2,2,4). Use for sports/competitions.",
            "DENSE_RANK(): ties share same number, no skip (1,2,2,3). Use for tiered rankings. Use for LeetCode 185.",
            "SQL execution order: FROM\u2192WHERE\u2192GROUP BY\u2192HAVING\u2192SELECT(windows here)\u2192ORDER BY\u2192LIMIT.",
            "Cannot use window function result in WHERE. Always wrap in CTE first."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create employees table and insert 5 rows. Run the ROW_NUMBER query. Explain in writing why Dave gets rank 1.",
            "<strong>Step 2:</strong> Add Frank (Engineering, 90000) and run all 3 ranking functions. Write the difference you see in Bob's row.",
            "<strong>Step 3:</strong> Write the DENSE_RANK top-3-per-department query from scratch without looking. Test it shows both Eve and Frank when tied.",
            "<strong>Step 4:</strong> Try adding WHERE rank_in_dept &lt;= 3 directly in the window query (no CTE). Note the error. Then fix it with a CTE."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> 2 Hard SQL Problems (60 mins timer)",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> 2 Hard SQL Problems (60 mins timer) \u2014 trace through 2-3 edge cases before writing any code.",
                "'Per group' or 'within each category' \u2192 your signal to use PARTITION BY.",
                "LeetCode 185 trap: 'top 3 salaries' not 'top 3 employees' \u2192 DENSE_RANK so ties both show.",
                "Can't use window function in WHERE? Wrap in CTE \u2014 this is always the fix.",
                "High-cardinality PARTITION BY (user_id) = good parallelism. Low-cardinality (boolean) = hotspot."
            ]
        },
        "HardProblem": "Boss Problem (Meta): 5-billion row table: user_events(user_id, event_type, revenue, event_ts). (1) Rank each user's events by revenue using DENSE_RANK. (2) Return only top 3 per user. (3) Add running total revenue per user. Explain: Which PARTITION BY key? What happens memory-wise with no PARTITION BY on 5B rows? How would you run this in Spark?"
    },
    {
        "Week": 2,
        "Day": "Sunday",
        "Phase": "1. Filter (Coding)",
        "Theme": "Rest",
        "SpecificTopic": "Rest & Review",
        "ActionItem_Deliverable": "Review Recursive CTE syntax",
        "LeetCodeProblem": null,
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>NULL Handling</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Active Rest \u2014 How Memory Consolidates</div>\n<div class=\"rich\">\n<h4>The Neuroscience of Spaced Repetition</h4>\n<p>Rest days are not wasted days. During rest, your brain replays what it learned \u2014 a process called <strong>memory consolidation</strong>. Research consistently shows that <em>attempting to recall from memory</em> is 3\u00d7 more effective at building durable knowledge than re-reading notes. The act of struggling to remember \u2014 even failing \u2014 strengthens the neural pathway more than passive review.</p>\n<p>Think of it like muscle recovery: a weightlifter who trains 7 days straight without rest gets weaker, not stronger. The growth happens during recovery. Your brain is the same.</p>\n<h4>Today's Protocol (4 hours, structured)</h4>\n<ol>\n  <li><strong>Active Recall (60 min):</strong> Close everything. On a blank page, write every concept, formula, and pattern from this week from memory. Then check.</li>\n  <li><strong>Error Review (60 min):</strong> Find the hardest problem you struggled with. Attempt it again completely from scratch \u2014 no notes.</li>\n  <li><strong>Teach It (45 min):</strong> Open the AI chatbot and explain Gaps & Islands to it as if it's a junior engineer. If you stumble, that's your gap.</li>\n  <li><strong>Next Week Preview (15 min):</strong> Skim next Monday's topic at headline level only \u2014 priming improves retention when you study it deeply.</li>\n</ol>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Active Recall Quiz \u2014 No Notes Allowed</div>\n<div class=\"rich\">\n<h4>Answer These From Memory. Then Check.</h4>\n<pre>Window Functions:\n\u25a1 What is the difference between ROW_NUMBER, RANK, and DENSE_RANK?\n  (Write the output for a table with two tied rows)\n\u25a1 Write the syntax for a 7-day rolling average from memory\n\u25a1 What does PARTITION BY do differently from GROUP BY?\n\u25a1 Write the complete Gaps & Islands CTE from memory\n\u25a1 What is the difference between ROWS BETWEEN and RANGE BETWEEN?\n\u25a1 LAG(col, 1, 0) vs LAG(col, 1) \u2014 what is different?\n\u25a1 In what SQL execution order step do window functions run?\n\u25a1 Why can't you write WHERE rank &lt;= 3 in the same query as ROW_NUMBER?\n\nScore yourself:\n8/8 = \ud83c\udfc6 Mastery level \u2014 you're ready for interviews on this\n5-7/8 = \ud83d\udfe1 Good \u2014 review the 1-2 concepts you missed\n&lt; 5/8 = \ud83d\udd34 Go back to the specific day's content, re-read Level 1-2</pre>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Spaced Repetition Schedule</div>\n<div class=\"rich\">\n<h4>When to Review This Week's Material</h4>\n<table>\n<tr><th>Review this content</th><th>When</th><th>How</th></tr>\n<tr><td>This week</td><td>Today</td><td>Active recall quiz above</td></tr>\n<tr><td>This week</td><td>In 3 days</td><td>Re-solve the hard problem from Day 4</td></tr>\n<tr><td>This week</td><td>In 7 days</td><td>LeetCode: solve problems 185, 180, 196</td></tr>\n<tr><td>This week</td><td>In 30 days</td><td>Explain all 4 concepts to someone else</td></tr>\n</table>\n<h4>The Feynman Technique</h4>\n<p>After studying any concept, close your notes and explain it as if you're teaching a newcomer. Use only simple words. Avoid jargon. If you stumble, that's exactly where your understanding breaks down \u2014 return to just that part. This technique built Richard Feynman's legendary ability to explain complex physics simply.</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Interview Readiness Self-Assessment</div>\n<div class=\"rich\">\n<pre>Rate yourself 1\u20135 on each topic (honest self-assessment):\n  1 = \"I need to re-read from scratch\"\n  3 = \"I understand but need to look at notes\"\n  5 = \"I can code this under pressure from memory\"\n\nROW_NUMBER / RANK / DENSE_RANK:        ___/5\nRolling windows (ROWS BETWEEN):        ___/5\nLAG/LEAD (adjacent row comparison):    ___/5\nGaps & Islands (date streaks):         ___/5\nSession detection (timestamp version): ___/5\n\nAction:\n  Score &lt;3: revisit that day's Level 1 and Level 2 content tomorrow\n  Score 3:  do one LeetCode problem on that specific topic\n  Score 5:  you're interview-ready \u2705 on this topic</pre>\n<h4>FAANG Interview Reality Check</h4>\n<p>At meta, Amazon, and Google data engineering interviews, you'll be asked to write SQL live on a shared screen without autocomplete or documentation. The most common reason candidates fail: they know the concept but freeze on the syntax under pressure. The fix: write every pattern from memory at least 5 times this week. Muscle memory is more reliable than conscious recall under stress.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Memory consolidation happens during rest \u2014 the brain replays and stabilizes learned patterns during downtime.",
            "Active recall (testing yourself) builds 3x stronger memory than re-reading notes.",
            "Spaced repetition: review at day 1, day 3, day 7, day 30 for maximum retention.",
            "The Feynman Technique: if you can't explain it simply, you don't fully understand it yet.",
            "Error analysis: the problems you got WRONG are your highest-value review items \u2014 spend the most time there."
        ],
        "Tasks": [
            "<strong>Active Recall:</strong> Write the Gaps & Islands CTE from memory on paper. Check it against Day 4.",
            "<strong>Re-attempt:</strong> Pick the hardest problem from this week and solve it in 20 minutes from scratch.",
            "<strong>Teach it:</strong> Explain RANK vs DENSE_RANK to the AI chatbot as if it's a junior. Get it right without notes.",
            "<strong>Preview:</strong> Skim next week's Monday topic for 10 minutes \u2014 just headline concepts."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Review Recursive CTE syntax",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Review Recursive CTE syntax \u2014 trace through 2-3 edge cases before writing any code.",
                "Use the AI chatbot to quiz yourself \u2014 ask it 'Give me a SQL window function interview question.'",
                "Write SQL from memory in a plain text editor \u2014 no autocomplete. This builds true recall.",
                "The hardest concept this week: spend 30 minutes writing a full explanation in your own words."
            ]
        },
        "HardProblem": "Connect-the-dots: Write 2-3 paragraphs explaining how ROW_NUMBER, rolling averages, LAG, and Gaps & Islands could ALL be used in a single production pipeline for a ride-sharing company. What problem does each solve? How do they complement each other? What would break if you removed any one of them?"
    },
    {
        "Week": 3,
        "Day": "Monday",
        "Phase": "2. Build (Architecture)",
        "Theme": "Data Modeling",
        "SpecificTopic": "Dimensional Modeling Basics",
        "ActionItem_Deliverable": "Draw Star Schema vs Snowflake Schema for Retail",
        "LeetCodeProblem": "<strong>LC 1484 \u2013 Group Sold Products by Date</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": "Done",
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Recursive CTEs</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 The History \u2014 Why This Exists</div><div class=\"rich\"><h4>Why Dimensional Modeling Exists \u2014 The History</h4><p>In the 1960s databases were built for transactions: record an order, update inventory, log a payment. Every table was tightly normalized. Queries were simple lookups. IBM's relational model was perfect for this.</p><p>Then businesses wanted <em>analytics</em>. Not \"what is this customer's balance?\" but \"how did western-region revenue compare to last year, by product category and month?\" These questions required joining 8-12 tables, aggregating millions of rows, slicing across multiple dimensions simultaneously. Normalized OLTP schemas were terrible at this \u2014 joins were slow, SQL was unreadable, answers took hours.</p><p><strong>Ralph Kimball's insight (1990s):</strong> every business question has the same structure \u2014 \"How much [measure] by [dimension] by [dimension]?\" He designed the <strong>Star Schema</strong>: one central fact table of numbers, surrounded by dimension tables of context. The shape is a star. It mirrors how humans think about data.</p><pre>           DIM_PRODUCT             DIM_DATE\n           (category, brand)       (year, month, quarter)\n                    \\\\              /\n       DIM_STORE \u2500\u2500 FACT_SALES \u2500\u2500 DIM_CUSTOMER\n       (city,region)  (revenue,   (segment, age)\n                       units,\n                       discount)\n</pre><p>\u270d\ufe0f <strong>Core rule:</strong> Fact tables contain MEASUREMENTS (numbers you aggregate: revenue, clicks, quantity). Dimension tables contain CONTEXT (who, what, where, when). Quick test: \"Can I SUM this column?\" \u2014 yes \u2192 fact/measure, no \u2192 dimension/attribute.</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Building a Star Schema</div><div class=\"rich\"><h4>Building a Star Schema \u2014 Step by Step</h4><p>The fact table contains one row per business event \u2014 one sale, one click, one payment. It has foreign keys to every dimension (who/what/where/when) and numeric measures.</p><pre>CREATE TABLE fact_sales (\n  sale_id      BIGINT PRIMARY KEY,\n  date_key     INT REFERENCES dim_date(date_key),       -- WHEN\n  product_key  INT REFERENCES dim_product(product_key), -- WHAT\n  store_key    INT REFERENCES dim_store(store_key),      -- WHERE\n  customer_key INT REFERENCES dim_customer(customer_key),-- WHO\n  quantity_sold INT,\n  unit_price    DECIMAL(10,2),\n  total_revenue DECIMAL(10,2)   -- additive: SUM across all dims\n);</pre><h4>The Date Dimension \u2014 Why It's Special</h4><p>The date dimension is pre-populated for 10 years (3,650 rows). It stores year, quarter, month, week, day, holiday/weekend flags so analysts can <code>GROUP BY d.quarter</code> without ever calling EXTRACT(). It is the most important and most universally present dimension in any data warehouse.</p><pre>CREATE TABLE dim_date (\n  date_key    INT PRIMARY KEY,   -- e.g. 20240115\n  full_date   DATE,\n  year INT, quarter INT, month INT, month_name VARCHAR(20),\n  is_holiday BOOLEAN, is_weekend BOOLEAN\n);</pre><p><strong>Surrogate keys:</strong> We use simple integer PKs in dimension tables, not natural business keys. If an upstream product ID format changes, only the ETL mapping changes \u2014 not the entire fact table.</p></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Star vs Snowflake + Grain</div><div class=\"rich\"><h4>Star vs Snowflake Schema \u2014 When to Normalize Dimensions</h4><p><strong>Star schema:</strong> dimension tables are denormalized \u2014 category, subcategory, and brand all live in one flat product dimension. Some string repetition, but JOIN queries are simple (one join per dimension).</p><p><strong>Snowflake schema:</strong> dimension tables are normalized \u2014 dim_product \u2192 dim_subcategory \u2192 dim_category, each linked by FK. Less redundancy, but every analytical query needs 3 joins to get from a sale to its category.</p><table><tr><th>Aspect</th><td>Star Schema</td><td>Snowflake Schema</td></tr><tr><th>Storage</th><td>More (repeated strings)</td><td>Less (normalized)</td></tr><tr><th>Query complexity</th><td>1 join per dimension</td><td>3\u20134 joins per dimension chain</td></tr><tr><th>Query speed</th><td>Faster (fewer joins)</td><td>Slower on large data sets</td></tr><tr><th>FAANG preference</th><td>\u2705 Almost universal</td><td>\u274c Rare \u2014 only for very large dims</td></tr></table><h4>Grain \u2014 The Most Critical Design Decision</h4><p>A fact table's <strong>grain</strong> is the precise definition of what one row represents. Getting grain wrong is the most expensive modeling mistake \u2014 nearly impossible to fix without rebuilding.</p><pre>Grains for sales data:\n  \u2705 \"One row per line item in a sales transaction\" (finest grain)\n  \u2705 \"One row per sales receipt\"\n  \u2705 \"One row per day per store total\"\n  \u274c \"One row per transaction... sometimes per day\" \u2014 MIXED grain!\n\nMixed grain: SUM(revenue) double-counts. Every analyst gets different numbers.\nTrust in the data warehouse collapses.</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 FAANG Interview Framework</div><div class=\"rich\"><h4>The Interview Framework: Designing a Data Warehouse From Scratch</h4><p>FAANG interview: \"Design a data warehouse for an e-commerce company.\" The interviewer scores: (1) grain identified first, (2) facts vs dimensions correct, (3) SCD strategy named, (4) query pattern driven design.</p><pre>Step 1 \u2014 Ask clarifying questions:\n  \"What questions will analysts need to answer?\"\n  \"What is the finest level of detail needed? Line item or order?\"\n  \"How often does product/customer data change?\"\n  \"Expected data volume \u2014 rows per day?\"\n\nStep 2 \u2014 State the grain explicitly:\n  \"One row per order line item\"\n\nStep 3 \u2014 Identify measures:\n  quantity, unit_price, discount, line_total, margin\n\nStep 4 \u2014 Identify dimensions:\n  dim_date (when), dim_customer (who), dim_product (what),\n  dim_store (where), dim_promotion (why discounted)\n\nStep 5 \u2014 Handle attribute changes:\n  \"Products change prices, customers move \u2014 SCD Type 2 on both\"\n\nStep 6 \u2014 Plan aggregations:\n  \"Daily dashboard sums pre-aggregated in agg_daily_sales\n   so we don't scan the 10B-row fact table on every request\"</pre></div></div></div>",
        "KeyConcepts": [
            "Star schema = fact table (measures) surrounded by dimension tables (context). Optimized for analytical queries.",
            "Fact table: events (sales, clicks). Contains FKs to dimensions + numeric measures to aggregate.",
            "Additive measures: SUM across all dims (revenue). Semi-additive: only across some. Non-additive: never SUM (ratios).",
            "Surrogate keys: simple integer PKs. Insulate DW from upstream ID format changes.",
            "Grain: precise definition of one fact row. Must be declared, consistent, never mixed.",
            "Star vs Snowflake: star = denormalized = simpler queries. Snowflake = normalized = more joins.",
            "Date dimension: pre-built, one row per day, 10 years. Year/quarter/month/holiday flags for fast GROUP BY.",
            "Design sequence: grain \u2192 measures \u2192 dimensions \u2192 SCD strategy \u2192 aggregation plan."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Draw on paper: a star schema for a food delivery app. Define the grain. Name all fact measures and all dimensions.",
            "<strong>Step 2:</strong> Write DDL for fact_orders and dim_restaurant. Include surrogate keys and all measures.",
            "<strong>Step 3:</strong> Write a query joining fact_sales \u2192 dim_date \u2192 dim_product to get monthly revenue by category.",
            "<strong>Step 4:</strong> Redesign as a snowflake \u2014 add dim_subcategory and dim_category as separate tables. How many more JOINs does the same query require?"
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Draw Star Schema vs Snowflake Schema for Retail",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Draw Star Schema vs Snowflake Schema for Retail \u2014 trace through 2-3 edge cases before writing any code.",
                "'Can I SUM this column?' \u2192 fact/measure. 'Is this descriptive context?' \u2192 dimension attribute.",
                "Always declare grain explicitly before DDL. Mixed-grain fact tables are the #1 data warehouse bug.",
                "Surrogate keys protect against upstream ID changes. Never use natural source keys as fact table FKs.",
                "Date dimension: analysts GROUP BY d.quarter without EXTRACT() \u2014 huge usability win."
            ]
        },
        "HardProblem": "Boss Problem (Amazon): Design a DW for marketplace analytics. Sellers list products, customers place orders with multiple line items, items can be returned. Design fact + dimension tables for: (1) revenue by seller, product category, date, (2) return rate analysis, (3) seller performance ranking. State grain of each fact table. Handle: products changing categories over time, international orders with currency conversion."
    },
    {
        "Week": 3,
        "Day": "Tuesday",
        "Phase": "2. Build (Architecture)",
        "Theme": "Data Modeling",
        "SpecificTopic": "SCD Types (1, 2, 3)",
        "ActionItem_Deliverable": "Implement SCD Type 2 Merge logic in SQL",
        "LeetCodeProblem": "<strong>LC 1193 - Monthly Transactions 1</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": "Done",
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Mock Assessment</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 The Problem and Three Strategies</div><div class=\"rich\"><h4>The Business Problem That Makes SCDs Hard</h4><p>You've built a star schema. dim_customer has customer_id, name, city, loyalty_tier. Your boss asks: \"Show revenue from gold-tier customers last year \u2014 but I want the tier they had <em>AT THE TIME OF PURCHASE</em>, not what tier they have today.\"</p><p>This reveals the fundamental problem: <strong>the real world changes over time</strong>. Customers move cities. Products change categories. If you simply UPDATE a dimension row, you permanently lose the history. Yesterday's record no longer exists. Historical questions become unanswerable.</p><p>This is the <strong>Slowly Changing Dimension (SCD)</strong> problem. Three strategies dominate:</p><ul><li><strong>SCD Type 1:</strong> Overwrite. No history kept. Simple but lossy.</li><li><strong>SCD Type 2:</strong> Add a new row per change. Full history. The gold standard.</li><li><strong>SCD Type 3:</strong> Add a \"previous value\" column. One level of history.</li></ul><p>\u270d\ufe0f <strong>In 95% of FAANG interviews and DW jobs, SCD means Type 2. Know this one deeply.</strong></p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Type 1, Type 2, Type 3 \u2014 Full Examples</div><div class=\"rich\"><h4>SCD Type 1: Overwrite \u2014 Simple but History-Destroying</h4><pre>-- Alice moves from Boston to Dallas:\nUPDATE dim_customer SET city = 'Dallas' WHERE customer_id = 42;\n-- \u26a0\ufe0f  Boston is GONE. All historical orders now falsely show Dallas.\n-- Use Type 1 ONLY for data corrections (typos) \u2014 never for real changes.</pre><h4>SCD Type 2: New Row Per Change \u2014 Full History Preserved</h4><pre>CREATE TABLE dim_customer (\n  customer_key   INT PRIMARY KEY,  -- surrogate: changes with each version\n  customer_id    INT,              -- natural key: stays the same always\n  name           VARCHAR(100),\n  city           VARCHAR(100),\n  loyalty_tier   VARCHAR(20),\n  effective_start DATE NOT NULL,\n  effective_end   DATE,            -- NULL = currently active\n  is_current      BOOLEAN DEFAULT TRUE\n);\n\n-- Alice starts in Boston, Silver tier:\nINSERT INTO dim_customer VALUES (1001,42,'Alice','Boston','Silver','2023-01-01',NULL,TRUE);\n\n-- Alice moves to Dallas on March 15, 2024:\n-- Step 1: Close old row\nUPDATE dim_customer SET effective_end='2024-03-15', is_current=FALSE\nWHERE customer_id=42 AND is_current=TRUE;\n\n-- Step 2: Insert new row (NEW surrogate key!)\nINSERT INTO dim_customer VALUES (1087,42,'Alice','Dallas','Silver','2024-03-15',NULL,TRUE);\n\n-- Alice now has TWO rows:\n-- key=1001: Boston, Silver, 2023-01-01 \u2192 2024-03-15 (historical)\n-- key=1087: Dallas, Silver, 2024-03-15 \u2192 NULL (current)\n\n-- fact_sales rows from 2023 \u2192 customer_key=1001 (Boston)\n-- fact_sales rows from Apr 2024 \u2192 customer_key=1087 (Dallas)\n-- Revenue by city is now historically accurate \u2705</pre><h4>SCD Type 3: Previous Value Column \u2014 One Level Only</h4><pre>ALTER TABLE dim_customer ADD COLUMN prev_city VARCHAR(100);\nUPDATE dim_customer SET prev_city=city, city='Dallas' WHERE customer_id=42;\n-- city=Dallas, prev_city=Boston\n-- \u26a0\ufe0f  If Alice moves again to Austin: prev_city=Dallas, Boston is gone.\n-- Use for: planned reorgs where you need both old+new for 6 months only.</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Query Patterns and ETL Pipeline</div><div class=\"rich\"><h4>Three Essential SCD Type 2 Query Patterns</h4><p><strong>Pattern 1: Current state</strong></p><pre>SELECT customer_id, name, city, loyalty_tier\nFROM dim_customer WHERE is_current = TRUE;</pre><p><strong>Pattern 2: Historical accuracy \u2014 tier at time of purchase</strong></p><pre>-- Revenue by tier AT time of sale (not current tier)\nSELECT c.loyalty_tier, SUM(f.total_revenue)\nFROM fact_sales f\nJOIN dim_customer c ON f.customer_key = c.customer_key  -- direct FK!\nJOIN dim_date d ON f.date_key = d.date_key\nWHERE d.year = 2023\nGROUP BY c.loyalty_tier;\n-- Works automatically: fact FK already points to the right version</pre><p><strong>Pattern 3: Point-in-time</strong> \u2014 what tier was customer 42 on Feb 1, 2024?</p><pre>SELECT customer_id, name, city, loyalty_tier\nFROM dim_customer\nWHERE customer_id = 42\n  AND effective_start <= '2024-02-01'\n  AND (effective_end > '2024-02-01' OR effective_end IS NULL);</pre><h4>The SCD ETL Pipeline \u2014 How Nightly Loads Work</h4><ol><li>Extract: pull changed records from OLTP (via CDC \u2014 Change Data Capture)</li><li>Compare: for each changed record, compare to current dim row. Any tracked attribute changed?</li><li>Expire: UPDATE old row \u2014 set effective_end = today, is_current = FALSE</li><li>Insert: INSERT new row with new surrogate key and new attribute values</li><li>No-change: skip records with unchanged tracked attributes entirely</li></ol></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Bi-temporal + Late-Arriving Facts</div><div class=\"rich\"><h4>Bi-temporal Modeling \u2014 Two Timelines</h4><p>SCD Type 2 tracks ONE timeline: when did the attribute change in our database? Bi-temporal modeling tracks TWO: <strong>valid time</strong> (when true in reality) vs <strong>transaction time</strong> (when our system recorded it). A customer might move cities Jan 1 but we don't record it until Jan 15 \u2014 these differ.</p><pre>CREATE TABLE dim_customer_bitemporal (\n  customer_key    INT PRIMARY KEY,\n  customer_id     INT,\n  city            VARCHAR(100),\n  -- Axis 1: when true in the real world\n  valid_start     DATE,\n  valid_end       DATE,\n  -- Axis 2: when our system recorded this\n  recorded_start  TIMESTAMP,\n  recorded_end    TIMESTAMP\n);\n-- Enables: \"What did our SYSTEM BELIEVE on Jan 10 about Jan 1's state?\"\n-- Critical for: auditing, compliance, retroactive corrections</pre><h4>Late-Arriving Facts \u2014 The Night Shift Problem</h4><p>A mobile app logs an event offline, connects to WiFi 3 days later. The event arrives Day+3 but the dimension may have changed in those 3 days. Load the fact by joining to the dimension version active on the ORIGINAL event date.</p><pre>INSERT INTO fact_sales (customer_key, ...)\nSELECT c.customer_key, ...\nFROM source_events e\nJOIN dim_customer c\n  ON c.customer_id = e.customer_id\n  AND e.event_date BETWEEN c.effective_start AND COALESCE(c.effective_end, '9999-12-31');</pre></div></div></div>",
        "KeyConcepts": [
            "SCD = Slowly Changing Dimension. Strategy for handling dimension attribute changes over time.",
            "SCD Type 1: overwrite. Simple, destroys history. Use only for data corrections (typos).",
            "SCD Type 2: new row per change. Full history preserved. The industry standard.",
            "SCD Type 2 columns: surrogate key (changes), natural key (constant), effective_start, effective_end, is_current.",
            "Fact table FK to surrogate key automatically delivers historical accuracy in all joins.",
            "SCD Type 3: previous_value column. One level of history. Use for planned reorgs, not permanent tracking.",
            "Point-in-time query: WHERE natural_key=X AND start<=date AND (end>date OR end IS NULL).",
            "Bi-temporal: valid time (real world) + transaction time (recorded). For auditing and late corrections.",
            "Late-arriving facts: join to dimension version active on the original event date, not current."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create dim_customer with SCD Type 2 columns. Insert one customer (Boston, Silver tier).",
            "<strong>Step 2:</strong> Simulate a tier upgrade (Silver \u2192 Gold). Write the UPDATE + INSERT correctly.",
            "<strong>Step 3:</strong> Write a point-in-time query: 'What tier was customer 42 on March 1, 2024?' \u2014 dates span before and after the change.",
            "<strong>Step 4:</strong> A late-arriving fact arrives with event_date=2023-11-15. Write the INSERT that joins to the correct historical dimension version."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Implement SCD Type 2 Merge logic in SQL",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Implement SCD Type 2 Merge logic in SQL \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: 'Which SCD type?' \u2014 almost always Type 2. Explain why Type 1 loses history.",
                "Surrogate key changes with each new version row. Natural/business key stays the same across all versions.",
                "Current records: WHERE is_current=TRUE or WHERE effective_end IS NULL. Both work.",
                "MERGE statement (UPSERT) handles the SCD Type 2 ETL expire+insert in a single SQL command."
            ]
        },
        "HardProblem": "Boss Problem (Spotify): 50M users, average 4 tier changes over 3 years. (1) Design dim_user with SCD Type 2. (2) A user can change country too \u2014 when both tier AND country change on the same day, do you create 1 or 2 new rows? Why? (3) Write a MERGE statement that handles insert/update(expire+insert)/no-change in one SQL command. (4) If 200K users change tier every day, how does this affect fact table FK updates?"
    },
    {
        "Week": 3,
        "Day": "Wednesday",
        "Phase": "2. Build (Architecture)",
        "Theme": "Data Modeling",
        "SpecificTopic": "Normalization",
        "ActionItem_Deliverable": "Normalize a messy Excel sheet to 3NF",
        "LeetCodeProblem": "<strong>LC 242 \u2013 Valid Anagram</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": "done",
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Gaps & Islands (Logic)</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 The Anomaly Problem</div><div class=\"rich\"><h4>The Anomaly Problem \u2014 What Happens Without Normalization</h4><p>Imagine storing everything in one flat table:</p><pre>orders_flat:\norder_id \u2502 customer_id \u2502 customer_email    \u2502 product_id \u2502 product_name \u2502 price\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\n1001     \u2502  42         \u2502 alice@example.com  \u2502  P01       \u2502 iPhone 15    \u2502  999\n1002     \u2502  42         \u2502 alice@example.com  \u2502  P02       \u2502 AirPods      \u2502  199\n1003     \u2502  55         \u2502 bob@example.com    \u2502  P01       \u2502 iPhone 15    \u2502  999\n1004     \u2502  42         \u2502 alice_new@ex.com   \u2502  P03       \u2502 MacBook      \u2502 1999</pre><p>This structure has three types of anomalies:</p><ul><li><strong>Update anomaly:</strong> Alice changes email \u2192 must UPDATE every row for customer 42. Miss one row \u2192 customer 42 now has two different emails \u2192 data is inconsistent.</li><li><strong>Insert anomaly:</strong> Cannot add a new product to the database until someone orders it \u2014 product data lives inside the orders row, not in its own table.</li><li><strong>Delete anomaly:</strong> Cancel order 1003 \u2192 lose all information about Bob's existence entirely.</li></ul><p><strong>Normalization</strong> eliminates these anomalies by ensuring each fact lives in exactly one place. Formalized by E.F. Codd (inventor of the relational model) in 1972.</p><p>The three main normal forms:</p><ul><li><strong>1NF</strong> \u2014 Atomic values. No lists or multi-values in one cell.</li><li><strong>2NF</strong> \u2014 Every non-key column depends on the ENTIRE primary key.</li><li><strong>3NF</strong> \u2014 Non-key columns depend ONLY on the primary key, not on each other.</li></ul><p>\u270d\ufe0f <strong>Memory trick:</strong> \"The key (1NF), the whole key (2NF), and nothing but the key (3NF).\"</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 1NF, 2NF, 3NF Step by Step</div><div class=\"rich\"><h4>1NF: Atomic Values \u2014 No Multi-Values in a Cell</h4><pre>-- \u274c VIOLATES 1NF: multiple phone numbers in one cell\ncustomer_id \u2502 name  \u2502 phone_numbers\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n42          \u2502 Alice \u2502 555-1234, 555-9876   \u2190 TWO values in one cell!\n\n-- \u2705 1NF: separate table, one value per row\ncustomer_phones: (customer_id, phone_number)\n42, 555-1234\n42, 555-9876</pre><h4>2NF: No Partial Dependencies (Only for Composite PKs)</h4><p>2NF only matters when the primary key uses multiple columns. Every non-key column must depend on the FULL composite key, not just part of it.</p><pre>-- \u274c VIOLATES 2NF \u2014 PK = (order_id, product_id)\norder_items: order_id\u2502product_id\u2502quantity\u2502product_name\u2502category\n-- product_name and category depend ONLY on product_id, not the full PK!\n\n-- \u2705 2NF: split out partial-dependent columns\norder_items: (order_id, product_id, quantity)   \u2190 full-key only\nproducts:    (product_id, product_name, category) \u2190 product-specific</pre><h4>3NF: No Transitive Dependencies</h4><p>3NF: a non-key column must not determine another non-key column. zip_code \u2192 city is a transitive dependency (zip determines city, not customer_id).</p><pre>-- \u274c VIOLATES 3NF: zip_code determines city (non-key \u2192 non-key)\ncustomers: customer_id\u2502name\u2502zip_code\u2502city\n42, Alice, 10001, New York   \u2190 city determined by zip, not by customer_id!\n55, Bob,   10001, New York   \u2190 same zip, same city always\n\n-- \u2705 3NF: separate the zip-to-city mapping\nzip_codes: (zip_code, city, state)   \u2190 zip determines city here\ncustomers: (customer_id, name, zip_code FK)  \u2190 customer holds the zip</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Denormalization \u2014 When to Break Rules</div><div class=\"rich\"><h4>Denormalization \u2014 When NOT to Normalize</h4><p>A fully normalized (3NF) schema is ideal for OLTP where updates are frequent. For analytical queries on large data sets, full normalization is a performance disaster \u2014 every business question requires 5\u20138 JOINs, and JOINs on 100M+ rows are expensive.</p><p><strong>Denormalization</strong> is the deliberate re-introduction of redundancy for query speed. This is intentional engineering, not bad design.</p><pre>-- Normalized 3NF: 6 joins to get revenue by category by region\nSELECT r.region, p.category, SUM(f.revenue)\nFROM fact_sales f\nJOIN dim_order o       ON f.order_id = o.order_id\nJOIN dim_product p     ON o.product_id = p.product_id\nJOIN dim_category c    ON p.category_id = c.category_id\nJOIN dim_customer cu   ON o.customer_id = cu.customer_id\nJOIN dim_region r      ON cu.region_id = r.region_id\nGROUP BY r.region, p.category;\n\n-- Star schema (denormalized): 2 joins only\nSELECT ds.region, dp.category, SUM(f.revenue)\nFROM fact_sales f\nJOIN dim_product dp ON f.product_key = dp.product_key\nJOIN dim_store   ds ON f.store_key   = ds.store_key\nGROUP BY ds.region, dp.category;</pre><table><tr><th>Scenario</th><td>Normalize?</td><td>Denormalize?</td></tr><tr><th>OLTP (transactions, user accounts)</th><td>\u2705 Yes \u2014 updates frequent</td><td>\u274c Causes anomalies</td></tr><tr><th>Analytics / BI dashboards</th><td>\u274c Too many joins, too slow</td><td>\u2705 Wide pre-joined tables</td></tr><tr><th>Data warehouse (star schema)</th><td>\u274c Snowflake = complex queries</td><td>\u2705 Flat dimension tables</td></tr></table></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 BCNF, 4NF, Decision Framework</div><div class=\"rich\"><h4>BCNF, 4NF, and the Limits of Normalization</h4><p><strong>BCNF (Boyce-Codd Normal Form)</strong> is a slightly stronger version of 3NF: every determinant must be a candidate key. Most 3NF tables are automatically in BCNF \u2014 violations only occur with multiple overlapping candidate keys. In practice: achieve 3NF and BCNF is usually also satisfied.</p><p><strong>4NF</strong> handles multi-valued dependencies: when columns are independently multi-valued but stored together, creating false combinations.</p><pre>-- \u274c 4NF violation: Alice has 2 hobbies AND 2 languages\n-- stored together \u2192 4 rows, but only 2+2 independent facts\nperson \u2502 hobby    \u2502 language\nAlice  \u2502 painting \u2502 English\nAlice  \u2502 painting \u2502 French   \u2190 painting accidentally paired with French\nAlice  \u2502 cooking  \u2502 English\nAlice  \u2502 cooking  \u2502 French\n\n-- \u2705 4NF: separate independent multi-valued facts\nperson_hobbies:   (Alice,painting), (Alice,cooking)\nperson_languages: (Alice,English), (Alice,French)</pre><h4>The Practical Decision Framework</h4><p>At FAANG, \"is this normalized correctly?\" is always followed by \"for what purpose?\" The answer changes the entire design:</p><ul><li><strong>OLTP:</strong> aim for 3NF/BCNF \u2014 consistency, update integrity paramount</li><li><strong>Analytics/DW:</strong> deliberately denormalize \u2014 query speed paramount</li><li><strong>Streaming/NoSQL:</strong> schema may not be relational \u2014 access pattern drives design</li><li>Know which context you are in before choosing normalization strategy</li></ul></div></div></div>",
        "KeyConcepts": [
            "Normalization eliminates insert/update/delete anomalies. Each fact lives in exactly one place.",
            "1NF: atomic values. No lists, no arrays, no comma-separated multiple values in one cell.",
            "2NF: every non-key column depends on the FULL composite PK. Only relevant for composite-PK tables.",
            "3NF: every non-key column depends ONLY on the PK. No column-to-column (transitive) dependencies.",
            "Memory trick: 'the key (1NF), the whole key (2NF), and nothing but the key (3NF)'.",
            "Denormalization: deliberate redundancy for query speed. Appropriate for analytics. Not a mistake.",
            "OLTP \u2192 normalize (updates frequent). Analytics \u2192 denormalize (queries frequent).",
            "BCNF: stronger than 3NF \u2014 every determinant is a candidate key. Usually automatic with 3NF."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Given: orders(order_id, customer_id, customer_email, product_id, product_name, quantity, price). Identify which normal form it violates and which specific columns cause violations.",
            "<strong>Step 2:</strong> Normalize to 3NF. Draw the schema with 3 tables, their PKs and FKs.",
            "<strong>Step 3:</strong> Write DDL for all 3 normalized tables with PK and FK constraints.",
            "<strong>Step 4:</strong> Write the query on the normalized schema that gets total revenue by product category for customer 42. How many JOINs vs the flat table version?"
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Normalize a messy Excel sheet to 3NF",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Normalize a messy Excel sheet to 3NF \u2014 trace through 2-3 edge cases before writing any code.",
                "Updating one value \u2192 must change multiple rows \u2192 find which normalization form is violated.",
                "2NF violations always involve composite PKs. Single-column PK = automatically 2NF.",
                "Transitive dependency: A\u2192B\u2192C where A is PK. Violates 3NF. Fix: move B\u2192C to its own table.",
                "Denormalization is correct when the trade-off is intentional and documented."
            ]
        },
        "HardProblem": "Boss Problem (Stripe): Flat table: payments(payment_id, merchant_id, merchant_name, merchant_country, buyer_id, buyer_email, buyer_country, payment_method_id, method_type, method_last4, amount, currency, timestamp). (1) Identify every normalization violation \u2014 state which NF rule is broken and why. (2) Normalize to 3NF \u2014 draw the full schema. (3) A business analyst says 'your normalized schema is too slow \u2014 queries take 2 minutes.' How do you respond and what do you build?"
    },
    {
        "Week": 3,
        "Day": "Thursday",
        "Phase": "2. Build (Architecture)",
        "Theme": "Data Modeling",
        "SpecificTopic": "NoSQL Modeling",
        "ActionItem_Deliverable": "Compare Wide-Column (Cassandra) vs Relational",
        "LeetCodeProblem": "<strong>LC 49 \u2013 Group Anagrams</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Execution Plans</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Why NoSQL Exists \u2014 The Mental Model Shift</div><div class=\"rich\"><h4>Why NoSQL Was Invented \u2014 The 2000s Scaling Crisis</h4><p>In the early 2000s, companies like Google, Amazon, and Facebook hit a fundamental wall with relational databases:</p><ul><li><strong>Scale:</strong> PostgreSQL on one server handles maybe 10TB. Google had petabytes.</li><li><strong>Speed:</strong> JOIN operations on billions of rows are slow regardless of optimization.</li><li><strong>Flexibility:</strong> Rigid schemas could not keep up with fast-changing product requirements.</li></ul><p>NoSQL was born to solve these. But it comes with trade-offs. The most important: you lose the ability to freely JOIN tables and get ACID guarantees simultaneously.</p><h4>The Fundamental Mental Model Shift</h4><p><strong>Relational (SQL):</strong> \"Design around entities and relationships. Queries figure themselves out. Normalize first, add indexes later.\"</p><p><strong>NoSQL:</strong> \"Design around ACCESS PATTERNS. Know your most common queries first, then design your data model to serve them in O(1). If you do not know your access patterns, you cannot design a NoSQL model.\"</p><p>\u270d\ufe0f <strong>Write this down:</strong> This is the most important conceptual shift in NoSQL. SQL is query-flexible but schema-rigid. NoSQL is query-specific but schema-flexible.</p><h4>The 4 Families of NoSQL Databases</h4><table><tr><th>Type</th><td>Examples</td><td>Best For</td></tr><tr><th>Key-Value</th><td>Redis, DynamoDB</td><td>Session storage, caching, simple lookups by ID</td></tr><tr><th>Wide-Column</th><td>Cassandra, HBase</td><td>Time-series, event logs, high write throughput</td></tr><tr><th>Document</th><td>MongoDB, Firestore</td><td>JSON objects, variable structure, nested documents</td></tr><tr><th>Graph</th><td>Neo4j, Amazon Neptune</td><td>Social networks, fraud detection, relationship traversal</td></tr></table><p>The family choice is determined by your access patterns. Use the wrong family and even perfect schema design cannot save query performance.</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Cassandra: Wide-Column + Three Rules</div><div class=\"rich\"><h4>Cassandra: Wide-Column Store \u2014 Three Absolute Rules</h4><p>Apache Cassandra is the choice when you need: massive write throughput (millions of writes/second), linear horizontal scalability, and no-single-point-of-failure availability. Used by Apple (Siri storage), Netflix (viewing history), Instagram (feed). But Cassandra has hard rules \u2014 violating them destroys performance.</p><p>The three rules:</p><ol><li><strong>Rule 1: Queries drive tables.</strong> In SQL you design one table and write any query. In Cassandra, design one table PER query pattern. The table exists to serve exactly one access pattern efficiently.</li><li><strong>Rule 2: No JOINs, no subqueries.</strong> Data must be pre-joined at write time. If you need user data alongside their posts, denormalize: store user data inside the posts table.</li><li><strong>Rule 3: No unbounded queries.</strong> Every query MUST include the partition key in WHERE. You cannot do full table scans or range queries without a partition key \u2014 Cassandra will reject or time out.</li></ol><h4>Partition Key vs Clustering Key</h4><p>In Cassandra, the PRIMARY KEY has two parts: <code>(partition_key, clustering_key)</code>. The partition key determines which node stores the data. The clustering key determines the sort order of rows WITHIN a partition.</p><pre>-- Netflix watch history: one partition per user\n-- All of one user's history on the same node for fast retrieval\nCREATE TABLE watch_history (\n  user_id    UUID,              -- PARTITION KEY: routes to one node\n  watched_at TIMESTAMP,         -- CLUSTERING KEY: sorted within partition\n  show_id    UUID,\n  show_title TEXT,\n  duration_mins INT,\n  PRIMARY KEY (user_id, watched_at)\n) WITH CLUSTERING ORDER BY (watched_at DESC);\n\n-- \u2705 Fast: give me user 42's last 20 shows\nSELECT * FROM watch_history WHERE user_id=42 LIMIT 20;\n\n-- \u274c Rejected: no partition key given \u2014 would scan entire cluster\nSELECT * FROM watch_history WHERE show_title='Stranger Things';</pre><p>The LIMIT 20 is served from a single node in milliseconds \u2014 O(1) lookup. The rejected query would require scanning every partition on every node \u2014 Cassandra refuses it.</p></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 DynamoDB: Single-Table Design</div><div class=\"rich\"><h4>DynamoDB: Single-Table Design</h4><p>Amazon DynamoDB is a managed key-value and document store. The \"single-table design\" pattern \u2014 storing multiple entity types in ONE table using a generic partition key (PK) and sort key (SK) \u2014 is the most important and most misunderstood DynamoDB pattern.</p><p>The motivation: DynamoDB charges for reads/writes per request. If you need Customer + Orders + OrderItems in one API call, querying 3 separate tables costs 3 requests. Single-table design allows all related entities to be co-located under one partition \u2014 fetched in a single request.</p><pre>-- Single table: \"ecommerce_table\"\n-- pk=partition key, sk=sort key \u2014 generic names, specific values\n--\n-- Row type  \u2502 pk            \u2502 sk              \u2502 attributes\n-- \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n-- Customer  \u2502 CUST#42       \u2502 METADATA        \u2502 name, email, city\n-- Order     \u2502 CUST#42       \u2502 ORDER#1001      \u2502 total, status, date\n-- OrderItem \u2502 CUST#42       \u2502 ORDER#1001#P01  \u2502 qty, price, name\n-- Product   \u2502 PRODUCT#P01   \u2502 METADATA        \u2502 name, category, price\n--\n-- Query: get customer 42 + all their orders in ONE request:\nGET pk=CUST#42            \u2192 customer metadata + all orders + all items\n\n-- Query: get just orders for customer 42:\nQUERY pk=CUST#42, sk BEGINS_WITH \"ORDER#\"</pre><h4>The SQL\u2192NoSQL Decision Framework</h4><table><tr><th>Requirement</th><td>Choose</td></tr><tr><th>Complex ad-hoc reporting with JOINs</th><td>PostgreSQL / BigQuery (SQL)</td></tr><tr><th>High write throughput (>100K writes/sec), time-series</th><td>Cassandra</td></tr><tr><th>Single-millisecond reads by ID at massive scale</th><td>DynamoDB / Redis</td></tr><tr><th>Flexible/variable document structure</th><td>MongoDB</td></tr><tr><th>Graph traversal (friends-of-friends, fraud rings)</th><td>Neo4j / Neptune</td></tr><tr><th>Caching, session storage (data expires)</th><td>Redis</td></tr></table></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 CAP Theorem + Instagram Feed Design</div><div class=\"rich\"><h4>CAP Theorem \u2014 The Fundamental Distributed Systems Constraint</h4><p>CAP theorem (Brewer, 2000): in a distributed system, you can guarantee at most TWO of three properties simultaneously:</p><ul><li><strong>Consistency (C):</strong> Every read receives the most recent write, or an error. No stale data.</li><li><strong>Availability (A):</strong> Every request receives a response (not guaranteed to be latest).</li><li><strong>Partition Tolerance (P):</strong> The system operates even when network messages are lost between nodes.</li></ul><p>Network partitions are unavoidable in real distributed systems \u2014 you ALWAYS need P. So the real choice is: sacrifice C (choose AP) or sacrifice A (choose CP).</p><table><tr><th>Database</th><td>CAP Choice</td><td>Trade-off</td></tr><tr><th>Cassandra</th><td>AP (tunable)</td><td>May return stale reads. Uses eventual consistency by default.</td></tr><tr><th>HBase</th><td>CP</td><td>May refuse requests during partition. Strongly consistent.</td></tr><tr><th>DynamoDB</th><td>AP (default) / CP (opt-in)</td><td>Eventually consistent by default, strongly consistent reads available.</td></tr><tr><th>Redis</th><td>AP (single-node) / CP (cluster)</td><td>Depends on replication config and cluster mode.</td></tr><tr><th>PostgreSQL (single)</th><td>CP</td><td>No partition tolerance \u2014 it is a single node.</td></tr></table><h4>FAANG Interview: Modeling Instagram Feed in Cassandra</h4><p>Interview question: \"Design the data model for Instagram user feeds in Cassandra.\"</p><pre>-- Access pattern: \"Get my feed \u2014 posts from people I follow, newest first\"\n-- Option A: Fan-out on Write\n--   When Alice posts \u2192 immediately write to every follower's feed partition\n--   Pro: feed reads are O(1)\n--   Con: if Alice has 10M followers, one post = 10M writes\n\n-- Option B: Fan-out on Read\n--   When user requests feed \u2192 read from all 300 followees' timelines, merge\n--   Pro: writes are cheap (one write per post)\n--   Con: feed reads are O(followees) \u2014 slow for users who follow 1,000 accounts\n\n-- Real Instagram hybrid: fan-out on write for normal users,\n--   fan-out on read for celebrity accounts (10M+ followers)</pre></div></div></div>",
        "KeyConcepts": [
            "NoSQL was built to address SQL's scale (storage), speed (join cost), and flexibility (schema rigidity) limits.",
            "Fundamental shift: SQL = design around entities, queries are flexible. NoSQL = design around access patterns first.",
            "Key-Value (Redis/DynamoDB): O(1) by ID. Wide-Column (Cassandra): time-series, high write throughput.",
            "Document (MongoDB): variable/nested structures. Graph (Neo4j): relationship traversal.",
            "Cassandra Rule 1: one table per query. Rule 2: no JOINs \u2014 pre-join at write time. Rule 3: always provide partition key.",
            "Partition key = which node stores the data. Clustering key = sort order within that partition.",
            "DynamoDB single-table: all entity types in one table with generic PK/SK. Co-located for single-request fetches.",
            "CAP theorem: choose 2 of Consistency/Availability/Partition-tolerance. Partitions are unavoidable \u2192 choose C or A.",
            "Cassandra = AP (eventual). HBase = CP (strong). DynamoDB = AP by default, CP opt-in."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Design a Cassandra table for: 'Get all messages in a chat room, newest first, limit 50.' State partition key, clustering key, and why.",
            "<strong>Step 2:</strong> Design a DynamoDB single-table for an e-commerce app with customers, orders, and order items. Show 3 rows of sample data.",
            "<strong>Step 3:</strong> Given the fan-out Instagram feed problem: write the data model for fan-out-on-write. What happens when a celebrity with 10M followers posts?",
            "<strong>Step 4:</strong> For each of 3 systems (Twitter, Netflix, Uber), state which NoSQL family you'd use and why."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Compare Wide-Column (Cassandra) vs Relational",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Compare Wide-Column (Cassandra) vs Relational \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: 'When would you use Cassandra?' \u2192 high write throughput, time-series, queries always by partition key.",
                "Cassandra: missing partition key in WHERE = full cluster scan = query rejected. Always include partition key.",
                "DynamoDB single-table: use BEGINS_WITH on SK to query subsets (all orders for a customer).",
                "CAP: Cassandra AP = may return stale reads. HBase CP = may refuse under partition. Trade-off, not bug."
            ]
        },
        "HardProblem": "Boss Problem (Facebook Messenger): Design the data model for a chat app. Access patterns: (1) get the last 50 messages in a conversation, newest first; (2) get all conversations for a user, sorted by most recent message; (3) get unread message count per conversation. 10B messages/day. Design: SQL, Cassandra, or DynamoDB? Why? What is the partition key for each table? Handle: group chats (1 conversation, many users), message deletion, read receipts."
    },
    {
        "Week": 3,
        "Day": "Friday",
        "Phase": "2. Build (Architecture)",
        "Theme": "Data Modeling",
        "SpecificTopic": "Case Study: Social Media",
        "ActionItem_Deliverable": "Design Schema for Instagram Likes & Comments",
        "LeetCodeProblem": "<strong>LC 128 \u2013 Longest Consecutive Sequence</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>NULL Handling</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 What Schema Design Interviews Test</div><div class=\"rich\"><h4>What Schema Design Interviews Actually Test</h4><p>Schema design interviews are not about knowing the \"right answer.\" They test: (1) how you ask clarifying questions before designing, (2) how you identify access patterns before picking a database, (3) how you handle evolving requirements, (4) how you discuss trade-offs openly instead of presenting one solution as absolute truth.</p><p>The most common failure: jumping into CREATE TABLE statements in the first 30 seconds. Interviewers specifically watch to see whether you ask questions first. An engineer who designs first and asks questions later is dangerous in production.</p><h4>The Framework: 5 Questions Before Any Design</h4><ol><li><strong>\"What are the most frequent read queries?\"</strong> \u2014 Access patterns drive everything.</li><li><strong>\"What are the most frequent write patterns?\"</strong> \u2014 High write \u2192 Cassandra. Mostly reads \u2192 PostgreSQL/DynamoDB.</li><li><strong>\"What is the expected data volume?\"</strong> \u2014 10K rows/day \u2192 any DB. 10M rows/day \u2192 need partitioning strategy.</li><li><strong>\"How fresh must the data be?\"</strong> \u2014 Real-time \u2192 strong consistency. Dashboard \u2192 eventual OK.</li><li><strong>\"What is the read-to-write ratio?\"</strong> \u2014 1000:1 \u2192 optimize reads. 1:1 \u2192 optimize both.</li></ol><p>\u270d\ufe0f After asking these 5 questions, you have enough information to make an informed schema decision. Before asking them, you are guessing.</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Twitter Schema \u2014 Full Worked Example</div><div class=\"rich\"><h4>Designing Twitter's Schema \u2014 A Worked Example</h4><p><strong>Requirements:</strong> Users post tweets. Users follow other users. Users see a timeline of tweets from accounts they follow. System has 500M users, 500M tweets/day, average user follows 300 accounts.</p><h4>Step 1: Access Patterns</h4><pre>Read patterns (must be fast):\n  R1: Get a user's home timeline (latest tweets from followees), newest first\n  R2: Get a user's own tweets (profile page)\n  R3: Get a single tweet by ID\n  R4: Get a user's follower/following counts\n\nWrite patterns:\n  W1: Post a new tweet\n  W2: Follow / unfollow a user\n  W3: Like / retweet a tweet\n\nVolume: 500M tweets/day = ~5,800 tweets/second. 100B timeline reads/day.</pre><h4>Step 2: Data Entities and SQL Schema</h4><pre>-- Core entities\nCREATE TABLE users (\n  user_id BIGINT PRIMARY KEY,\n  username VARCHAR(50) UNIQUE,\n  display_name VARCHAR(100),\n  bio TEXT,\n  follower_count INT,    -- denormalized counter (avoid COUNT(*) on every read)\n  following_count INT\n);\n\nCREATE TABLE tweets (\n  tweet_id BIGINT PRIMARY KEY,  -- Snowflake ID: encodes timestamp+machine+seq\n  user_id  BIGINT REFERENCES users(user_id),\n  content  VARCHAR(280),\n  created_at TIMESTAMP,\n  like_count INT,         -- denormalized: avoid COUNT per query\n  retweet_count INT\n);\n\nCREATE TABLE follows (\n  follower_id BIGINT REFERENCES users(user_id),\n  followee_id BIGINT REFERENCES users(user_id),\n  followed_at TIMESTAMP,\n  PRIMARY KEY (follower_id, followee_id)\n);\nCREATE INDEX ON follows(followee_id);  -- reverse: who follows this user?</pre><h4>Step 3: The Timeline Problem</h4><p>Naive timeline query: <code>SELECT t.* FROM tweets t JOIN follows f ON t.user_id = f.followee_id WHERE f.follower_id=42 ORDER BY created_at DESC LIMIT 20</code>. This joins 500M tweets \u00d7 300 followees = runs the follower check on millions of rows. Too slow.</p><p><strong>Solution: Precomputed timeline fan-out.</strong> When Alice posts a tweet, immediately write it to every follower's timeline cache (Redis sorted set by timestamp). Timeline reads become O(1). Trade-off: writes are amplified (300 follower writes per tweet from an average user).</p></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Edge Cases \u2014 The Interview Differentiator</div><div class=\"rich\"><h4>Handling Edge Cases \u2014 The Interview Differentiator</h4><p>Strong schema design candidates address edge cases before the interviewer asks. Here are the 5 most common schema edge cases and how to handle each:</p><table><tr><th>Edge Case</th><td>Problem</td><td>Solution</td></tr><tr><th>Soft deletes</th><td>DELETE removes audit trail</td><td>Add is_deleted BOOLEAN + deleted_at TIMESTAMP. Filter WHERE NOT is_deleted.</td></tr><tr><th>Large blobs (images/video)</th><td>Storing files in DB is slow and expensive</td><td>Store in object storage (S3). DB stores only the URL/path.</td></tr><tr><th>Counters (likes, followers)</th><td>COUNT(*) on every request is expensive</td><td>Denormalize: maintain a counter column. Increment on write.</td></tr><tr><th>User-generated content moderation</th><td>Need to hide content without deleting</td><td>Add status enum: ACTIVE / HIDDEN / REMOVED. Filter on status.</td></tr><tr><th>Time zones</th><td>ambiguous timestamps in analytics</td><td>Always store timestamps in UTC. Convert to user local time in the application layer.</td></tr></table><h4>Indexing as Part of Schema Design</h4><p>An incomplete schema interview answer presents only the CREATE TABLE DDL. A complete answer also states: which columns need indexes and why. For every frequent query's WHERE and ORDER BY clause, explicitly name the index.</p><pre>-- Twitter schema indexes:\nCREATE INDEX tweets_user_created ON tweets(user_id, created_at DESC);\n-- Supports: WHERE user_id=X ORDER BY created_at DESC LIMIT 20 (profile page)\n\nCREATE INDEX follows_followee ON follows(followee_id);\n-- Supports: WHERE followee_id=X (find all followers of a user)\n\nCREATE INDEX tweets_created ON tweets(created_at DESC);\n-- Supports: global trending (recent tweets across all users)</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Uber Schema + Geospatial Design</div><div class=\"rich\"><h4>Uber/Lyft Schema: The Ride + Driver Matching Problem</h4><p>A more complex FAANG schema problem: design the data model for Uber trips.</p><pre>-- Entities: users (riders), drivers, vehicles, trips, payments\n\nCREATE TABLE drivers (\n  driver_id   BIGINT PRIMARY KEY,\n  user_id     BIGINT UNIQUE REFERENCES users(user_id),\n  license_no  VARCHAR(50),\n  rating      DECIMAL(3,2),\n  status      ENUM('ACTIVE','INACTIVE','SUSPENDED')\n);\n\nCREATE TABLE trips (\n  trip_id         BIGINT PRIMARY KEY,\n  rider_id        BIGINT REFERENCES users(user_id),\n  driver_id       BIGINT REFERENCES drivers(driver_id),\n  vehicle_id      BIGINT REFERENCES vehicles(vehicle_id),\n  status          ENUM('REQUESTED','ACCEPTED','IN_PROGRESS','COMPLETED','CANCELLED'),\n  requested_at    TIMESTAMP,\n  pickup_lat      DECIMAL(9,6),\n  pickup_lng      DECIMAL(9,6),\n  dropoff_lat     DECIMAL(9,6),\n  dropoff_lng     DECIMAL(9,6),\n  fare_amount     DECIMAL(10,2),\n  distance_miles  DECIMAL(8,2),\n  duration_mins   INT\n);\n\n-- For matching: drivers need to be queried by location\n-- Use geospatial index (PostGIS) on current lat/lng\nCREATE INDEX idx_driver_location ON driver_locations\nUSING GIST (location);  -- enables radius queries: WHERE ST_Distance(location, point) < 2km</pre><p><strong>Key design decisions to state in an interview:</strong></p><ul><li>Driver real-time location goes in a SEPARATE table (high write frequency, different retention)</li><li>Geospatial queries require GIST index (not B-Tree)</li><li>Trip status uses ENUM \u2014 adding a new state requires ALTER TABLE (discuss migration strategy)</li><li>Fare calculation stored as result (denormalized) \u2014 never recompute from distance/time on every read</li><li>Soft deletes on trips (audit trail, disputes) \u2014 never hard DELETE trip records</li></ul></div></div></div>",
        "KeyConcepts": [
            "Schema design interviews test process (ask first, design second) more than encyclopedic knowledge.",
            "5 questions before any design: access patterns, write patterns, volume, freshness requirement, read/write ratio.",
            "Twitter timeline problem: naive JOIN on followees is O(followees) \u2014 precomputed feed fan-out is O(1).",
            "Denormalized counters (like_count, follower_count): maintain on write to avoid COUNT(*) on every read.",
            "Always include index design in your schema answer \u2014 incomplete without it.",
            "Snowflake IDs (tweet_id): encode timestamp + machine ID + sequence. Sortable by time, no central coordinator.",
            "Geospatial queries require GIST index (not B-Tree). Always mention this for location-based schemas.",
            "Soft deletes: is_deleted + deleted_at. Never hard DELETE records that may be needed for disputes/audits."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Design the schema for a URL shortener (bit.ly). What are the access patterns? What is the primary table? What index makes redirect O(1)?",
            "<strong>Step 2:</strong> Design the schema for a parking app (SpotHero). Users book parking spots by location and time. Handle: spots have complex pricing rules, bookings can be cancelled.",
            "<strong>Step 3:</strong> Take your Twitter schema from Level 2. Add: (1) soft deletes on tweets, (2) a hashtags table with many-to-many to tweets, (3) a media_attachments table for images.",
            "<strong>Step 4:</strong> Mock: design YouTube's schema in 20 minutes. Ask yourself the 5 questions first. Then design. Then check: did you state all indexes? Did you mention edge cases?"
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Design Schema for Instagram Likes & Comments",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Design Schema for Instagram Likes & Comments \u2014 trace through 2-3 edge cases before writing any code.",
                "Never jump to CREATE TABLE before asking clarifying questions \u2014 it is a red flag in schema interviews.",
                "Counter columns (follower_count): faster than COUNT(*) per request. Trade-off: counts can drift if not careful.",
                "Status as ENUM vs VARCHAR: ENUM is validated at DB level (safer). VARCHAR is more flexible for new states.",
                "Always state your indexes \u2014 'I would add an index on user_id + created_at DESC for the profile page query.'"
            ]
        },
        "HardProblem": "Boss Problem (Airbnb): Design Airbnb's complete data model: properties (photos, amenities, room types), hosts, guests, bookings (no double-booking), reviews (bidirectional: host reviews guest AND guest reviews property), pricing (variable by date, seasonality). The analytics team needs a star schema for: revenue by city/month, occupancy rate by property type, host performance. Design: (1) operational schema (OLTP), (2) DW schema (star schema), (3) which data flows from OLTP to DW via ETL."
    },
    {
        "Week": 3,
        "Day": "Saturday",
        "Phase": "2. Build (Architecture)",
        "Theme": "Data Modeling",
        "SpecificTopic": "Mock Design Round",
        "ActionItem_Deliverable": "Whiteboard Session: Design Uber Eats Schema",
        "LeetCodeProblem": "<strong>LC 347 \u2013 Top K Frequent Elements</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Mock Assessment</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 What Schema Design Interviews Test</div><div class=\"rich\"><h4>What Schema Design Interviews Actually Test</h4><p>Schema design interviews are not about knowing the \"right answer.\" They test: (1) how you ask clarifying questions before designing, (2) how you identify access patterns before picking a database, (3) how you handle evolving requirements, (4) how you discuss trade-offs openly instead of presenting one solution as absolute truth.</p><p>The most common failure: jumping into CREATE TABLE statements in the first 30 seconds. Interviewers specifically watch to see whether you ask questions first. An engineer who designs first and asks questions later is dangerous in production.</p><h4>The Framework: 5 Questions Before Any Design</h4><ol><li><strong>\"What are the most frequent read queries?\"</strong> \u2014 Access patterns drive everything.</li><li><strong>\"What are the most frequent write patterns?\"</strong> \u2014 High write \u2192 Cassandra. Mostly reads \u2192 PostgreSQL/DynamoDB.</li><li><strong>\"What is the expected data volume?\"</strong> \u2014 10K rows/day \u2192 any DB. 10M rows/day \u2192 need partitioning strategy.</li><li><strong>\"How fresh must the data be?\"</strong> \u2014 Real-time \u2192 strong consistency. Dashboard \u2192 eventual OK.</li><li><strong>\"What is the read-to-write ratio?\"</strong> \u2014 1000:1 \u2192 optimize reads. 1:1 \u2192 optimize both.</li></ol><p>\u270d\ufe0f After asking these 5 questions, you have enough information to make an informed schema decision. Before asking them, you are guessing.</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Twitter Schema \u2014 Full Worked Example</div><div class=\"rich\"><h4>Designing Twitter's Schema \u2014 A Worked Example</h4><p><strong>Requirements:</strong> Users post tweets. Users follow other users. Users see a timeline of tweets from accounts they follow. System has 500M users, 500M tweets/day, average user follows 300 accounts.</p><h4>Step 1: Access Patterns</h4><pre>Read patterns (must be fast):\n  R1: Get a user's home timeline (latest tweets from followees), newest first\n  R2: Get a user's own tweets (profile page)\n  R3: Get a single tweet by ID\n  R4: Get a user's follower/following counts\n\nWrite patterns:\n  W1: Post a new tweet\n  W2: Follow / unfollow a user\n  W3: Like / retweet a tweet\n\nVolume: 500M tweets/day = ~5,800 tweets/second. 100B timeline reads/day.</pre><h4>Step 2: Data Entities and SQL Schema</h4><pre>-- Core entities\nCREATE TABLE users (\n  user_id BIGINT PRIMARY KEY,\n  username VARCHAR(50) UNIQUE,\n  display_name VARCHAR(100),\n  bio TEXT,\n  follower_count INT,    -- denormalized counter (avoid COUNT(*) on every read)\n  following_count INT\n);\n\nCREATE TABLE tweets (\n  tweet_id BIGINT PRIMARY KEY,  -- Snowflake ID: encodes timestamp+machine+seq\n  user_id  BIGINT REFERENCES users(user_id),\n  content  VARCHAR(280),\n  created_at TIMESTAMP,\n  like_count INT,         -- denormalized: avoid COUNT per query\n  retweet_count INT\n);\n\nCREATE TABLE follows (\n  follower_id BIGINT REFERENCES users(user_id),\n  followee_id BIGINT REFERENCES users(user_id),\n  followed_at TIMESTAMP,\n  PRIMARY KEY (follower_id, followee_id)\n);\nCREATE INDEX ON follows(followee_id);  -- reverse: who follows this user?</pre><h4>Step 3: The Timeline Problem</h4><p>Naive timeline query: <code>SELECT t.* FROM tweets t JOIN follows f ON t.user_id = f.followee_id WHERE f.follower_id=42 ORDER BY created_at DESC LIMIT 20</code>. This joins 500M tweets \u00d7 300 followees = runs the follower check on millions of rows. Too slow.</p><p><strong>Solution: Precomputed timeline fan-out.</strong> When Alice posts a tweet, immediately write it to every follower's timeline cache (Redis sorted set by timestamp). Timeline reads become O(1). Trade-off: writes are amplified (300 follower writes per tweet from an average user).</p></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Edge Cases \u2014 The Interview Differentiator</div><div class=\"rich\"><h4>Handling Edge Cases \u2014 The Interview Differentiator</h4><p>Strong schema design candidates address edge cases before the interviewer asks. Here are the 5 most common schema edge cases and how to handle each:</p><table><tr><th>Edge Case</th><td>Problem</td><td>Solution</td></tr><tr><th>Soft deletes</th><td>DELETE removes audit trail</td><td>Add is_deleted BOOLEAN + deleted_at TIMESTAMP. Filter WHERE NOT is_deleted.</td></tr><tr><th>Large blobs (images/video)</th><td>Storing files in DB is slow and expensive</td><td>Store in object storage (S3). DB stores only the URL/path.</td></tr><tr><th>Counters (likes, followers)</th><td>COUNT(*) on every request is expensive</td><td>Denormalize: maintain a counter column. Increment on write.</td></tr><tr><th>User-generated content moderation</th><td>Need to hide content without deleting</td><td>Add status enum: ACTIVE / HIDDEN / REMOVED. Filter on status.</td></tr><tr><th>Time zones</th><td>ambiguous timestamps in analytics</td><td>Always store timestamps in UTC. Convert to user local time in the application layer.</td></tr></table><h4>Indexing as Part of Schema Design</h4><p>An incomplete schema interview answer presents only the CREATE TABLE DDL. A complete answer also states: which columns need indexes and why. For every frequent query's WHERE and ORDER BY clause, explicitly name the index.</p><pre>-- Twitter schema indexes:\nCREATE INDEX tweets_user_created ON tweets(user_id, created_at DESC);\n-- Supports: WHERE user_id=X ORDER BY created_at DESC LIMIT 20 (profile page)\n\nCREATE INDEX follows_followee ON follows(followee_id);\n-- Supports: WHERE followee_id=X (find all followers of a user)\n\nCREATE INDEX tweets_created ON tweets(created_at DESC);\n-- Supports: global trending (recent tweets across all users)</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Uber Schema + Geospatial Design</div><div class=\"rich\"><h4>Uber/Lyft Schema: The Ride + Driver Matching Problem</h4><p>A more complex FAANG schema problem: design the data model for Uber trips.</p><pre>-- Entities: users (riders), drivers, vehicles, trips, payments\n\nCREATE TABLE drivers (\n  driver_id   BIGINT PRIMARY KEY,\n  user_id     BIGINT UNIQUE REFERENCES users(user_id),\n  license_no  VARCHAR(50),\n  rating      DECIMAL(3,2),\n  status      ENUM('ACTIVE','INACTIVE','SUSPENDED')\n);\n\nCREATE TABLE trips (\n  trip_id         BIGINT PRIMARY KEY,\n  rider_id        BIGINT REFERENCES users(user_id),\n  driver_id       BIGINT REFERENCES drivers(driver_id),\n  vehicle_id      BIGINT REFERENCES vehicles(vehicle_id),\n  status          ENUM('REQUESTED','ACCEPTED','IN_PROGRESS','COMPLETED','CANCELLED'),\n  requested_at    TIMESTAMP,\n  pickup_lat      DECIMAL(9,6),\n  pickup_lng      DECIMAL(9,6),\n  dropoff_lat     DECIMAL(9,6),\n  dropoff_lng     DECIMAL(9,6),\n  fare_amount     DECIMAL(10,2),\n  distance_miles  DECIMAL(8,2),\n  duration_mins   INT\n);\n\n-- For matching: drivers need to be queried by location\n-- Use geospatial index (PostGIS) on current lat/lng\nCREATE INDEX idx_driver_location ON driver_locations\nUSING GIST (location);  -- enables radius queries: WHERE ST_Distance(location, point) < 2km</pre><p><strong>Key design decisions to state in an interview:</strong></p><ul><li>Driver real-time location goes in a SEPARATE table (high write frequency, different retention)</li><li>Geospatial queries require GIST index (not B-Tree)</li><li>Trip status uses ENUM \u2014 adding a new state requires ALTER TABLE (discuss migration strategy)</li><li>Fare calculation stored as result (denormalized) \u2014 never recompute from distance/time on every read</li><li>Soft deletes on trips (audit trail, disputes) \u2014 never hard DELETE trip records</li></ul></div></div></div>",
        "KeyConcepts": [
            "Schema design interviews test process (ask first, design second) more than encyclopedic knowledge.",
            "5 questions before any design: access patterns, write patterns, volume, freshness requirement, read/write ratio.",
            "Twitter timeline problem: naive JOIN on followees is O(followees) \u2014 precomputed feed fan-out is O(1).",
            "Denormalized counters (like_count, follower_count): maintain on write to avoid COUNT(*) on every read.",
            "Always include index design in your schema answer \u2014 incomplete without it.",
            "Snowflake IDs (tweet_id): encode timestamp + machine ID + sequence. Sortable by time, no central coordinator.",
            "Geospatial queries require GIST index (not B-Tree). Always mention this for location-based schemas.",
            "Soft deletes: is_deleted + deleted_at. Never hard DELETE records that may be needed for disputes/audits."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Design the schema for a URL shortener (bit.ly). What are the access patterns? What is the primary table? What index makes redirect O(1)?",
            "<strong>Step 2:</strong> Design the schema for a parking app (SpotHero). Users book parking spots by location and time. Handle: spots have complex pricing rules, bookings can be cancelled.",
            "<strong>Step 3:</strong> Take your Twitter schema from Level 2. Add: (1) soft deletes on tweets, (2) a hashtags table with many-to-many to tweets, (3) a media_attachments table for images.",
            "<strong>Step 4:</strong> Mock: design YouTube's schema in 20 minutes. Ask yourself the 5 questions first. Then design. Then check: did you state all indexes? Did you mention edge cases?"
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Whiteboard Session: Design Uber Eats Schema",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Whiteboard Session: Design Uber Eats Schema \u2014 trace through 2-3 edge cases before writing any code.",
                "Never jump to CREATE TABLE before asking clarifying questions \u2014 it is a red flag in schema interviews.",
                "Counter columns (follower_count): faster than COUNT(*) per request. Trade-off: counts can drift if not careful.",
                "Status as ENUM vs VARCHAR: ENUM is validated at DB level (safer). VARCHAR is more flexible for new states.",
                "Always state your indexes \u2014 'I would add an index on user_id + created_at DESC for the profile page query.'"
            ]
        },
        "HardProblem": "Boss Problem (Airbnb): Design Airbnb's complete data model: properties (photos, amenities, room types), hosts, guests, bookings (no double-booking), reviews (bidirectional: host reviews guest AND guest reviews property), pricing (variable by date, seasonality). The analytics team needs a star schema for: revenue by city/month, occupancy rate by property type, host performance. Design: (1) operational schema (OLTP), (2) DW schema (star schema), (3) which data flows from OLTP to DW via ETL."
    },
    {
        "Week": 3,
        "Day": "Sunday",
        "Phase": "2. Build (Architecture)",
        "Theme": "Rest",
        "SpecificTopic": "Rest & Review",
        "ActionItem_Deliverable": "Review Data Modeling concepts + push labs to GitHub",
        "LeetCodeProblem": null,
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>SCD Types (1, 2, 3)</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Active Rest \u2014 How Memory Consolidates</div>\n<div class=\"rich\">\n<h4>The Neuroscience of Spaced Repetition</h4>\n<p>Rest days are not wasted days. During rest, your brain replays what it learned \u2014 a process called <strong>memory consolidation</strong>. Research consistently shows that <em>attempting to recall from memory</em> is 3\u00d7 more effective at building durable knowledge than re-reading notes. The act of struggling to remember \u2014 even failing \u2014 strengthens the neural pathway more than passive review.</p>\n<p>Think of it like muscle recovery: a weightlifter who trains 7 days straight without rest gets weaker, not stronger. The growth happens during recovery. Your brain is the same.</p>\n<h4>Today's Protocol (4 hours, structured)</h4>\n<ol>\n  <li><strong>Active Recall (60 min):</strong> Close everything. On a blank page, write every concept, formula, and pattern from this week from memory. Then check.</li>\n  <li><strong>Error Review (60 min):</strong> Find the hardest problem you struggled with. Attempt it again completely from scratch \u2014 no notes.</li>\n  <li><strong>Teach It (45 min):</strong> Open the AI chatbot and explain Gaps & Islands to it as if it's a junior engineer. If you stumble, that's your gap.</li>\n  <li><strong>Next Week Preview (15 min):</strong> Skim next Monday's topic at headline level only \u2014 priming improves retention when you study it deeply.</li>\n</ol>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Active Recall Quiz \u2014 No Notes Allowed</div>\n<div class=\"rich\">\n<h4>Answer These From Memory. Then Check.</h4>\n<pre>Window Functions:\n\u25a1 What is the difference between ROW_NUMBER, RANK, and DENSE_RANK?\n  (Write the output for a table with two tied rows)\n\u25a1 Write the syntax for a 7-day rolling average from memory\n\u25a1 What does PARTITION BY do differently from GROUP BY?\n\u25a1 Write the complete Gaps & Islands CTE from memory\n\u25a1 What is the difference between ROWS BETWEEN and RANGE BETWEEN?\n\u25a1 LAG(col, 1, 0) vs LAG(col, 1) \u2014 what is different?\n\u25a1 In what SQL execution order step do window functions run?\n\u25a1 Why can't you write WHERE rank &lt;= 3 in the same query as ROW_NUMBER?\n\nScore yourself:\n8/8 = \ud83c\udfc6 Mastery level \u2014 you're ready for interviews on this\n5-7/8 = \ud83d\udfe1 Good \u2014 review the 1-2 concepts you missed\n&lt; 5/8 = \ud83d\udd34 Go back to the specific day's content, re-read Level 1-2</pre>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Spaced Repetition Schedule</div>\n<div class=\"rich\">\n<h4>When to Review This Week's Material</h4>\n<table>\n<tr><th>Review this content</th><th>When</th><th>How</th></tr>\n<tr><td>This week</td><td>Today</td><td>Active recall quiz above</td></tr>\n<tr><td>This week</td><td>In 3 days</td><td>Re-solve the hard problem from Day 4</td></tr>\n<tr><td>This week</td><td>In 7 days</td><td>LeetCode: solve problems 185, 180, 196</td></tr>\n<tr><td>This week</td><td>In 30 days</td><td>Explain all 4 concepts to someone else</td></tr>\n</table>\n<h4>The Feynman Technique</h4>\n<p>After studying any concept, close your notes and explain it as if you're teaching a newcomer. Use only simple words. Avoid jargon. If you stumble, that's exactly where your understanding breaks down \u2014 return to just that part. This technique built Richard Feynman's legendary ability to explain complex physics simply.</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Interview Readiness Self-Assessment</div>\n<div class=\"rich\">\n<pre>Rate yourself 1\u20135 on each topic (honest self-assessment):\n  1 = \"I need to re-read from scratch\"\n  3 = \"I understand but need to look at notes\"\n  5 = \"I can code this under pressure from memory\"\n\nROW_NUMBER / RANK / DENSE_RANK:        ___/5\nRolling windows (ROWS BETWEEN):        ___/5\nLAG/LEAD (adjacent row comparison):    ___/5\nGaps & Islands (date streaks):         ___/5\nSession detection (timestamp version): ___/5\n\nAction:\n  Score &lt;3: revisit that day's Level 1 and Level 2 content tomorrow\n  Score 3:  do one LeetCode problem on that specific topic\n  Score 5:  you're interview-ready \u2705 on this topic</pre>\n<h4>FAANG Interview Reality Check</h4>\n<p>At meta, Amazon, and Google data engineering interviews, you'll be asked to write SQL live on a shared screen without autocomplete or documentation. The most common reason candidates fail: they know the concept but freeze on the syntax under pressure. The fix: write every pattern from memory at least 5 times this week. Muscle memory is more reliable than conscious recall under stress.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Memory consolidation happens during rest \u2014 the brain replays and stabilizes learned patterns during downtime.",
            "Active recall (testing yourself) builds 3x stronger memory than re-reading notes.",
            "Spaced repetition: review at day 1, day 3, day 7, day 30 for maximum retention.",
            "The Feynman Technique: if you can't explain it simply, you don't fully understand it yet.",
            "Error analysis: the problems you got WRONG are your highest-value review items \u2014 spend the most time there."
        ],
        "Tasks": [
            "<strong>Active Recall:</strong> Write the Gaps & Islands CTE from memory on paper. Check it against Day 4.",
            "<strong>Re-attempt:</strong> Pick the hardest problem from this week and solve it in 20 minutes from scratch.",
            "<strong>Teach it:</strong> Explain RANK vs DENSE_RANK to the AI chatbot as if it's a junior. Get it right without notes.",
            "<strong>Preview:</strong> Skim next week's Monday topic for 10 minutes \u2014 just headline concepts."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Review Data Modeling concepts + push labs to GitHub",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Review Data Modeling concepts + push labs to GitHub \u2014 trace through 2-3 edge cases before writing any code.",
                "Use the AI chatbot to quiz yourself \u2014 ask it 'Give me a SQL window function interview question.'",
                "Write SQL from memory in a plain text editor \u2014 no autocomplete. This builds true recall.",
                "The hardest concept this week: spend 30 minutes writing a full explanation in your own words."
            ]
        },
        "HardProblem": "Connect-the-dots: Write 2-3 paragraphs explaining how ROW_NUMBER, rolling averages, LAG, and Gaps & Islands could ALL be used in a single production pipeline for a ride-sharing company. What problem does each solve? How do they complement each other? What would break if you removed any one of them?"
    },
    {
        "Week": 4,
        "Day": "Monday",
        "Phase": "1. Filter (Coding)",
        "Theme": "Python Logic",
        "SpecificTopic": "Hash Maps (Dicts)",
        "ActionItem_Deliverable": "Solve LeetCode 1 (Two Sum) in O(N) time",
        "LeetCodeProblem": "<strong>LC 1 \u2013 Two Sum</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>NoSQL Modeling</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Why These Structures Matter in DE</div><div class=\"rich\"><h4>Why Python Data Engineers Need These Structures</h4><p>SQL handles sets of rows. Python handles the transformations, pipelines, and logic that SQL cannot: parsing nested JSON, building lookups between datasets, deduplicating records, implementing custom aggregations, streaming transformations, and interview-style algorithmic challenges.</p><p>Week 4 is about the Python data structures and patterns that show up constantly in data engineering interviews at FAANG \u2014 hash maps, sets, generators, sliding windows, and heaps. These are NOT just LeetCode prep. They solve real data problems every day.</p><h4>Hash Maps (Dictionaries) \u2014 The Single Most Important Python Structure</h4><p>A hash map (Python: <code>dict</code>) stores key-value pairs and provides O(1) average case lookup. Internally, Python's dict uses a hash table: it hashes the key, computes an array index, and stores the value there. This means finding any key requires computing one hash function \u2014 NOT scanning 10M items. This is the fundamental property that makes hash maps so powerful.</p><pre># Hash map lookup time does not grow with size\nd = {}  # empty dict \u2014 takes up ~200 bytes\nfor i in range(10_000_000):\n    d[i] = i * 2  # 10 million entries\n\n# This lookup is STILL O(1) \u2014 same speed as the empty dict\nprint(d[9_999_999])  # instant\n\n# Compare to a list:\nlst = list(range(10_000_000))\n9_999_999 in lst  # O(n) \u2014 has to scan all 10M items\nlst.index(9_999_999)  # O(n) \u2014 same\n\n# Rule: if you find yourself doing \"x in my_list\" in a loop, use a set or dict instead.</pre><h4>The Three Core Hash Map Patterns in Data Engineering</h4><ul><li><strong>Frequency counting</strong>: count occurrences of any key in O(n)</li><li><strong>Grouping / bucketing</strong>: group rows by a key without sorting</li><li><strong>Two-pass lookup</strong>: build a lookup table in pass 1, answer queries in pass 2</li></ul></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Hash Map Patterns \u2014 Counter, Group, Join</div><div class=\"rich\"><h4>Pattern 1: Frequency Counting \u2014 The Counter Pattern</h4><p><code>collections.Counter</code> is a specialized dict for counting. It is the right tool for any \"how many times does X appear?\" question.</p><pre>from collections import Counter\n\n# Raw data: a stream of user events\nevents = [\"click\",\"scroll\",\"click\",\"purchase\",\"click\",\"scroll\",\"purchase\"]\n\n# Count frequencies in O(n)\nfreq = Counter(events)\n# Counter({\"click\": 3, \"scroll\": 2, \"purchase\": 2})\n\n# Most common events\ntop_3 = freq.most_common(3)  # [(\"click\",3), (\"scroll\",2), (\"purchase\",2)]\n\n# In data engineering: count most frequent API error codes\nerror_logs = [\"404\",\"500\",\"404\",\"503\",\"404\",\"500\",\"404\"]\nerror_counts = Counter(error_logs)\nprint(error_counts.most_common(2))  # [('404', 4), ('500', 2)]\n\n# Interview pattern: does any value appear more than N/2 times?\ndef majority_element(nums):\n    c = Counter(nums)\n    return max(c, key=c.get)  # element with highest count</pre><h4>Pattern 2: Grouping \u2014 defaultdict</h4><pre>from collections import defaultdict\n\n# Group transactions by customer_id\ntransactions = [\n    {\"cust\": 42, \"amount\": 100},\n    {\"cust\": 55, \"amount\": 200},\n    {\"cust\": 42, \"amount\": 50},\n]\n\nby_customer = defaultdict(list)  # default value is an empty list\nfor txn in transactions:\n    by_customer[txn[\"cust\"]].append(txn[\"amount\"])\n\n# Result: {42: [100, 50], 55: [200]}\n\n# Now compute per-customer totals in O(n) total:\ntotals = {cust: sum(amounts) for cust, amounts in by_customer.items()}</pre><h4>Pattern 3: Two-Pass Lookup \u2014 Find Complement</h4><pre># Classic interview: Two Sum \u2014 find pair that sums to target\n# Brute force: O(n^2) \u2014 check every pair\n# Hash map approach: O(n) \u2014 one pass\n\ndef two_sum(nums, target):\n    seen = {}  # value \u2192 index\n    for i, num in enumerate(nums):\n        complement = target - num\n        if complement in seen:   # O(1) lookup!\n            return [seen[complement], i]\n        seen[num] = i\n    return []\n\n# Data engineering version: join two datasets in memory\n# (when SQL JOIN is not available, e.g. in a Python streaming pipeline)\ndef hash_join(left_rows, right_rows):\n    # Pass 1: build lookup from right table\n    lookup = {row[\"id\"]: row for row in right_rows}  # O(n)\n\n    # Pass 2: look up each left row in O(1)\n    for left in left_rows:  # O(m)\n        right = lookup.get(left[\"right_id\"])\n        if right:\n            yield {**left, **right}  # merged row\n    # Total: O(n + m) instead of O(n * m)</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Sets + Sliding Window Technique</div><div class=\"rich\"><h4>Sets \u2014 O(1) Membership and Deduplication</h4><p>A Python <code>set</code> is essentially a dict with only keys (no values). It provides O(1) membership testing and deduplication. In data engineering: dedup large record streams, compute intersection/difference of ID lists.</p><pre># Deduplication \u2014 preserve only unique records\nuser_ids = [1, 2, 3, 2, 4, 1, 5, 3]\nunique_ids = list(set(user_ids))  # [1, 2, 3, 4, 5] \u2014 order not preserved!\n\n# If order matters: use dict.fromkeys()\nunique_ordered = list(dict.fromkeys(user_ids))  # [1, 2, 3, 4, 5] \u2014 order preserved\n\n# Set operations for data reconciliation:\ndb_users = {1, 2, 3, 4, 5}\napp_users = {3, 4, 5, 6, 7}\n\nonly_in_db  = db_users - app_users   # {1, 2}    \u2014 in DB but not app\nonly_in_app = app_users - db_users   # {6, 7}    \u2014 in app but not DB\nin_both     = db_users & app_users   # {3, 4, 5} \u2014 intersection\nin_either   = db_users | app_users   # {1,2,3,4,5,6,7} \u2014 union\n\n# DE use case: find orders with no matching customer (data quality check)\norder_customer_ids = {o[\"cust_id\"] for o in orders}\ncustomer_ids       = {c[\"id\"] for c in customers}\norphan_orders = order_customer_ids - customer_ids  # cust IDs with no record</pre><h4>Sliding Window \u2014 Fixed and Variable Width</h4><p>Sliding window is NOT a data structure \u2014 it's an algorithmic technique. Instead of computing an aggregate for every possible window from scratch (O(n\u00d7k)), you maintain a running state as the window slides, re-using previous computation: O(n).</p><pre># Fixed window: 3-day rolling sum\ndef rolling_sum(values, k):\n    window_sum = sum(values[:k])  # compute first window\n    results = [window_sum]\n    for i in range(k, len(values)):\n        window_sum += values[i]       # add new element\n        window_sum -= values[i - k]   # remove oldest element\n        results.append(window_sum)\n    return results\n\n# Variable window: longest subarray with sum <= budget\ndef longest_under_budget(costs, budget):\n    left = 0\n    current_sum = 0\n    max_len = 0\n    for right in range(len(costs)):\n        current_sum += costs[right]\n        while current_sum > budget:   # shrink window from left\n            current_sum -= costs[left]\n            left += 1\n        max_len = max(max_len, right - left + 1)\n    return max_len</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Heaps + Complexity Cheat Sheet</div><div class=\"rich\"><h4>Heaps \u2014 Priority Queues for Streaming Top-K</h4><p>A <strong>heap</strong> (Python: <code>heapq</code>) is a binary tree that always keeps the smallest element at the root. It provides O(log n) insert and O(log n) extract-min. This makes it ideal for \"top K\" problems in streaming data \u2014 maintain a heap of exactly K items, each incoming item either replaces the smallest or is discarded.</p><pre>import heapq\n\n# Find top-3 highest-revenue customers from a stream of millions\n# No need to sort 10M records \u2014 O(n log k) with k=3\ndef top_k_customers(stream, k):\n    heap = []  # min-heap of (revenue, cust_id)\n    for cust_id, revenue in stream:\n        heapq.heappush(heap, (revenue, cust_id))  # add to heap\n        if len(heap) > k:\n            heapq.heappop(heap)  # remove smallest \u2014 keeps top K\n    return sorted(heap, reverse=True)  # highest first\n\n# heapq.nlargest is the Pythonic version:\ntop_3 = heapq.nlargest(3, stream, key=lambda x: x[1])\n\n# Merge K sorted streams (very common in data engineering)\n# e.g., merge sorted log files from 10 servers\ndef merge_k_sorted(sorted_streams):\n    heap = []\n    iterators = [iter(s) for s in sorted_streams]\n    for i, it in enumerate(iterators):\n        val = next(it, None)\n        if val is not None:\n            heapq.heappush(heap, (val, i))  # (value, stream_index)\n    while heap:\n        val, i = heapq.heappop(heap)\n        yield val\n        nxt = next(iterators[i], None)\n        if nxt is not None:\n            heapq.heappush(heap, (nxt, i))</pre><p><strong>Complexity cheat sheet for FAANG interviews:</strong></p><table><tr><th>Structure</th><td>Insert</td><td>Lookup</td><td>Delete</td><td>Best for</td></tr><tr><th>dict</th><td>O(1)</td><td>O(1)</td><td>O(1)</td><td>Counting, grouping, two-pointer complement</td></tr><tr><th>set</th><td>O(1)</td><td>O(1)</td><td>O(1)</td><td>Dedup, membership, set math</td></tr><tr><th>list</th><td>O(1) append</td><td>O(n)</td><td>O(n)</td><td>Ordered sequence, indexed access</td></tr><tr><th>heapq</th><td>O(log n)</td><td>O(1) min</td><td>O(log n)</td><td>Top-K, streaming priority</td></tr><tr><th>deque</th><td>O(1) both ends</td><td>O(n)</td><td>O(1) ends</td><td>Sliding window, BFS queue</td></tr></table></div></div></div>",
        "KeyConcepts": [
            "dict provides O(1) average insert, lookup, delete. Uses hash table internally.",
            "Counter: frequency counting in O(n). most_common(k) returns top-k elements.",
            "defaultdict: auto-initializes missing keys \u2014 ideal for grouping records by key.",
            "Two-pass hash join: build lookup dict in O(n), query in O(m). Total O(n+m) vs O(n*m) nested loop.",
            "set: O(1) membership. Supports union | , intersection &, difference -. Use for dedup + set math.",
            "dict.fromkeys(list): deduplicate while preserving insertion order (unlike set()).",
            "Sliding window: maintain running state instead of recomputing from scratch. O(n) vs O(n*k).",
            "heapq: min-heap. Push/pop in O(log n). Use for top-K streaming, merge K sorted sequences."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Given a list of 1M user_ids with duplicates, find the top-5 most frequent users using Counter. Then dedup the list preserving order using dict.fromkeys().",
            "<strong>Step 2:</strong> Implement a hash join: two lists of dicts (orders and customers) sharing a customer_id field. Merge them in O(n+m) using a dict lookup.",
            "<strong>Step 3:</strong> Sliding window: find the max sum of any 3 consecutive days from 365 daily revenue values. Implement in O(n) \u2014 no nested loop.",
            "<strong>Step 4:</strong> Top-K streaming: simulate a stream of (user_id, purchase_amount) tuples. Find the top-10 spenders using heapq without storing the entire stream in memory."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Solve LeetCode 1 (Two Sum) in O(N) time",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Solve LeetCode 1 (Two Sum) in O(N) time \u2014 trace through 2-3 edge cases before writing any code.",
                "Seeing 'x in my_list' inside a loop? That's O(n\u00b2). Convert to set first: O(n) total.",
                "Counter(iterable) counts in one line. Combine two counters: c1 + c2 adds counts, c1 & c2 keeps minimums.",
                "heapq is a MIN-heap. For max-heap: store values as negatives (-val, key) then negate on extraction.",
                "Sliding window: two-pointer left/right. Move right always; move left only when constraint violated."
            ]
        },
        "HardProblem": "Boss Problem (Google): You receive a stream of (user_id, search_query, timestamp) events, 100M events/day. (1) Find the top-100 most searched queries in the last 24 hours \u2014 you cannot load all 100M events into RAM. Use a heap + streaming approach. (2) Find all users who searched for the same query more than 5 times \u2014 are they bots? Use Counter + filter. (3) Find all pairs of users who searched for the SAME rare query (appears < 10 times globally). How do you efficiently match pairs? (4) Time complexity analysis: what is the big-O of each part?"
    },
    {
        "Week": 4,
        "Day": "Tuesday",
        "Phase": "1. Filter (Coding)",
        "Theme": "Python Logic",
        "SpecificTopic": "Sets & Deduplication",
        "ActionItem_Deliverable": "Solve LeetCode 217 (Contains Duplicate)",
        "LeetCodeProblem": "<strong>LC 349 \u2013 Intersection of Two Arrays</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Execution Plans</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Why These Structures Matter in DE</div><div class=\"rich\"><h4>Why Python Data Engineers Need These Structures</h4><p>SQL handles sets of rows. Python handles the transformations, pipelines, and logic that SQL cannot: parsing nested JSON, building lookups between datasets, deduplicating records, implementing custom aggregations, streaming transformations, and interview-style algorithmic challenges.</p><p>Week 4 is about the Python data structures and patterns that show up constantly in data engineering interviews at FAANG \u2014 hash maps, sets, generators, sliding windows, and heaps. These are NOT just LeetCode prep. They solve real data problems every day.</p><h4>Hash Maps (Dictionaries) \u2014 The Single Most Important Python Structure</h4><p>A hash map (Python: <code>dict</code>) stores key-value pairs and provides O(1) average case lookup. Internally, Python's dict uses a hash table: it hashes the key, computes an array index, and stores the value there. This means finding any key requires computing one hash function \u2014 NOT scanning 10M items. This is the fundamental property that makes hash maps so powerful.</p><pre># Hash map lookup time does not grow with size\nd = {}  # empty dict \u2014 takes up ~200 bytes\nfor i in range(10_000_000):\n    d[i] = i * 2  # 10 million entries\n\n# This lookup is STILL O(1) \u2014 same speed as the empty dict\nprint(d[9_999_999])  # instant\n\n# Compare to a list:\nlst = list(range(10_000_000))\n9_999_999 in lst  # O(n) \u2014 has to scan all 10M items\nlst.index(9_999_999)  # O(n) \u2014 same\n\n# Rule: if you find yourself doing \"x in my_list\" in a loop, use a set or dict instead.</pre><h4>The Three Core Hash Map Patterns in Data Engineering</h4><ul><li><strong>Frequency counting</strong>: count occurrences of any key in O(n)</li><li><strong>Grouping / bucketing</strong>: group rows by a key without sorting</li><li><strong>Two-pass lookup</strong>: build a lookup table in pass 1, answer queries in pass 2</li></ul></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Hash Map Patterns \u2014 Counter, Group, Join</div><div class=\"rich\"><h4>Pattern 1: Frequency Counting \u2014 The Counter Pattern</h4><p><code>collections.Counter</code> is a specialized dict for counting. It is the right tool for any \"how many times does X appear?\" question.</p><pre>from collections import Counter\n\n# Raw data: a stream of user events\nevents = [\"click\",\"scroll\",\"click\",\"purchase\",\"click\",\"scroll\",\"purchase\"]\n\n# Count frequencies in O(n)\nfreq = Counter(events)\n# Counter({\"click\": 3, \"scroll\": 2, \"purchase\": 2})\n\n# Most common events\ntop_3 = freq.most_common(3)  # [(\"click\",3), (\"scroll\",2), (\"purchase\",2)]\n\n# In data engineering: count most frequent API error codes\nerror_logs = [\"404\",\"500\",\"404\",\"503\",\"404\",\"500\",\"404\"]\nerror_counts = Counter(error_logs)\nprint(error_counts.most_common(2))  # [('404', 4), ('500', 2)]\n\n# Interview pattern: does any value appear more than N/2 times?\ndef majority_element(nums):\n    c = Counter(nums)\n    return max(c, key=c.get)  # element with highest count</pre><h4>Pattern 2: Grouping \u2014 defaultdict</h4><pre>from collections import defaultdict\n\n# Group transactions by customer_id\ntransactions = [\n    {\"cust\": 42, \"amount\": 100},\n    {\"cust\": 55, \"amount\": 200},\n    {\"cust\": 42, \"amount\": 50},\n]\n\nby_customer = defaultdict(list)  # default value is an empty list\nfor txn in transactions:\n    by_customer[txn[\"cust\"]].append(txn[\"amount\"])\n\n# Result: {42: [100, 50], 55: [200]}\n\n# Now compute per-customer totals in O(n) total:\ntotals = {cust: sum(amounts) for cust, amounts in by_customer.items()}</pre><h4>Pattern 3: Two-Pass Lookup \u2014 Find Complement</h4><pre># Classic interview: Two Sum \u2014 find pair that sums to target\n# Brute force: O(n^2) \u2014 check every pair\n# Hash map approach: O(n) \u2014 one pass\n\ndef two_sum(nums, target):\n    seen = {}  # value \u2192 index\n    for i, num in enumerate(nums):\n        complement = target - num\n        if complement in seen:   # O(1) lookup!\n            return [seen[complement], i]\n        seen[num] = i\n    return []\n\n# Data engineering version: join two datasets in memory\n# (when SQL JOIN is not available, e.g. in a Python streaming pipeline)\ndef hash_join(left_rows, right_rows):\n    # Pass 1: build lookup from right table\n    lookup = {row[\"id\"]: row for row in right_rows}  # O(n)\n\n    # Pass 2: look up each left row in O(1)\n    for left in left_rows:  # O(m)\n        right = lookup.get(left[\"right_id\"])\n        if right:\n            yield {**left, **right}  # merged row\n    # Total: O(n + m) instead of O(n * m)</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Sets + Sliding Window Technique</div><div class=\"rich\"><h4>Sets \u2014 O(1) Membership and Deduplication</h4><p>A Python <code>set</code> is essentially a dict with only keys (no values). It provides O(1) membership testing and deduplication. In data engineering: dedup large record streams, compute intersection/difference of ID lists.</p><pre># Deduplication \u2014 preserve only unique records\nuser_ids = [1, 2, 3, 2, 4, 1, 5, 3]\nunique_ids = list(set(user_ids))  # [1, 2, 3, 4, 5] \u2014 order not preserved!\n\n# If order matters: use dict.fromkeys()\nunique_ordered = list(dict.fromkeys(user_ids))  # [1, 2, 3, 4, 5] \u2014 order preserved\n\n# Set operations for data reconciliation:\ndb_users = {1, 2, 3, 4, 5}\napp_users = {3, 4, 5, 6, 7}\n\nonly_in_db  = db_users - app_users   # {1, 2}    \u2014 in DB but not app\nonly_in_app = app_users - db_users   # {6, 7}    \u2014 in app but not DB\nin_both     = db_users & app_users   # {3, 4, 5} \u2014 intersection\nin_either   = db_users | app_users   # {1,2,3,4,5,6,7} \u2014 union\n\n# DE use case: find orders with no matching customer (data quality check)\norder_customer_ids = {o[\"cust_id\"] for o in orders}\ncustomer_ids       = {c[\"id\"] for c in customers}\norphan_orders = order_customer_ids - customer_ids  # cust IDs with no record</pre><h4>Sliding Window \u2014 Fixed and Variable Width</h4><p>Sliding window is NOT a data structure \u2014 it's an algorithmic technique. Instead of computing an aggregate for every possible window from scratch (O(n\u00d7k)), you maintain a running state as the window slides, re-using previous computation: O(n).</p><pre># Fixed window: 3-day rolling sum\ndef rolling_sum(values, k):\n    window_sum = sum(values[:k])  # compute first window\n    results = [window_sum]\n    for i in range(k, len(values)):\n        window_sum += values[i]       # add new element\n        window_sum -= values[i - k]   # remove oldest element\n        results.append(window_sum)\n    return results\n\n# Variable window: longest subarray with sum <= budget\ndef longest_under_budget(costs, budget):\n    left = 0\n    current_sum = 0\n    max_len = 0\n    for right in range(len(costs)):\n        current_sum += costs[right]\n        while current_sum > budget:   # shrink window from left\n            current_sum -= costs[left]\n            left += 1\n        max_len = max(max_len, right - left + 1)\n    return max_len</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Heaps + Complexity Cheat Sheet</div><div class=\"rich\"><h4>Heaps \u2014 Priority Queues for Streaming Top-K</h4><p>A <strong>heap</strong> (Python: <code>heapq</code>) is a binary tree that always keeps the smallest element at the root. It provides O(log n) insert and O(log n) extract-min. This makes it ideal for \"top K\" problems in streaming data \u2014 maintain a heap of exactly K items, each incoming item either replaces the smallest or is discarded.</p><pre>import heapq\n\n# Find top-3 highest-revenue customers from a stream of millions\n# No need to sort 10M records \u2014 O(n log k) with k=3\ndef top_k_customers(stream, k):\n    heap = []  # min-heap of (revenue, cust_id)\n    for cust_id, revenue in stream:\n        heapq.heappush(heap, (revenue, cust_id))  # add to heap\n        if len(heap) > k:\n            heapq.heappop(heap)  # remove smallest \u2014 keeps top K\n    return sorted(heap, reverse=True)  # highest first\n\n# heapq.nlargest is the Pythonic version:\ntop_3 = heapq.nlargest(3, stream, key=lambda x: x[1])\n\n# Merge K sorted streams (very common in data engineering)\n# e.g., merge sorted log files from 10 servers\ndef merge_k_sorted(sorted_streams):\n    heap = []\n    iterators = [iter(s) for s in sorted_streams]\n    for i, it in enumerate(iterators):\n        val = next(it, None)\n        if val is not None:\n            heapq.heappush(heap, (val, i))  # (value, stream_index)\n    while heap:\n        val, i = heapq.heappop(heap)\n        yield val\n        nxt = next(iterators[i], None)\n        if nxt is not None:\n            heapq.heappush(heap, (nxt, i))</pre><p><strong>Complexity cheat sheet for FAANG interviews:</strong></p><table><tr><th>Structure</th><td>Insert</td><td>Lookup</td><td>Delete</td><td>Best for</td></tr><tr><th>dict</th><td>O(1)</td><td>O(1)</td><td>O(1)</td><td>Counting, grouping, two-pointer complement</td></tr><tr><th>set</th><td>O(1)</td><td>O(1)</td><td>O(1)</td><td>Dedup, membership, set math</td></tr><tr><th>list</th><td>O(1) append</td><td>O(n)</td><td>O(n)</td><td>Ordered sequence, indexed access</td></tr><tr><th>heapq</th><td>O(log n)</td><td>O(1) min</td><td>O(log n)</td><td>Top-K, streaming priority</td></tr><tr><th>deque</th><td>O(1) both ends</td><td>O(n)</td><td>O(1) ends</td><td>Sliding window, BFS queue</td></tr></table></div></div></div>",
        "KeyConcepts": [
            "dict provides O(1) average insert, lookup, delete. Uses hash table internally.",
            "Counter: frequency counting in O(n). most_common(k) returns top-k elements.",
            "defaultdict: auto-initializes missing keys \u2014 ideal for grouping records by key.",
            "Two-pass hash join: build lookup dict in O(n), query in O(m). Total O(n+m) vs O(n*m) nested loop.",
            "set: O(1) membership. Supports union | , intersection &, difference -. Use for dedup + set math.",
            "dict.fromkeys(list): deduplicate while preserving insertion order (unlike set()).",
            "Sliding window: maintain running state instead of recomputing from scratch. O(n) vs O(n*k).",
            "heapq: min-heap. Push/pop in O(log n). Use for top-K streaming, merge K sorted sequences."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Given a list of 1M user_ids with duplicates, find the top-5 most frequent users using Counter. Then dedup the list preserving order using dict.fromkeys().",
            "<strong>Step 2:</strong> Implement a hash join: two lists of dicts (orders and customers) sharing a customer_id field. Merge them in O(n+m) using a dict lookup.",
            "<strong>Step 3:</strong> Sliding window: find the max sum of any 3 consecutive days from 365 daily revenue values. Implement in O(n) \u2014 no nested loop.",
            "<strong>Step 4:</strong> Top-K streaming: simulate a stream of (user_id, purchase_amount) tuples. Find the top-10 spenders using heapq without storing the entire stream in memory."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Solve LeetCode 217 (Contains Duplicate)",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Solve LeetCode 217 (Contains Duplicate) \u2014 trace through 2-3 edge cases before writing any code.",
                "Seeing 'x in my_list' inside a loop? That's O(n\u00b2). Convert to set first: O(n) total.",
                "Counter(iterable) counts in one line. Combine two counters: c1 + c2 adds counts, c1 & c2 keeps minimums.",
                "heapq is a MIN-heap. For max-heap: store values as negatives (-val, key) then negate on extraction.",
                "Sliding window: two-pointer left/right. Move right always; move left only when constraint violated."
            ]
        },
        "HardProblem": "Boss Problem (Google): You receive a stream of (user_id, search_query, timestamp) events, 100M events/day. (1) Find the top-100 most searched queries in the last 24 hours \u2014 you cannot load all 100M events into RAM. Use a heap + streaming approach. (2) Find all users who searched for the same query more than 5 times \u2014 are they bots? Use Counter + filter. (3) Find all pairs of users who searched for the SAME rare query (appears < 10 times globally). How do you efficiently match pairs? (4) Time complexity analysis: what is the big-O of each part?"
    },
    {
        "Week": 4,
        "Day": "Wednesday",
        "Phase": "1. Filter (Coding)",
        "Theme": "Python Logic",
        "SpecificTopic": "Sliding Window",
        "ActionItem_Deliverable": "Solve LeetCode 121 (Best Time to Buy Stock)",
        "LeetCodeProblem": "<strong>LC 3 \u2013 Longest Substring Without Repeat</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Gaps & Islands (Complex)</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Why These Structures Matter in DE</div><div class=\"rich\"><h4>Why Python Data Engineers Need These Structures</h4><p>SQL handles sets of rows. Python handles the transformations, pipelines, and logic that SQL cannot: parsing nested JSON, building lookups between datasets, deduplicating records, implementing custom aggregations, streaming transformations, and interview-style algorithmic challenges.</p><p>Week 4 is about the Python data structures and patterns that show up constantly in data engineering interviews at FAANG \u2014 hash maps, sets, generators, sliding windows, and heaps. These are NOT just LeetCode prep. They solve real data problems every day.</p><h4>Hash Maps (Dictionaries) \u2014 The Single Most Important Python Structure</h4><p>A hash map (Python: <code>dict</code>) stores key-value pairs and provides O(1) average case lookup. Internally, Python's dict uses a hash table: it hashes the key, computes an array index, and stores the value there. This means finding any key requires computing one hash function \u2014 NOT scanning 10M items. This is the fundamental property that makes hash maps so powerful.</p><pre># Hash map lookup time does not grow with size\nd = {}  # empty dict \u2014 takes up ~200 bytes\nfor i in range(10_000_000):\n    d[i] = i * 2  # 10 million entries\n\n# This lookup is STILL O(1) \u2014 same speed as the empty dict\nprint(d[9_999_999])  # instant\n\n# Compare to a list:\nlst = list(range(10_000_000))\n9_999_999 in lst  # O(n) \u2014 has to scan all 10M items\nlst.index(9_999_999)  # O(n) \u2014 same\n\n# Rule: if you find yourself doing \"x in my_list\" in a loop, use a set or dict instead.</pre><h4>The Three Core Hash Map Patterns in Data Engineering</h4><ul><li><strong>Frequency counting</strong>: count occurrences of any key in O(n)</li><li><strong>Grouping / bucketing</strong>: group rows by a key without sorting</li><li><strong>Two-pass lookup</strong>: build a lookup table in pass 1, answer queries in pass 2</li></ul></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Hash Map Patterns \u2014 Counter, Group, Join</div><div class=\"rich\"><h4>Pattern 1: Frequency Counting \u2014 The Counter Pattern</h4><p><code>collections.Counter</code> is a specialized dict for counting. It is the right tool for any \"how many times does X appear?\" question.</p><pre>from collections import Counter\n\n# Raw data: a stream of user events\nevents = [\"click\",\"scroll\",\"click\",\"purchase\",\"click\",\"scroll\",\"purchase\"]\n\n# Count frequencies in O(n)\nfreq = Counter(events)\n# Counter({\"click\": 3, \"scroll\": 2, \"purchase\": 2})\n\n# Most common events\ntop_3 = freq.most_common(3)  # [(\"click\",3), (\"scroll\",2), (\"purchase\",2)]\n\n# In data engineering: count most frequent API error codes\nerror_logs = [\"404\",\"500\",\"404\",\"503\",\"404\",\"500\",\"404\"]\nerror_counts = Counter(error_logs)\nprint(error_counts.most_common(2))  # [('404', 4), ('500', 2)]\n\n# Interview pattern: does any value appear more than N/2 times?\ndef majority_element(nums):\n    c = Counter(nums)\n    return max(c, key=c.get)  # element with highest count</pre><h4>Pattern 2: Grouping \u2014 defaultdict</h4><pre>from collections import defaultdict\n\n# Group transactions by customer_id\ntransactions = [\n    {\"cust\": 42, \"amount\": 100},\n    {\"cust\": 55, \"amount\": 200},\n    {\"cust\": 42, \"amount\": 50},\n]\n\nby_customer = defaultdict(list)  # default value is an empty list\nfor txn in transactions:\n    by_customer[txn[\"cust\"]].append(txn[\"amount\"])\n\n# Result: {42: [100, 50], 55: [200]}\n\n# Now compute per-customer totals in O(n) total:\ntotals = {cust: sum(amounts) for cust, amounts in by_customer.items()}</pre><h4>Pattern 3: Two-Pass Lookup \u2014 Find Complement</h4><pre># Classic interview: Two Sum \u2014 find pair that sums to target\n# Brute force: O(n^2) \u2014 check every pair\n# Hash map approach: O(n) \u2014 one pass\n\ndef two_sum(nums, target):\n    seen = {}  # value \u2192 index\n    for i, num in enumerate(nums):\n        complement = target - num\n        if complement in seen:   # O(1) lookup!\n            return [seen[complement], i]\n        seen[num] = i\n    return []\n\n# Data engineering version: join two datasets in memory\n# (when SQL JOIN is not available, e.g. in a Python streaming pipeline)\ndef hash_join(left_rows, right_rows):\n    # Pass 1: build lookup from right table\n    lookup = {row[\"id\"]: row for row in right_rows}  # O(n)\n\n    # Pass 2: look up each left row in O(1)\n    for left in left_rows:  # O(m)\n        right = lookup.get(left[\"right_id\"])\n        if right:\n            yield {**left, **right}  # merged row\n    # Total: O(n + m) instead of O(n * m)</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Sets + Sliding Window Technique</div><div class=\"rich\"><h4>Sets \u2014 O(1) Membership and Deduplication</h4><p>A Python <code>set</code> is essentially a dict with only keys (no values). It provides O(1) membership testing and deduplication. In data engineering: dedup large record streams, compute intersection/difference of ID lists.</p><pre># Deduplication \u2014 preserve only unique records\nuser_ids = [1, 2, 3, 2, 4, 1, 5, 3]\nunique_ids = list(set(user_ids))  # [1, 2, 3, 4, 5] \u2014 order not preserved!\n\n# If order matters: use dict.fromkeys()\nunique_ordered = list(dict.fromkeys(user_ids))  # [1, 2, 3, 4, 5] \u2014 order preserved\n\n# Set operations for data reconciliation:\ndb_users = {1, 2, 3, 4, 5}\napp_users = {3, 4, 5, 6, 7}\n\nonly_in_db  = db_users - app_users   # {1, 2}    \u2014 in DB but not app\nonly_in_app = app_users - db_users   # {6, 7}    \u2014 in app but not DB\nin_both     = db_users & app_users   # {3, 4, 5} \u2014 intersection\nin_either   = db_users | app_users   # {1,2,3,4,5,6,7} \u2014 union\n\n# DE use case: find orders with no matching customer (data quality check)\norder_customer_ids = {o[\"cust_id\"] for o in orders}\ncustomer_ids       = {c[\"id\"] for c in customers}\norphan_orders = order_customer_ids - customer_ids  # cust IDs with no record</pre><h4>Sliding Window \u2014 Fixed and Variable Width</h4><p>Sliding window is NOT a data structure \u2014 it's an algorithmic technique. Instead of computing an aggregate for every possible window from scratch (O(n\u00d7k)), you maintain a running state as the window slides, re-using previous computation: O(n).</p><pre># Fixed window: 3-day rolling sum\ndef rolling_sum(values, k):\n    window_sum = sum(values[:k])  # compute first window\n    results = [window_sum]\n    for i in range(k, len(values)):\n        window_sum += values[i]       # add new element\n        window_sum -= values[i - k]   # remove oldest element\n        results.append(window_sum)\n    return results\n\n# Variable window: longest subarray with sum <= budget\ndef longest_under_budget(costs, budget):\n    left = 0\n    current_sum = 0\n    max_len = 0\n    for right in range(len(costs)):\n        current_sum += costs[right]\n        while current_sum > budget:   # shrink window from left\n            current_sum -= costs[left]\n            left += 1\n        max_len = max(max_len, right - left + 1)\n    return max_len</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Heaps + Complexity Cheat Sheet</div><div class=\"rich\"><h4>Heaps \u2014 Priority Queues for Streaming Top-K</h4><p>A <strong>heap</strong> (Python: <code>heapq</code>) is a binary tree that always keeps the smallest element at the root. It provides O(log n) insert and O(log n) extract-min. This makes it ideal for \"top K\" problems in streaming data \u2014 maintain a heap of exactly K items, each incoming item either replaces the smallest or is discarded.</p><pre>import heapq\n\n# Find top-3 highest-revenue customers from a stream of millions\n# No need to sort 10M records \u2014 O(n log k) with k=3\ndef top_k_customers(stream, k):\n    heap = []  # min-heap of (revenue, cust_id)\n    for cust_id, revenue in stream:\n        heapq.heappush(heap, (revenue, cust_id))  # add to heap\n        if len(heap) > k:\n            heapq.heappop(heap)  # remove smallest \u2014 keeps top K\n    return sorted(heap, reverse=True)  # highest first\n\n# heapq.nlargest is the Pythonic version:\ntop_3 = heapq.nlargest(3, stream, key=lambda x: x[1])\n\n# Merge K sorted streams (very common in data engineering)\n# e.g., merge sorted log files from 10 servers\ndef merge_k_sorted(sorted_streams):\n    heap = []\n    iterators = [iter(s) for s in sorted_streams]\n    for i, it in enumerate(iterators):\n        val = next(it, None)\n        if val is not None:\n            heapq.heappush(heap, (val, i))  # (value, stream_index)\n    while heap:\n        val, i = heapq.heappop(heap)\n        yield val\n        nxt = next(iterators[i], None)\n        if nxt is not None:\n            heapq.heappush(heap, (nxt, i))</pre><p><strong>Complexity cheat sheet for FAANG interviews:</strong></p><table><tr><th>Structure</th><td>Insert</td><td>Lookup</td><td>Delete</td><td>Best for</td></tr><tr><th>dict</th><td>O(1)</td><td>O(1)</td><td>O(1)</td><td>Counting, grouping, two-pointer complement</td></tr><tr><th>set</th><td>O(1)</td><td>O(1)</td><td>O(1)</td><td>Dedup, membership, set math</td></tr><tr><th>list</th><td>O(1) append</td><td>O(n)</td><td>O(n)</td><td>Ordered sequence, indexed access</td></tr><tr><th>heapq</th><td>O(log n)</td><td>O(1) min</td><td>O(log n)</td><td>Top-K, streaming priority</td></tr><tr><th>deque</th><td>O(1) both ends</td><td>O(n)</td><td>O(1) ends</td><td>Sliding window, BFS queue</td></tr></table></div></div></div>",
        "KeyConcepts": [
            "dict provides O(1) average insert, lookup, delete. Uses hash table internally.",
            "Counter: frequency counting in O(n). most_common(k) returns top-k elements.",
            "defaultdict: auto-initializes missing keys \u2014 ideal for grouping records by key.",
            "Two-pass hash join: build lookup dict in O(n), query in O(m). Total O(n+m) vs O(n*m) nested loop.",
            "set: O(1) membership. Supports union | , intersection &, difference -. Use for dedup + set math.",
            "dict.fromkeys(list): deduplicate while preserving insertion order (unlike set()).",
            "Sliding window: maintain running state instead of recomputing from scratch. O(n) vs O(n*k).",
            "heapq: min-heap. Push/pop in O(log n). Use for top-K streaming, merge K sorted sequences."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Given a list of 1M user_ids with duplicates, find the top-5 most frequent users using Counter. Then dedup the list preserving order using dict.fromkeys().",
            "<strong>Step 2:</strong> Implement a hash join: two lists of dicts (orders and customers) sharing a customer_id field. Merge them in O(n+m) using a dict lookup.",
            "<strong>Step 3:</strong> Sliding window: find the max sum of any 3 consecutive days from 365 daily revenue values. Implement in O(n) \u2014 no nested loop.",
            "<strong>Step 4:</strong> Top-K streaming: simulate a stream of (user_id, purchase_amount) tuples. Find the top-10 spenders using heapq without storing the entire stream in memory."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Solve LeetCode 121 (Best Time to Buy Stock)",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Solve LeetCode 121 (Best Time to Buy Stock) \u2014 trace through 2-3 edge cases before writing any code.",
                "Seeing 'x in my_list' inside a loop? That's O(n\u00b2). Convert to set first: O(n) total.",
                "Counter(iterable) counts in one line. Combine two counters: c1 + c2 adds counts, c1 & c2 keeps minimums.",
                "heapq is a MIN-heap. For max-heap: store values as negatives (-val, key) then negate on extraction.",
                "Sliding window: two-pointer left/right. Move right always; move left only when constraint violated."
            ]
        },
        "HardProblem": "Boss Problem (Google): You receive a stream of (user_id, search_query, timestamp) events, 100M events/day. (1) Find the top-100 most searched queries in the last 24 hours \u2014 you cannot load all 100M events into RAM. Use a heap + streaming approach. (2) Find all users who searched for the same query more than 5 times \u2014 are they bots? Use Counter + filter. (3) Find all pairs of users who searched for the SAME rare query (appears < 10 times globally). How do you efficiently match pairs? (4) Time complexity analysis: what is the big-O of each part?"
    },
    {
        "Week": 4,
        "Day": "Thursday",
        "Phase": "1. Filter (Coding)",
        "Theme": "Python Logic",
        "SpecificTopic": "Heaps / Priority Queues",
        "ActionItem_Deliverable": "Solve LeetCode 347 (Top K Frequent Elements)",
        "LeetCodeProblem": "<strong>LC 215 \u2013 Kth Largest Element</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>NoSQL Modeling</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Why These Structures Matter in DE</div><div class=\"rich\"><h4>Why Python Data Engineers Need These Structures</h4><p>SQL handles sets of rows. Python handles the transformations, pipelines, and logic that SQL cannot: parsing nested JSON, building lookups between datasets, deduplicating records, implementing custom aggregations, streaming transformations, and interview-style algorithmic challenges.</p><p>Week 4 is about the Python data structures and patterns that show up constantly in data engineering interviews at FAANG \u2014 hash maps, sets, generators, sliding windows, and heaps. These are NOT just LeetCode prep. They solve real data problems every day.</p><h4>Hash Maps (Dictionaries) \u2014 The Single Most Important Python Structure</h4><p>A hash map (Python: <code>dict</code>) stores key-value pairs and provides O(1) average case lookup. Internally, Python's dict uses a hash table: it hashes the key, computes an array index, and stores the value there. This means finding any key requires computing one hash function \u2014 NOT scanning 10M items. This is the fundamental property that makes hash maps so powerful.</p><pre># Hash map lookup time does not grow with size\nd = {}  # empty dict \u2014 takes up ~200 bytes\nfor i in range(10_000_000):\n    d[i] = i * 2  # 10 million entries\n\n# This lookup is STILL O(1) \u2014 same speed as the empty dict\nprint(d[9_999_999])  # instant\n\n# Compare to a list:\nlst = list(range(10_000_000))\n9_999_999 in lst  # O(n) \u2014 has to scan all 10M items\nlst.index(9_999_999)  # O(n) \u2014 same\n\n# Rule: if you find yourself doing \"x in my_list\" in a loop, use a set or dict instead.</pre><h4>The Three Core Hash Map Patterns in Data Engineering</h4><ul><li><strong>Frequency counting</strong>: count occurrences of any key in O(n)</li><li><strong>Grouping / bucketing</strong>: group rows by a key without sorting</li><li><strong>Two-pass lookup</strong>: build a lookup table in pass 1, answer queries in pass 2</li></ul></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Hash Map Patterns \u2014 Counter, Group, Join</div><div class=\"rich\"><h4>Pattern 1: Frequency Counting \u2014 The Counter Pattern</h4><p><code>collections.Counter</code> is a specialized dict for counting. It is the right tool for any \"how many times does X appear?\" question.</p><pre>from collections import Counter\n\n# Raw data: a stream of user events\nevents = [\"click\",\"scroll\",\"click\",\"purchase\",\"click\",\"scroll\",\"purchase\"]\n\n# Count frequencies in O(n)\nfreq = Counter(events)\n# Counter({\"click\": 3, \"scroll\": 2, \"purchase\": 2})\n\n# Most common events\ntop_3 = freq.most_common(3)  # [(\"click\",3), (\"scroll\",2), (\"purchase\",2)]\n\n# In data engineering: count most frequent API error codes\nerror_logs = [\"404\",\"500\",\"404\",\"503\",\"404\",\"500\",\"404\"]\nerror_counts = Counter(error_logs)\nprint(error_counts.most_common(2))  # [('404', 4), ('500', 2)]\n\n# Interview pattern: does any value appear more than N/2 times?\ndef majority_element(nums):\n    c = Counter(nums)\n    return max(c, key=c.get)  # element with highest count</pre><h4>Pattern 2: Grouping \u2014 defaultdict</h4><pre>from collections import defaultdict\n\n# Group transactions by customer_id\ntransactions = [\n    {\"cust\": 42, \"amount\": 100},\n    {\"cust\": 55, \"amount\": 200},\n    {\"cust\": 42, \"amount\": 50},\n]\n\nby_customer = defaultdict(list)  # default value is an empty list\nfor txn in transactions:\n    by_customer[txn[\"cust\"]].append(txn[\"amount\"])\n\n# Result: {42: [100, 50], 55: [200]}\n\n# Now compute per-customer totals in O(n) total:\ntotals = {cust: sum(amounts) for cust, amounts in by_customer.items()}</pre><h4>Pattern 3: Two-Pass Lookup \u2014 Find Complement</h4><pre># Classic interview: Two Sum \u2014 find pair that sums to target\n# Brute force: O(n^2) \u2014 check every pair\n# Hash map approach: O(n) \u2014 one pass\n\ndef two_sum(nums, target):\n    seen = {}  # value \u2192 index\n    for i, num in enumerate(nums):\n        complement = target - num\n        if complement in seen:   # O(1) lookup!\n            return [seen[complement], i]\n        seen[num] = i\n    return []\n\n# Data engineering version: join two datasets in memory\n# (when SQL JOIN is not available, e.g. in a Python streaming pipeline)\ndef hash_join(left_rows, right_rows):\n    # Pass 1: build lookup from right table\n    lookup = {row[\"id\"]: row for row in right_rows}  # O(n)\n\n    # Pass 2: look up each left row in O(1)\n    for left in left_rows:  # O(m)\n        right = lookup.get(left[\"right_id\"])\n        if right:\n            yield {**left, **right}  # merged row\n    # Total: O(n + m) instead of O(n * m)</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Sets + Sliding Window Technique</div><div class=\"rich\"><h4>Sets \u2014 O(1) Membership and Deduplication</h4><p>A Python <code>set</code> is essentially a dict with only keys (no values). It provides O(1) membership testing and deduplication. In data engineering: dedup large record streams, compute intersection/difference of ID lists.</p><pre># Deduplication \u2014 preserve only unique records\nuser_ids = [1, 2, 3, 2, 4, 1, 5, 3]\nunique_ids = list(set(user_ids))  # [1, 2, 3, 4, 5] \u2014 order not preserved!\n\n# If order matters: use dict.fromkeys()\nunique_ordered = list(dict.fromkeys(user_ids))  # [1, 2, 3, 4, 5] \u2014 order preserved\n\n# Set operations for data reconciliation:\ndb_users = {1, 2, 3, 4, 5}\napp_users = {3, 4, 5, 6, 7}\n\nonly_in_db  = db_users - app_users   # {1, 2}    \u2014 in DB but not app\nonly_in_app = app_users - db_users   # {6, 7}    \u2014 in app but not DB\nin_both     = db_users & app_users   # {3, 4, 5} \u2014 intersection\nin_either   = db_users | app_users   # {1,2,3,4,5,6,7} \u2014 union\n\n# DE use case: find orders with no matching customer (data quality check)\norder_customer_ids = {o[\"cust_id\"] for o in orders}\ncustomer_ids       = {c[\"id\"] for c in customers}\norphan_orders = order_customer_ids - customer_ids  # cust IDs with no record</pre><h4>Sliding Window \u2014 Fixed and Variable Width</h4><p>Sliding window is NOT a data structure \u2014 it's an algorithmic technique. Instead of computing an aggregate for every possible window from scratch (O(n\u00d7k)), you maintain a running state as the window slides, re-using previous computation: O(n).</p><pre># Fixed window: 3-day rolling sum\ndef rolling_sum(values, k):\n    window_sum = sum(values[:k])  # compute first window\n    results = [window_sum]\n    for i in range(k, len(values)):\n        window_sum += values[i]       # add new element\n        window_sum -= values[i - k]   # remove oldest element\n        results.append(window_sum)\n    return results\n\n# Variable window: longest subarray with sum <= budget\ndef longest_under_budget(costs, budget):\n    left = 0\n    current_sum = 0\n    max_len = 0\n    for right in range(len(costs)):\n        current_sum += costs[right]\n        while current_sum > budget:   # shrink window from left\n            current_sum -= costs[left]\n            left += 1\n        max_len = max(max_len, right - left + 1)\n    return max_len</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Heaps + Complexity Cheat Sheet</div><div class=\"rich\"><h4>Heaps \u2014 Priority Queues for Streaming Top-K</h4><p>A <strong>heap</strong> (Python: <code>heapq</code>) is a binary tree that always keeps the smallest element at the root. It provides O(log n) insert and O(log n) extract-min. This makes it ideal for \"top K\" problems in streaming data \u2014 maintain a heap of exactly K items, each incoming item either replaces the smallest or is discarded.</p><pre>import heapq\n\n# Find top-3 highest-revenue customers from a stream of millions\n# No need to sort 10M records \u2014 O(n log k) with k=3\ndef top_k_customers(stream, k):\n    heap = []  # min-heap of (revenue, cust_id)\n    for cust_id, revenue in stream:\n        heapq.heappush(heap, (revenue, cust_id))  # add to heap\n        if len(heap) > k:\n            heapq.heappop(heap)  # remove smallest \u2014 keeps top K\n    return sorted(heap, reverse=True)  # highest first\n\n# heapq.nlargest is the Pythonic version:\ntop_3 = heapq.nlargest(3, stream, key=lambda x: x[1])\n\n# Merge K sorted streams (very common in data engineering)\n# e.g., merge sorted log files from 10 servers\ndef merge_k_sorted(sorted_streams):\n    heap = []\n    iterators = [iter(s) for s in sorted_streams]\n    for i, it in enumerate(iterators):\n        val = next(it, None)\n        if val is not None:\n            heapq.heappush(heap, (val, i))  # (value, stream_index)\n    while heap:\n        val, i = heapq.heappop(heap)\n        yield val\n        nxt = next(iterators[i], None)\n        if nxt is not None:\n            heapq.heappush(heap, (nxt, i))</pre><p><strong>Complexity cheat sheet for FAANG interviews:</strong></p><table><tr><th>Structure</th><td>Insert</td><td>Lookup</td><td>Delete</td><td>Best for</td></tr><tr><th>dict</th><td>O(1)</td><td>O(1)</td><td>O(1)</td><td>Counting, grouping, two-pointer complement</td></tr><tr><th>set</th><td>O(1)</td><td>O(1)</td><td>O(1)</td><td>Dedup, membership, set math</td></tr><tr><th>list</th><td>O(1) append</td><td>O(n)</td><td>O(n)</td><td>Ordered sequence, indexed access</td></tr><tr><th>heapq</th><td>O(log n)</td><td>O(1) min</td><td>O(log n)</td><td>Top-K, streaming priority</td></tr><tr><th>deque</th><td>O(1) both ends</td><td>O(n)</td><td>O(1) ends</td><td>Sliding window, BFS queue</td></tr></table></div></div></div>",
        "KeyConcepts": [
            "dict provides O(1) average insert, lookup, delete. Uses hash table internally.",
            "Counter: frequency counting in O(n). most_common(k) returns top-k elements.",
            "defaultdict: auto-initializes missing keys \u2014 ideal for grouping records by key.",
            "Two-pass hash join: build lookup dict in O(n), query in O(m). Total O(n+m) vs O(n*m) nested loop.",
            "set: O(1) membership. Supports union | , intersection &, difference -. Use for dedup + set math.",
            "dict.fromkeys(list): deduplicate while preserving insertion order (unlike set()).",
            "Sliding window: maintain running state instead of recomputing from scratch. O(n) vs O(n*k).",
            "heapq: min-heap. Push/pop in O(log n). Use for top-K streaming, merge K sorted sequences."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Given a list of 1M user_ids with duplicates, find the top-5 most frequent users using Counter. Then dedup the list preserving order using dict.fromkeys().",
            "<strong>Step 2:</strong> Implement a hash join: two lists of dicts (orders and customers) sharing a customer_id field. Merge them in O(n+m) using a dict lookup.",
            "<strong>Step 3:</strong> Sliding window: find the max sum of any 3 consecutive days from 365 daily revenue values. Implement in O(n) \u2014 no nested loop.",
            "<strong>Step 4:</strong> Top-K streaming: simulate a stream of (user_id, purchase_amount) tuples. Find the top-10 spenders using heapq without storing the entire stream in memory."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Solve LeetCode 347 (Top K Frequent Elements)",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Solve LeetCode 347 (Top K Frequent Elements) \u2014 trace through 2-3 edge cases before writing any code.",
                "Seeing 'x in my_list' inside a loop? That's O(n\u00b2). Convert to set first: O(n) total.",
                "Counter(iterable) counts in one line. Combine two counters: c1 + c2 adds counts, c1 & c2 keeps minimums.",
                "heapq is a MIN-heap. For max-heap: store values as negatives (-val, key) then negate on extraction.",
                "Sliding window: two-pointer left/right. Move right always; move left only when constraint violated."
            ]
        },
        "HardProblem": "Boss Problem (Google): You receive a stream of (user_id, search_query, timestamp) events, 100M events/day. (1) Find the top-100 most searched queries in the last 24 hours \u2014 you cannot load all 100M events into RAM. Use a heap + streaming approach. (2) Find all users who searched for the same query more than 5 times \u2014 are they bots? Use Counter + filter. (3) Find all pairs of users who searched for the SAME rare query (appears < 10 times globally). How do you efficiently match pairs? (4) Time complexity analysis: what is the big-O of each part?"
    },
    {
        "Week": 4,
        "Day": "Friday",
        "Phase": "1. Filter (Coding)",
        "Theme": "Python Logic",
        "SpecificTopic": "String Manipulation",
        "ActionItem_Deliverable": "Solve LeetCode 49 (Group Anagrams) without Regex",
        "LeetCodeProblem": "<strong>LC 438 \u2013 Find All Anagrams in String</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Case Study: Social Media</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Why These Structures Matter in DE</div><div class=\"rich\"><h4>Why Python Data Engineers Need These Structures</h4><p>SQL handles sets of rows. Python handles the transformations, pipelines, and logic that SQL cannot: parsing nested JSON, building lookups between datasets, deduplicating records, implementing custom aggregations, streaming transformations, and interview-style algorithmic challenges.</p><p>Week 4 is about the Python data structures and patterns that show up constantly in data engineering interviews at FAANG \u2014 hash maps, sets, generators, sliding windows, and heaps. These are NOT just LeetCode prep. They solve real data problems every day.</p><h4>Hash Maps (Dictionaries) \u2014 The Single Most Important Python Structure</h4><p>A hash map (Python: <code>dict</code>) stores key-value pairs and provides O(1) average case lookup. Internally, Python's dict uses a hash table: it hashes the key, computes an array index, and stores the value there. This means finding any key requires computing one hash function \u2014 NOT scanning 10M items. This is the fundamental property that makes hash maps so powerful.</p><pre># Hash map lookup time does not grow with size\nd = {}  # empty dict \u2014 takes up ~200 bytes\nfor i in range(10_000_000):\n    d[i] = i * 2  # 10 million entries\n\n# This lookup is STILL O(1) \u2014 same speed as the empty dict\nprint(d[9_999_999])  # instant\n\n# Compare to a list:\nlst = list(range(10_000_000))\n9_999_999 in lst  # O(n) \u2014 has to scan all 10M items\nlst.index(9_999_999)  # O(n) \u2014 same\n\n# Rule: if you find yourself doing \"x in my_list\" in a loop, use a set or dict instead.</pre><h4>The Three Core Hash Map Patterns in Data Engineering</h4><ul><li><strong>Frequency counting</strong>: count occurrences of any key in O(n)</li><li><strong>Grouping / bucketing</strong>: group rows by a key without sorting</li><li><strong>Two-pass lookup</strong>: build a lookup table in pass 1, answer queries in pass 2</li></ul></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Hash Map Patterns \u2014 Counter, Group, Join</div><div class=\"rich\"><h4>Pattern 1: Frequency Counting \u2014 The Counter Pattern</h4><p><code>collections.Counter</code> is a specialized dict for counting. It is the right tool for any \"how many times does X appear?\" question.</p><pre>from collections import Counter\n\n# Raw data: a stream of user events\nevents = [\"click\",\"scroll\",\"click\",\"purchase\",\"click\",\"scroll\",\"purchase\"]\n\n# Count frequencies in O(n)\nfreq = Counter(events)\n# Counter({\"click\": 3, \"scroll\": 2, \"purchase\": 2})\n\n# Most common events\ntop_3 = freq.most_common(3)  # [(\"click\",3), (\"scroll\",2), (\"purchase\",2)]\n\n# In data engineering: count most frequent API error codes\nerror_logs = [\"404\",\"500\",\"404\",\"503\",\"404\",\"500\",\"404\"]\nerror_counts = Counter(error_logs)\nprint(error_counts.most_common(2))  # [('404', 4), ('500', 2)]\n\n# Interview pattern: does any value appear more than N/2 times?\ndef majority_element(nums):\n    c = Counter(nums)\n    return max(c, key=c.get)  # element with highest count</pre><h4>Pattern 2: Grouping \u2014 defaultdict</h4><pre>from collections import defaultdict\n\n# Group transactions by customer_id\ntransactions = [\n    {\"cust\": 42, \"amount\": 100},\n    {\"cust\": 55, \"amount\": 200},\n    {\"cust\": 42, \"amount\": 50},\n]\n\nby_customer = defaultdict(list)  # default value is an empty list\nfor txn in transactions:\n    by_customer[txn[\"cust\"]].append(txn[\"amount\"])\n\n# Result: {42: [100, 50], 55: [200]}\n\n# Now compute per-customer totals in O(n) total:\ntotals = {cust: sum(amounts) for cust, amounts in by_customer.items()}</pre><h4>Pattern 3: Two-Pass Lookup \u2014 Find Complement</h4><pre># Classic interview: Two Sum \u2014 find pair that sums to target\n# Brute force: O(n^2) \u2014 check every pair\n# Hash map approach: O(n) \u2014 one pass\n\ndef two_sum(nums, target):\n    seen = {}  # value \u2192 index\n    for i, num in enumerate(nums):\n        complement = target - num\n        if complement in seen:   # O(1) lookup!\n            return [seen[complement], i]\n        seen[num] = i\n    return []\n\n# Data engineering version: join two datasets in memory\n# (when SQL JOIN is not available, e.g. in a Python streaming pipeline)\ndef hash_join(left_rows, right_rows):\n    # Pass 1: build lookup from right table\n    lookup = {row[\"id\"]: row for row in right_rows}  # O(n)\n\n    # Pass 2: look up each left row in O(1)\n    for left in left_rows:  # O(m)\n        right = lookup.get(left[\"right_id\"])\n        if right:\n            yield {**left, **right}  # merged row\n    # Total: O(n + m) instead of O(n * m)</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Sets + Sliding Window Technique</div><div class=\"rich\"><h4>Sets \u2014 O(1) Membership and Deduplication</h4><p>A Python <code>set</code> is essentially a dict with only keys (no values). It provides O(1) membership testing and deduplication. In data engineering: dedup large record streams, compute intersection/difference of ID lists.</p><pre># Deduplication \u2014 preserve only unique records\nuser_ids = [1, 2, 3, 2, 4, 1, 5, 3]\nunique_ids = list(set(user_ids))  # [1, 2, 3, 4, 5] \u2014 order not preserved!\n\n# If order matters: use dict.fromkeys()\nunique_ordered = list(dict.fromkeys(user_ids))  # [1, 2, 3, 4, 5] \u2014 order preserved\n\n# Set operations for data reconciliation:\ndb_users = {1, 2, 3, 4, 5}\napp_users = {3, 4, 5, 6, 7}\n\nonly_in_db  = db_users - app_users   # {1, 2}    \u2014 in DB but not app\nonly_in_app = app_users - db_users   # {6, 7}    \u2014 in app but not DB\nin_both     = db_users & app_users   # {3, 4, 5} \u2014 intersection\nin_either   = db_users | app_users   # {1,2,3,4,5,6,7} \u2014 union\n\n# DE use case: find orders with no matching customer (data quality check)\norder_customer_ids = {o[\"cust_id\"] for o in orders}\ncustomer_ids       = {c[\"id\"] for c in customers}\norphan_orders = order_customer_ids - customer_ids  # cust IDs with no record</pre><h4>Sliding Window \u2014 Fixed and Variable Width</h4><p>Sliding window is NOT a data structure \u2014 it's an algorithmic technique. Instead of computing an aggregate for every possible window from scratch (O(n\u00d7k)), you maintain a running state as the window slides, re-using previous computation: O(n).</p><pre># Fixed window: 3-day rolling sum\ndef rolling_sum(values, k):\n    window_sum = sum(values[:k])  # compute first window\n    results = [window_sum]\n    for i in range(k, len(values)):\n        window_sum += values[i]       # add new element\n        window_sum -= values[i - k]   # remove oldest element\n        results.append(window_sum)\n    return results\n\n# Variable window: longest subarray with sum <= budget\ndef longest_under_budget(costs, budget):\n    left = 0\n    current_sum = 0\n    max_len = 0\n    for right in range(len(costs)):\n        current_sum += costs[right]\n        while current_sum > budget:   # shrink window from left\n            current_sum -= costs[left]\n            left += 1\n        max_len = max(max_len, right - left + 1)\n    return max_len</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Heaps + Complexity Cheat Sheet</div><div class=\"rich\"><h4>Heaps \u2014 Priority Queues for Streaming Top-K</h4><p>A <strong>heap</strong> (Python: <code>heapq</code>) is a binary tree that always keeps the smallest element at the root. It provides O(log n) insert and O(log n) extract-min. This makes it ideal for \"top K\" problems in streaming data \u2014 maintain a heap of exactly K items, each incoming item either replaces the smallest or is discarded.</p><pre>import heapq\n\n# Find top-3 highest-revenue customers from a stream of millions\n# No need to sort 10M records \u2014 O(n log k) with k=3\ndef top_k_customers(stream, k):\n    heap = []  # min-heap of (revenue, cust_id)\n    for cust_id, revenue in stream:\n        heapq.heappush(heap, (revenue, cust_id))  # add to heap\n        if len(heap) > k:\n            heapq.heappop(heap)  # remove smallest \u2014 keeps top K\n    return sorted(heap, reverse=True)  # highest first\n\n# heapq.nlargest is the Pythonic version:\ntop_3 = heapq.nlargest(3, stream, key=lambda x: x[1])\n\n# Merge K sorted streams (very common in data engineering)\n# e.g., merge sorted log files from 10 servers\ndef merge_k_sorted(sorted_streams):\n    heap = []\n    iterators = [iter(s) for s in sorted_streams]\n    for i, it in enumerate(iterators):\n        val = next(it, None)\n        if val is not None:\n            heapq.heappush(heap, (val, i))  # (value, stream_index)\n    while heap:\n        val, i = heapq.heappop(heap)\n        yield val\n        nxt = next(iterators[i], None)\n        if nxt is not None:\n            heapq.heappush(heap, (nxt, i))</pre><p><strong>Complexity cheat sheet for FAANG interviews:</strong></p><table><tr><th>Structure</th><td>Insert</td><td>Lookup</td><td>Delete</td><td>Best for</td></tr><tr><th>dict</th><td>O(1)</td><td>O(1)</td><td>O(1)</td><td>Counting, grouping, two-pointer complement</td></tr><tr><th>set</th><td>O(1)</td><td>O(1)</td><td>O(1)</td><td>Dedup, membership, set math</td></tr><tr><th>list</th><td>O(1) append</td><td>O(n)</td><td>O(n)</td><td>Ordered sequence, indexed access</td></tr><tr><th>heapq</th><td>O(log n)</td><td>O(1) min</td><td>O(log n)</td><td>Top-K, streaming priority</td></tr><tr><th>deque</th><td>O(1) both ends</td><td>O(n)</td><td>O(1) ends</td><td>Sliding window, BFS queue</td></tr></table></div></div></div>",
        "KeyConcepts": [
            "dict provides O(1) average insert, lookup, delete. Uses hash table internally.",
            "Counter: frequency counting in O(n). most_common(k) returns top-k elements.",
            "defaultdict: auto-initializes missing keys \u2014 ideal for grouping records by key.",
            "Two-pass hash join: build lookup dict in O(n), query in O(m). Total O(n+m) vs O(n*m) nested loop.",
            "set: O(1) membership. Supports union | , intersection &, difference -. Use for dedup + set math.",
            "dict.fromkeys(list): deduplicate while preserving insertion order (unlike set()).",
            "Sliding window: maintain running state instead of recomputing from scratch. O(n) vs O(n*k).",
            "heapq: min-heap. Push/pop in O(log n). Use for top-K streaming, merge K sorted sequences."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Given a list of 1M user_ids with duplicates, find the top-5 most frequent users using Counter. Then dedup the list preserving order using dict.fromkeys().",
            "<strong>Step 2:</strong> Implement a hash join: two lists of dicts (orders and customers) sharing a customer_id field. Merge them in O(n+m) using a dict lookup.",
            "<strong>Step 3:</strong> Sliding window: find the max sum of any 3 consecutive days from 365 daily revenue values. Implement in O(n) \u2014 no nested loop.",
            "<strong>Step 4:</strong> Top-K streaming: simulate a stream of (user_id, purchase_amount) tuples. Find the top-10 spenders using heapq without storing the entire stream in memory."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Solve LeetCode 49 (Group Anagrams) without Regex",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Solve LeetCode 49 (Group Anagrams) without Regex \u2014 trace through 2-3 edge cases before writing any code.",
                "Seeing 'x in my_list' inside a loop? That's O(n\u00b2). Convert to set first: O(n) total.",
                "Counter(iterable) counts in one line. Combine two counters: c1 + c2 adds counts, c1 & c2 keeps minimums.",
                "heapq is a MIN-heap. For max-heap: store values as negatives (-val, key) then negate on extraction.",
                "Sliding window: two-pointer left/right. Move right always; move left only when constraint violated."
            ]
        },
        "HardProblem": "Boss Problem (Google): You receive a stream of (user_id, search_query, timestamp) events, 100M events/day. (1) Find the top-100 most searched queries in the last 24 hours \u2014 you cannot load all 100M events into RAM. Use a heap + streaming approach. (2) Find all users who searched for the same query more than 5 times \u2014 are they bots? Use Counter + filter. (3) Find all pairs of users who searched for the SAME rare query (appears < 10 times globally). How do you efficiently match pairs? (4) Time complexity analysis: what is the big-O of each part?"
    },
    {
        "Week": 4,
        "Day": "Saturday",
        "Phase": "1. Filter (Coding)",
        "Theme": "Python Logic",
        "SpecificTopic": "Mock Assessment",
        "ActionItem_Deliverable": "3 Python Mediums (45 mins timer)",
        "LeetCodeProblem": "<strong>LC 560 \u2013 Subarray Sum Equals K</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Mock Design Round</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 The Problem Window Functions Solve</div>\n<div class=\"rich\">\n<h4>Start Here \u2014 Before Any Code</h4>\n<p>You're a data analyst at Spotify. Your manager asks: <em>\"Show me each employee's salary AND the average salary for their department \u2014 on the same row.\"</em></p>\n<p>Your first instinct: <code>GROUP BY department</code>. Problem: <strong>GROUP BY collapses rows</strong>. You get one row per department, losing individual employee detail. You can't have both on the same row \u2014 unless you use a slow self-join. This is exactly the gap <strong>Window Functions</strong> fill: they compute aggregates across related rows while <strong>keeping every row in the output</strong>.</p>\n<h4>The Mental Model: A Sliding Frame of Glass</h4>\n<p>Imagine placing a sheet of glass over a spreadsheet. For each row, the database looks through that glass at a \"window\" of rows, performs a calculation, and writes the result back into a new column \u2014 without removing the original row.</p>\n<pre>GROUP BY result:              Window Function result:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 dept        \u2502 avg_sal \u2502    \u2502 name  \u2502 dept        \u2502 salary \u2502 dept_avg \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Engineering \u2502  90,000 \u2502    \u2502 Alice \u2502 Engineering \u2502 95,000 \u2502  90,000  \u2502\n\u2502 Marketing   \u2502  72,500 \u2502    \u2502 Bob   \u2502 Engineering \u2502 85,000 \u2502  90,000  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 Eve   \u2502 Engineering \u2502 90,000 \u2502  90,000  \u2502\n2 rows \u2014 detail lost!         \u2502 Carol \u2502 Marketing   \u2502 70,000 \u2502  72,500  \u2502\n                              \u2502 Dave  \u2502 Marketing   \u2502 75,000 \u2502  72,500  \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              5 rows \u2014 ALL detail preserved \u2705</pre>\n<p>\u270d\ufe0f <strong>Write this down:</strong> Window functions NEVER reduce row count. They only ADD new computed columns. The <code>OVER()</code> keyword is the signal that tells the database \"this is a window function.\" Without OVER(), AVG() collapses rows. With OVER(), it keeps all rows.</p>\n<h4>The Syntax Structure</h4>\n<pre>FUNCTION_NAME() OVER (\n    PARTITION BY column   -- which rows form the window for each row\n    ORDER BY column       -- sort order within the window\n    ROWS BETWEEN ...      -- optional: how many surrounding rows to include\n)</pre>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 ROW_NUMBER \u2014 Step by Step with Full Explanation</div>\n<div class=\"rich\">\n<h4>What ROW_NUMBER() Does</h4>\n<p><code>ROW_NUMBER()</code> assigns a unique sequential integer to each row within a window, sorted as you specify. It ALWAYS gives unique numbers \u2014 even if two rows are completely identical. This makes it perfect for deduplication: you can always pick row #1 per group and discard the rest.</p>\n<h4>Step 1 \u2014 Create and populate the table</h4>\n<pre>CREATE TABLE employees (\n  emp_id   INT,\n  emp_name VARCHAR(50),\n  dept     VARCHAR(50),\n  salary   INT\n);\nINSERT INTO employees VALUES\n  (1, 'Alice', 'Engineering', 95000),\n  (2, 'Bob',   'Engineering', 85000),\n  (3, 'Carol', 'Marketing',   70000),\n  (4, 'Dave',  'Marketing',   75000),\n  (5, 'Eve',   'Engineering', 90000);</pre>\n<h4>Step 2 \u2014 Run your first window function</h4>\n<pre>SELECT\n  emp_name, dept, salary,\n  ROW_NUMBER() OVER (\n    PARTITION BY dept      -- restart numbering for each department\n    ORDER BY salary DESC   -- highest salary = rank 1\n  ) AS rank_in_dept\nFROM employees;</pre>\n<h4>Step 3 \u2014 Trace the output and understand WHY</h4>\n<pre>emp_name \u2502 dept        \u2502 salary \u2502 rank_in_dept\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nAlice    \u2502 Engineering \u2502  95000 \u2502      1    \u2190 highest in Eng \u2192 rank 1\nEve      \u2502 Engineering \u2502  90000 \u2502      2    \u2190 2nd in Eng\nBob      \u2502 Engineering \u2502  85000 \u2502      3    \u2190 3rd in Eng\nDave     \u2502 Marketing   \u2502  75000 \u2502      1    \u2190 RESTARTS! Highest in Marketing\nCarol    \u2502 Marketing   \u2502  70000 \u2502      2    \u2190 2nd in Marketing</pre>\n<p>Dave gets rank 1 even though he earns less than Bob \u2014 because PARTITION BY dept created a completely separate window for Marketing. Dave is simply the top earner within his own department's window.</p>\n<h4>What the Engine Does Internally</h4>\n<ol>\n  <li>Read all 5 rows from the table</li>\n  <li>Split into partitions: Engineering (3 rows), Marketing (2 rows)</li>\n  <li>Sort each partition by salary DESC</li>\n  <li>Assign row numbers 1,2,3... within each sorted partition</li>\n  <li>Attach numbers back to original rows and return all 5 rows</li>\n</ol>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 RANK vs DENSE_RANK \u2014 Why Ties Change Everything</div>\n<div class=\"rich\">\n<h4>The Problem with Ties in Real Data</h4>\n<p>In practice, duplicate values are common: two products with the same rating, two employees with the same salary, two cities with equal population. The three ranking functions handle ties differently, and choosing the wrong one produces wrong business results.</p>\n<pre>-- Add Frank \u2014 same salary as Eve\nINSERT INTO employees VALUES (6, 'Frank', 'Engineering', 90000);\n\nSELECT emp_name, salary,\n  ROW_NUMBER() OVER (ORDER BY salary DESC) AS row_num,\n  RANK()       OVER (ORDER BY salary DESC) AS rnk,\n  DENSE_RANK() OVER (ORDER BY salary DESC) AS dense_rnk\nFROM employees WHERE dept = 'Engineering';</pre>\n<pre>emp_name \u2502 salary \u2502 row_num \u2502 rnk \u2502 dense_rnk\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nAlice    \u2502  95000 \u2502    1    \u2502  1  \u2502     1\nEve      \u2502  90000 \u2502    2    \u2502  2  \u2502     2    \u2190 tied with Frank\nFrank    \u2502  90000 \u2502    3    \u2502  2  \u2502     2    \u2190 tied with Eve\nBob      \u2502  85000 \u2502    4    \u2502  4  \u2502     3    \u2190 RANK skips 3, DENSE_RANK doesn't</pre>\n<p>Bob gets RANK 4 because ranks 1, 2, 2 are taken \u2014 the number 3 is \"skipped\" to reflect that two people occupy the 2nd position. DENSE_RANK gives Bob a 3 with no gap.</p>\n<h4>Choosing the Right Function</h4>\n<table>\n<tr><th>Need exactly one result per group (dedup)?</th><td>ROW_NUMBER()</td></tr>\n<tr><th>Gap after tie is meaningful (sports: no 3rd if two share 2nd)?</th><td>RANK()</td></tr>\n<tr><th>Sequential tiers without gaps (medals, grade bands)?</th><td>DENSE_RANK()</td></tr>\n</table>\n<p>\u270d\ufe0f <strong>Interview trap (LeetCode 185):</strong> \"Top 3 salary values per department\" \u2014 ties must BOTH appear. Use DENSE_RANK not ROW_NUMBER. ROW_NUMBER eliminates tied rows arbitrarily \u2014 wrong answer.</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 FAANG Pattern: Top-N Per Group + Scale Secrets</div>\n<div class=\"rich\">\n<h4>The Most Common FAANG Window Function Pattern</h4>\n<p>Top-N per group appears in virtually every FAANG interview. The key constraint: you CANNOT filter on a window function in the same SELECT \u2014 window functions run in step 5 of SQL's execution order, but WHERE runs in step 2. The solution is always a CTE.</p>\n<pre>-- Top 3 earners per department\nWITH ranked AS (\n  SELECT emp_name, dept, salary,\n    DENSE_RANK() OVER (PARTITION BY dept ORDER BY salary DESC) AS rk\n    -- DENSE_RANK: if 2 people tie for 2nd, both appear in top 3\n  FROM employees\n)\nSELECT dept, emp_name, salary\nFROM ranked\nWHERE rk &lt;= 3          -- NOW we can filter because rk is a real column\nORDER BY dept, rk;</pre>\n<h4>SQL Execution Order \u2014 Memorize This</h4>\n<pre>1. FROM       \u2190 identify tables\n2. WHERE      \u2190 filter rows (window functions do NOT exist here yet!)\n3. GROUP BY   \u2190 aggregate\n4. HAVING     \u2190 filter aggregates\n5. SELECT     \u2190 compute expressions \u2014 window functions are evaluated HERE\n6. ORDER BY   \u2190 sort final result\n7. LIMIT      \u2190 restrict rows\n\nThis is why you need a CTE: the outer WHERE sees the CTE's rk column\nbecause by then, the CTE's step 5 has already been executed.</pre>\n<h4>Scale: Why PARTITION BY Key Choice Matters</h4>\n<p>On billions of rows: good PARTITION BY keys (user_id, device_id) spread data across thousands of nodes \u2014 parallel and fast. Bad keys (is_active: TRUE/FALSE) dump 99% of data on one node \u2014 memory crash. Always ask: \"does my partition key distribute data evenly?\"</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Window functions compute over related rows WITHOUT collapsing output \u2014 unlike GROUP BY.",
            "OVER() transforms any aggregate into a window function. Without OVER: GROUP BY. With OVER: window.",
            "PARTITION BY = independent windows per group. Rankings restart per partition.",
            "ORDER BY inside OVER = sort order within partition. Determines which row gets rank 1.",
            "ROW_NUMBER(): always unique. Use for dedup, pagination, keeping exactly 1 row per group.",
            "RANK(): ties share same number, next is SKIPPED (1,2,2,4). Use for sports/competitions.",
            "DENSE_RANK(): ties share same number, no skip (1,2,2,3). Use for tiered rankings. Use for LeetCode 185.",
            "SQL execution order: FROM\u2192WHERE\u2192GROUP BY\u2192HAVING\u2192SELECT(windows here)\u2192ORDER BY\u2192LIMIT.",
            "Cannot use window function result in WHERE. Always wrap in CTE first."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create employees table and insert 5 rows. Run the ROW_NUMBER query. Explain in writing why Dave gets rank 1.",
            "<strong>Step 2:</strong> Add Frank (Engineering, 90000) and run all 3 ranking functions. Write the difference you see in Bob's row.",
            "<strong>Step 3:</strong> Write the DENSE_RANK top-3-per-department query from scratch without looking. Test it shows both Eve and Frank when tied.",
            "<strong>Step 4:</strong> Try adding WHERE rank_in_dept &lt;= 3 directly in the window query (no CTE). Note the error. Then fix it with a CTE."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> 3 Python Mediums (45 mins timer)",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> 3 Python Mediums (45 mins timer) \u2014 trace through 2-3 edge cases before writing any code.",
                "'Per group' or 'within each category' \u2192 your signal to use PARTITION BY.",
                "LeetCode 185 trap: 'top 3 salaries' not 'top 3 employees' \u2192 DENSE_RANK so ties both show.",
                "Can't use window function in WHERE? Wrap in CTE \u2014 this is always the fix.",
                "High-cardinality PARTITION BY (user_id) = good parallelism. Low-cardinality (boolean) = hotspot."
            ]
        },
        "HardProblem": "Boss Problem (Meta): 5-billion row table: user_events(user_id, event_type, revenue, event_ts). (1) Rank each user's events by revenue using DENSE_RANK. (2) Return only top 3 per user. (3) Add running total revenue per user. Explain: Which PARTITION BY key? What happens memory-wise with no PARTITION BY on 5B rows? How would you run this in Spark?"
    },
    {
        "Week": 4,
        "Day": "Sunday",
        "Phase": "1. Filter (Coding)",
        "Theme": "Rest",
        "SpecificTopic": "Rest & Review",
        "ActionItem_Deliverable": "Review Big-O Notation for Dicts vs Lists",
        "LeetCodeProblem": null,
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Gaps & Islands (Complex)</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Active Rest \u2014 How Memory Consolidates</div>\n<div class=\"rich\">\n<h4>The Neuroscience of Spaced Repetition</h4>\n<p>Rest days are not wasted days. During rest, your brain replays what it learned \u2014 a process called <strong>memory consolidation</strong>. Research consistently shows that <em>attempting to recall from memory</em> is 3\u00d7 more effective at building durable knowledge than re-reading notes. The act of struggling to remember \u2014 even failing \u2014 strengthens the neural pathway more than passive review.</p>\n<p>Think of it like muscle recovery: a weightlifter who trains 7 days straight without rest gets weaker, not stronger. The growth happens during recovery. Your brain is the same.</p>\n<h4>Today's Protocol (4 hours, structured)</h4>\n<ol>\n  <li><strong>Active Recall (60 min):</strong> Close everything. On a blank page, write every concept, formula, and pattern from this week from memory. Then check.</li>\n  <li><strong>Error Review (60 min):</strong> Find the hardest problem you struggled with. Attempt it again completely from scratch \u2014 no notes.</li>\n  <li><strong>Teach It (45 min):</strong> Open the AI chatbot and explain Gaps & Islands to it as if it's a junior engineer. If you stumble, that's your gap.</li>\n  <li><strong>Next Week Preview (15 min):</strong> Skim next Monday's topic at headline level only \u2014 priming improves retention when you study it deeply.</li>\n</ol>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Active Recall Quiz \u2014 No Notes Allowed</div>\n<div class=\"rich\">\n<h4>Answer These From Memory. Then Check.</h4>\n<pre>Window Functions:\n\u25a1 What is the difference between ROW_NUMBER, RANK, and DENSE_RANK?\n  (Write the output for a table with two tied rows)\n\u25a1 Write the syntax for a 7-day rolling average from memory\n\u25a1 What does PARTITION BY do differently from GROUP BY?\n\u25a1 Write the complete Gaps & Islands CTE from memory\n\u25a1 What is the difference between ROWS BETWEEN and RANGE BETWEEN?\n\u25a1 LAG(col, 1, 0) vs LAG(col, 1) \u2014 what is different?\n\u25a1 In what SQL execution order step do window functions run?\n\u25a1 Why can't you write WHERE rank &lt;= 3 in the same query as ROW_NUMBER?\n\nScore yourself:\n8/8 = \ud83c\udfc6 Mastery level \u2014 you're ready for interviews on this\n5-7/8 = \ud83d\udfe1 Good \u2014 review the 1-2 concepts you missed\n&lt; 5/8 = \ud83d\udd34 Go back to the specific day's content, re-read Level 1-2</pre>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Spaced Repetition Schedule</div>\n<div class=\"rich\">\n<h4>When to Review This Week's Material</h4>\n<table>\n<tr><th>Review this content</th><th>When</th><th>How</th></tr>\n<tr><td>This week</td><td>Today</td><td>Active recall quiz above</td></tr>\n<tr><td>This week</td><td>In 3 days</td><td>Re-solve the hard problem from Day 4</td></tr>\n<tr><td>This week</td><td>In 7 days</td><td>LeetCode: solve problems 185, 180, 196</td></tr>\n<tr><td>This week</td><td>In 30 days</td><td>Explain all 4 concepts to someone else</td></tr>\n</table>\n<h4>The Feynman Technique</h4>\n<p>After studying any concept, close your notes and explain it as if you're teaching a newcomer. Use only simple words. Avoid jargon. If you stumble, that's exactly where your understanding breaks down \u2014 return to just that part. This technique built Richard Feynman's legendary ability to explain complex physics simply.</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Interview Readiness Self-Assessment</div>\n<div class=\"rich\">\n<pre>Rate yourself 1\u20135 on each topic (honest self-assessment):\n  1 = \"I need to re-read from scratch\"\n  3 = \"I understand but need to look at notes\"\n  5 = \"I can code this under pressure from memory\"\n\nROW_NUMBER / RANK / DENSE_RANK:        ___/5\nRolling windows (ROWS BETWEEN):        ___/5\nLAG/LEAD (adjacent row comparison):    ___/5\nGaps & Islands (date streaks):         ___/5\nSession detection (timestamp version): ___/5\n\nAction:\n  Score &lt;3: revisit that day's Level 1 and Level 2 content tomorrow\n  Score 3:  do one LeetCode problem on that specific topic\n  Score 5:  you're interview-ready \u2705 on this topic</pre>\n<h4>FAANG Interview Reality Check</h4>\n<p>At meta, Amazon, and Google data engineering interviews, you'll be asked to write SQL live on a shared screen without autocomplete or documentation. The most common reason candidates fail: they know the concept but freeze on the syntax under pressure. The fix: write every pattern from memory at least 5 times this week. Muscle memory is more reliable than conscious recall under stress.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Memory consolidation happens during rest \u2014 the brain replays and stabilizes learned patterns during downtime.",
            "Active recall (testing yourself) builds 3x stronger memory than re-reading notes.",
            "Spaced repetition: review at day 1, day 3, day 7, day 30 for maximum retention.",
            "The Feynman Technique: if you can't explain it simply, you don't fully understand it yet.",
            "Error analysis: the problems you got WRONG are your highest-value review items \u2014 spend the most time there."
        ],
        "Tasks": [
            "<strong>Active Recall:</strong> Write the Gaps & Islands CTE from memory on paper. Check it against Day 4.",
            "<strong>Re-attempt:</strong> Pick the hardest problem from this week and solve it in 20 minutes from scratch.",
            "<strong>Teach it:</strong> Explain RANK vs DENSE_RANK to the AI chatbot as if it's a junior. Get it right without notes.",
            "<strong>Preview:</strong> Skim next week's Monday topic for 10 minutes \u2014 just headline concepts."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Review Big-O Notation for Dicts vs Lists",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Review Big-O Notation for Dicts vs Lists \u2014 trace through 2-3 edge cases before writing any code.",
                "Use the AI chatbot to quiz yourself \u2014 ask it 'Give me a SQL window function interview question.'",
                "Write SQL from memory in a plain text editor \u2014 no autocomplete. This builds true recall.",
                "The hardest concept this week: spend 30 minutes writing a full explanation in your own words."
            ]
        },
        "HardProblem": "Connect-the-dots: Write 2-3 paragraphs explaining how ROW_NUMBER, rolling averages, LAG, and Gaps & Islands could ALL be used in a single production pipeline for a ride-sharing company. What problem does each solve? How do they complement each other? What would break if you removed any one of them?"
    },
    {
        "Week": 5,
        "Day": "Monday",
        "Phase": "1. Filter (Coding)",
        "Theme": "Python Systems",
        "SpecificTopic": "Generators",
        "ActionItem_Deliverable": "Write read_massive_file(filename) using yield",
        "LeetCodeProblem": "<strong>LC 200 \u2013 Number of Islands</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Mock Design Round</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Generators \u2014 The Memory Problem</div><div class=\"rich\"><h4>The Memory Problem \u2014 Why Generators Exist</h4><p>Imagine you need to process a 100GB log file. The naive approach: <code>lines = open(\"log.txt\").readlines()</code>. This reads all 100GB into RAM at once. If you only have 8GB of RAM, your process crashes. Even if it doesn't crash, you've consumed 100GB of memory for a computation that might only need 100MB at any one time.</p><p>This is the fundamental problem generators solve: <strong>lazy evaluation</strong>. Instead of computing all values upfront and storing them in memory, a generator computes the NEXT value only when asked. It uses O(1) memory regardless of the total sequence size.</p><h4>Generator Functions \u2014 The yield Keyword</h4><pre># Regular function: returns a list (loads everything into memory)\ndef read_all_lines(filename):\n    return open(filename).readlines()  # 100GB in RAM if large file!\n\n# Generator function: yields one line at a time (O(1) memory)\ndef stream_lines(filename):\n    with open(filename) as f:\n        for line in f:          # f iterates line by line \u2014 OS handles this\n            yield line.strip()  # pause here, return line, resume on next()\n\n# Usage: process 100GB file with 8GB RAM \u2014 works perfectly\nfor line in stream_lines(\"huge_log.txt\"):\n    process(line)  # only one line lives in memory at any time\n\n# A generator is a function that remembers WHERE IT WAS in execution.\n# \"yield\" means: return this value, pause here, resume from here next time.</pre><h4>Generator Expressions \u2014 The Lazy List Comprehension</h4><pre># List comprehension: creates all values in memory immediately\nsquares_list = [x**2 for x in range(10_000_000)]   # 80MB in memory\n\n# Generator expression: creates values on demand, same syntax with ()\nsquares_gen  = (x**2 for x in range(10_000_000))    # tiny (generator object)\n\n# Both work identically in a for loop:\ntotal = sum(x**2 for x in range(10_000_000))  # never stores all 10M values!\n\n# Rule for data engineering: use generator expression whenever you will\n# iterate through results ONCE (sum, filter, write to file, stream out).\n# Use list when you need to: index into results, iterate multiple times, or check len().</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 ETL Pipeline with Generator Chaining</div><div class=\"rich\"><h4>Real-World DE Pattern: ETL Pipeline with Generators</h4><p>The most powerful use of generators in data engineering: chaining them into a pipeline. Each stage of the ETL is a generator that reads from the previous stage. The entire pipeline runs with O(1) memory \u2014 data flows through one record at a time.</p><pre># ETL Pipeline: read \u2192 parse \u2192 filter \u2192 transform \u2192 write\n# Each function is a generator. Data flows record-by-record.\n\ndef read_csv(filepath):\n    \"\"\"Stage 1: Read file line by line.\"\"\"\n    with open(filepath) as f:\n        next(f)  # skip header\n        for line in f:\n            yield line.strip()\n\ndef parse_rows(lines):\n    \"\"\"Stage 2: Parse CSV text into dicts.\"\"\"\n    for line in lines:  # lines is itself a generator!\n        parts = line.split(\",\")\n        yield {\"user_id\": parts[0], \"amount\": float(parts[1]), \"date\": parts[2]}\n\ndef filter_active(rows):\n    \"\"\"Stage 3: Keep only recent rows.\"\"\"\n    cutoff = \"2024-01-01\"\n    for row in rows:\n        if row[\"date\"] >= cutoff:\n            yield row\n\ndef enrich(rows, lookup):\n    \"\"\"Stage 4: Add metadata from lookup table.\"\"\"\n    for row in rows:\n        row[\"region\"] = lookup.get(row[\"user_id\"], \"unknown\")\n        yield row\n\n# Chain: data flows through all stages lazily, one record at a time\nlookup = build_region_lookup()  # a dict \u2014 O(m) memory\npipeline = enrich(\n    filter_active(\n        parse_rows(\n            read_csv(\"events.csv\")  # 10GB file\n        )\n    ),\n    lookup\n)\n\nfor record in pipeline:  # triggers execution \u2014 pulls one record at a time\n    write_to_warehouse(record)\n# Total memory: O(1) for the pipeline + O(m) for the lookup dict</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Decorators + Context Managers</div><div class=\"rich\"><h4>Decorators \u2014 Functions That Wrap Functions</h4><p>A <strong>decorator</strong> is a function that takes another function as input, wraps it with additional behavior, and returns the wrapped version. The <code>@</code> syntax is shorthand for <code>func = decorator(func)</code>.</p><p>In data engineering: decorators handle retry logic, caching, metrics, logging, and access control \u2014 without cluttering the core business logic.</p><pre>import time\nimport functools\n\n# Decorator: retry an operation up to N times on failure\ndef retry(max_attempts=3, delay=1.0):\n    def decorator(func):\n        @functools.wraps(func)  # preserve original function metadata\n        def wrapper(*args, **kwargs):\n            for attempt in range(1, max_attempts + 1):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    if attempt == max_attempts:\n                        raise  # re-raise on final attempt\n                    print(f\"Attempt {attempt} failed: {e}. Retrying...\")\n                    time.sleep(delay)\n        return wrapper\n    return decorator\n\n# Usage: apply retry behavior to any function with one line\n@retry(max_attempts=5, delay=2.0)\ndef fetch_from_api(endpoint):\n    \"\"\"Fetches data from a flaky external API.\"\"\"\n    response = requests.get(endpoint)\n    response.raise_for_status()\n    return response.json()\n\n# Equivalent to: fetch_from_api = retry(5, 2.0)(fetch_from_api)</pre><h4>Context Managers \u2014 Guaranteed Resource Cleanup</h4><pre># Context manager: database connection that always closes\nfrom contextlib import contextmanager\n\n@contextmanager\ndef db_connection(conn_string):\n    conn = create_connection(conn_string)\n    try:\n        yield conn          # caller gets the connection here\n    finally:\n        conn.close()        # ALWAYS runs, even if exception occurs\n\n# Usage: connection guaranteed to close even on exception\nwith db_connection(\"postgresql://...\") as conn:\n    conn.execute(\"INSERT ...\")</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Memory Profiling + 500GB Pipeline</div><div class=\"rich\"><h4>Memory Profiling and Optimization Strategies</h4><p>In production data engineering, understanding memory at a deep level prevents outages. The three main memory killers in Python DE pipelines:</p><ul><li><strong>Loading entire files/datasets into lists</strong>: Replace with generators + streaming</li><li><strong>Multiple copies of the same string</strong>: Python interns short strings but not long ones \u2014 use string interning or category dtypes in Pandas</li><li><strong>DataFrames with wrong dtypes</strong>: a DataFrame with int64 columns that hold values 0-100 uses 8x more memory than int8</li></ul><pre># Memory profiling with memory_profiler\n# pip install memory-profiler\nfrom memory_profiler import profile\n\n@profile\ndef load_and_process():\n    data = pd.read_csv(\"large.csv\")     # baseline memory\n    # LINE 1: 2.0 GiB\n    data = optimize_dtypes(data)         # shrink dtypes\n    # LINE 2: 0.5 GiB \u2014 75% reduction!\n    return data\n\n# Dtype optimization:\ndef optimize_dtypes(df):\n    for col in df.select_dtypes(\"int64\").columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n    for col in df.select_dtypes(\"float64\").columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n    for col in df.select_dtypes(\"object\").columns:\n        if df[col].nunique() / len(df) < 0.5:  # <50% unique \u2192 category\n            df[col] = df[col].astype(\"category\")\n    return df\n\n# Processing chunks: alternative to generators for files too large for memory\nfor chunk in pd.read_csv(\"huge.csv\", chunksize=100_000):\n    process(chunk)  # process 100K rows at a time</pre><p><strong>FAANG interview question:</strong> \"Your pipeline processes 500GB of logs daily. Memory on your EC2 instance is 32GB. Walk me through how you would process this.\" Correct answer: generators + chunked reading + streaming writes, never load the full dataset simultaneously.</p></div></div></div>",
        "KeyConcepts": [
            "Generator function: uses 'yield' to pause/resume. Returns a generator object, not a list.",
            "Generator uses O(1) memory: it never holds all values at once \u2014 computes next value on demand.",
            "Generator expression: (expr for x in iterable) \u2014 identical syntax to list comp but lazy.",
            "Pipeline chaining: each ETL stage is a generator reading from the previous. Data flows one record at a time.",
            "Decorator: higher-order function that wraps another. '@retry' adds retry logic without changing core code.",
            "@functools.wraps preserves function name/docstring when writing decorators.",
            "Context manager: 'with' block guarantees cleanup (finally) even on exceptions.",
            "Memory optimization: dtype downcast (int64\u2192int8), category dtype for low-cardinality strings."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write a generator that reads a CSV file line by line and yields parsed dict rows. Compare memory usage vs reading the full file into a list.",
            "<strong>Step 2:</strong> Build a 3-stage generator pipeline: read \u2192 filter (only rows where amount > 100) \u2192 transform (add a 'revenue_tier' field). Process a 1M-row file.",
            "<strong>Step 3:</strong> Write a @retry(max_attempts=3, delay=1) decorator. Apply it to a function that simulates random network failures. Verify it retries correctly.",
            "<strong>Step 4:</strong> Write optimize_dtypes(df). Apply it to a DataFrame with int64 and object columns. Report memory usage before and after with df.memory_usage(deep=True).sum()."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Write read_massive_file(filename) using yield",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Write read_massive_file(filename) using yield \u2014 trace through 2-3 edge cases before writing any code.",
                "Can you iterate through results with a single for loop? \u2192 use generator (saves memory).",
                "Need to index into results, get len(), or iterate multiple times? \u2192 convert to list first.",
                "Chunked pandas: pd.read_csv('file.csv', chunksize=100_000) processes file in chunks without full load.",
                "Generators are lazy \u2014 they do NOTHING until iterated. Chaining generators costs zero compute up front."
            ]
        },
        "HardProblem": "Boss Problem (Netflix): Your nightly ETL reads from an S3 bucket containing 50,000 gzipped JSON files (total ~500GB uncompressed). Each file has user viewing events. (1) Write a generator pipeline: stream S3 object list \u2192 download each file \u2192 gunzip \u2192 parse JSON lines \u2192 filter by date \u2192 yield events. The pipeline should use <2GB RAM regardless of total file count. (2) For each event, do a dict lookup against a 20M-row user metadata table loaded in RAM. What is the maximum size this lookup dict can be? (3) If processing takes 14 hours, how do you parallelize using multiprocessing.Pool while keeping memory bounded?"
    },
    {
        "Week": 5,
        "Day": "Tuesday",
        "Phase": "1. Filter (Coding)",
        "Theme": "Python Systems",
        "SpecificTopic": "Recursive Parsing",
        "ActionItem_Deliverable": "Write script to flatten 5-level deep JSON object",
        "LeetCodeProblem": "<strong>LC 236 \u2013 LCA of Binary Tree</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Lead/Lag Pattern</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Generators \u2014 The Memory Problem</div><div class=\"rich\"><h4>The Memory Problem \u2014 Why Generators Exist</h4><p>Imagine you need to process a 100GB log file. The naive approach: <code>lines = open(\"log.txt\").readlines()</code>. This reads all 100GB into RAM at once. If you only have 8GB of RAM, your process crashes. Even if it doesn't crash, you've consumed 100GB of memory for a computation that might only need 100MB at any one time.</p><p>This is the fundamental problem generators solve: <strong>lazy evaluation</strong>. Instead of computing all values upfront and storing them in memory, a generator computes the NEXT value only when asked. It uses O(1) memory regardless of the total sequence size.</p><h4>Generator Functions \u2014 The yield Keyword</h4><pre># Regular function: returns a list (loads everything into memory)\ndef read_all_lines(filename):\n    return open(filename).readlines()  # 100GB in RAM if large file!\n\n# Generator function: yields one line at a time (O(1) memory)\ndef stream_lines(filename):\n    with open(filename) as f:\n        for line in f:          # f iterates line by line \u2014 OS handles this\n            yield line.strip()  # pause here, return line, resume on next()\n\n# Usage: process 100GB file with 8GB RAM \u2014 works perfectly\nfor line in stream_lines(\"huge_log.txt\"):\n    process(line)  # only one line lives in memory at any time\n\n# A generator is a function that remembers WHERE IT WAS in execution.\n# \"yield\" means: return this value, pause here, resume from here next time.</pre><h4>Generator Expressions \u2014 The Lazy List Comprehension</h4><pre># List comprehension: creates all values in memory immediately\nsquares_list = [x**2 for x in range(10_000_000)]   # 80MB in memory\n\n# Generator expression: creates values on demand, same syntax with ()\nsquares_gen  = (x**2 for x in range(10_000_000))    # tiny (generator object)\n\n# Both work identically in a for loop:\ntotal = sum(x**2 for x in range(10_000_000))  # never stores all 10M values!\n\n# Rule for data engineering: use generator expression whenever you will\n# iterate through results ONCE (sum, filter, write to file, stream out).\n# Use list when you need to: index into results, iterate multiple times, or check len().</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 ETL Pipeline with Generator Chaining</div><div class=\"rich\"><h4>Real-World DE Pattern: ETL Pipeline with Generators</h4><p>The most powerful use of generators in data engineering: chaining them into a pipeline. Each stage of the ETL is a generator that reads from the previous stage. The entire pipeline runs with O(1) memory \u2014 data flows through one record at a time.</p><pre># ETL Pipeline: read \u2192 parse \u2192 filter \u2192 transform \u2192 write\n# Each function is a generator. Data flows record-by-record.\n\ndef read_csv(filepath):\n    \"\"\"Stage 1: Read file line by line.\"\"\"\n    with open(filepath) as f:\n        next(f)  # skip header\n        for line in f:\n            yield line.strip()\n\ndef parse_rows(lines):\n    \"\"\"Stage 2: Parse CSV text into dicts.\"\"\"\n    for line in lines:  # lines is itself a generator!\n        parts = line.split(\",\")\n        yield {\"user_id\": parts[0], \"amount\": float(parts[1]), \"date\": parts[2]}\n\ndef filter_active(rows):\n    \"\"\"Stage 3: Keep only recent rows.\"\"\"\n    cutoff = \"2024-01-01\"\n    for row in rows:\n        if row[\"date\"] >= cutoff:\n            yield row\n\ndef enrich(rows, lookup):\n    \"\"\"Stage 4: Add metadata from lookup table.\"\"\"\n    for row in rows:\n        row[\"region\"] = lookup.get(row[\"user_id\"], \"unknown\")\n        yield row\n\n# Chain: data flows through all stages lazily, one record at a time\nlookup = build_region_lookup()  # a dict \u2014 O(m) memory\npipeline = enrich(\n    filter_active(\n        parse_rows(\n            read_csv(\"events.csv\")  # 10GB file\n        )\n    ),\n    lookup\n)\n\nfor record in pipeline:  # triggers execution \u2014 pulls one record at a time\n    write_to_warehouse(record)\n# Total memory: O(1) for the pipeline + O(m) for the lookup dict</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Decorators + Context Managers</div><div class=\"rich\"><h4>Decorators \u2014 Functions That Wrap Functions</h4><p>A <strong>decorator</strong> is a function that takes another function as input, wraps it with additional behavior, and returns the wrapped version. The <code>@</code> syntax is shorthand for <code>func = decorator(func)</code>.</p><p>In data engineering: decorators handle retry logic, caching, metrics, logging, and access control \u2014 without cluttering the core business logic.</p><pre>import time\nimport functools\n\n# Decorator: retry an operation up to N times on failure\ndef retry(max_attempts=3, delay=1.0):\n    def decorator(func):\n        @functools.wraps(func)  # preserve original function metadata\n        def wrapper(*args, **kwargs):\n            for attempt in range(1, max_attempts + 1):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    if attempt == max_attempts:\n                        raise  # re-raise on final attempt\n                    print(f\"Attempt {attempt} failed: {e}. Retrying...\")\n                    time.sleep(delay)\n        return wrapper\n    return decorator\n\n# Usage: apply retry behavior to any function with one line\n@retry(max_attempts=5, delay=2.0)\ndef fetch_from_api(endpoint):\n    \"\"\"Fetches data from a flaky external API.\"\"\"\n    response = requests.get(endpoint)\n    response.raise_for_status()\n    return response.json()\n\n# Equivalent to: fetch_from_api = retry(5, 2.0)(fetch_from_api)</pre><h4>Context Managers \u2014 Guaranteed Resource Cleanup</h4><pre># Context manager: database connection that always closes\nfrom contextlib import contextmanager\n\n@contextmanager\ndef db_connection(conn_string):\n    conn = create_connection(conn_string)\n    try:\n        yield conn          # caller gets the connection here\n    finally:\n        conn.close()        # ALWAYS runs, even if exception occurs\n\n# Usage: connection guaranteed to close even on exception\nwith db_connection(\"postgresql://...\") as conn:\n    conn.execute(\"INSERT ...\")</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Memory Profiling + 500GB Pipeline</div><div class=\"rich\"><h4>Memory Profiling and Optimization Strategies</h4><p>In production data engineering, understanding memory at a deep level prevents outages. The three main memory killers in Python DE pipelines:</p><ul><li><strong>Loading entire files/datasets into lists</strong>: Replace with generators + streaming</li><li><strong>Multiple copies of the same string</strong>: Python interns short strings but not long ones \u2014 use string interning or category dtypes in Pandas</li><li><strong>DataFrames with wrong dtypes</strong>: a DataFrame with int64 columns that hold values 0-100 uses 8x more memory than int8</li></ul><pre># Memory profiling with memory_profiler\n# pip install memory-profiler\nfrom memory_profiler import profile\n\n@profile\ndef load_and_process():\n    data = pd.read_csv(\"large.csv\")     # baseline memory\n    # LINE 1: 2.0 GiB\n    data = optimize_dtypes(data)         # shrink dtypes\n    # LINE 2: 0.5 GiB \u2014 75% reduction!\n    return data\n\n# Dtype optimization:\ndef optimize_dtypes(df):\n    for col in df.select_dtypes(\"int64\").columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n    for col in df.select_dtypes(\"float64\").columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n    for col in df.select_dtypes(\"object\").columns:\n        if df[col].nunique() / len(df) < 0.5:  # <50% unique \u2192 category\n            df[col] = df[col].astype(\"category\")\n    return df\n\n# Processing chunks: alternative to generators for files too large for memory\nfor chunk in pd.read_csv(\"huge.csv\", chunksize=100_000):\n    process(chunk)  # process 100K rows at a time</pre><p><strong>FAANG interview question:</strong> \"Your pipeline processes 500GB of logs daily. Memory on your EC2 instance is 32GB. Walk me through how you would process this.\" Correct answer: generators + chunked reading + streaming writes, never load the full dataset simultaneously.</p></div></div></div>",
        "KeyConcepts": [
            "Generator function: uses 'yield' to pause/resume. Returns a generator object, not a list.",
            "Generator uses O(1) memory: it never holds all values at once \u2014 computes next value on demand.",
            "Generator expression: (expr for x in iterable) \u2014 identical syntax to list comp but lazy.",
            "Pipeline chaining: each ETL stage is a generator reading from the previous. Data flows one record at a time.",
            "Decorator: higher-order function that wraps another. '@retry' adds retry logic without changing core code.",
            "@functools.wraps preserves function name/docstring when writing decorators.",
            "Context manager: 'with' block guarantees cleanup (finally) even on exceptions.",
            "Memory optimization: dtype downcast (int64\u2192int8), category dtype for low-cardinality strings."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write a generator that reads a CSV file line by line and yields parsed dict rows. Compare memory usage vs reading the full file into a list.",
            "<strong>Step 2:</strong> Build a 3-stage generator pipeline: read \u2192 filter (only rows where amount > 100) \u2192 transform (add a 'revenue_tier' field). Process a 1M-row file.",
            "<strong>Step 3:</strong> Write a @retry(max_attempts=3, delay=1) decorator. Apply it to a function that simulates random network failures. Verify it retries correctly.",
            "<strong>Step 4:</strong> Write optimize_dtypes(df). Apply it to a DataFrame with int64 and object columns. Report memory usage before and after with df.memory_usage(deep=True).sum()."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Write script to flatten 5-level deep JSON object",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Write script to flatten 5-level deep JSON object \u2014 trace through 2-3 edge cases before writing any code.",
                "Can you iterate through results with a single for loop? \u2192 use generator (saves memory).",
                "Need to index into results, get len(), or iterate multiple times? \u2192 convert to list first.",
                "Chunked pandas: pd.read_csv('file.csv', chunksize=100_000) processes file in chunks without full load.",
                "Generators are lazy \u2014 they do NOTHING until iterated. Chaining generators costs zero compute up front."
            ]
        },
        "HardProblem": "Boss Problem (Netflix): Your nightly ETL reads from an S3 bucket containing 50,000 gzipped JSON files (total ~500GB uncompressed). Each file has user viewing events. (1) Write a generator pipeline: stream S3 object list \u2192 download each file \u2192 gunzip \u2192 parse JSON lines \u2192 filter by date \u2192 yield events. The pipeline should use <2GB RAM regardless of total file count. (2) For each event, do a dict lookup against a 20M-row user metadata table loaded in RAM. What is the maximum size this lookup dict can be? (3) If processing takes 14 hours, how do you parallelize using multiprocessing.Pool while keeping memory bounded?"
    },
    {
        "Week": 5,
        "Day": "Wednesday",
        "Phase": "1. Filter (Coding)",
        "Theme": "Python Systems",
        "SpecificTopic": "File I/O",
        "ActionItem_Deliverable": "Convert 100k row CSV to JSON Lines without Pandas",
        "LeetCodeProblem": "<strong>LC 206 \u2013 Reverse Linked List</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Mock Design Round</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Generators \u2014 The Memory Problem</div><div class=\"rich\"><h4>The Memory Problem \u2014 Why Generators Exist</h4><p>Imagine you need to process a 100GB log file. The naive approach: <code>lines = open(\"log.txt\").readlines()</code>. This reads all 100GB into RAM at once. If you only have 8GB of RAM, your process crashes. Even if it doesn't crash, you've consumed 100GB of memory for a computation that might only need 100MB at any one time.</p><p>This is the fundamental problem generators solve: <strong>lazy evaluation</strong>. Instead of computing all values upfront and storing them in memory, a generator computes the NEXT value only when asked. It uses O(1) memory regardless of the total sequence size.</p><h4>Generator Functions \u2014 The yield Keyword</h4><pre># Regular function: returns a list (loads everything into memory)\ndef read_all_lines(filename):\n    return open(filename).readlines()  # 100GB in RAM if large file!\n\n# Generator function: yields one line at a time (O(1) memory)\ndef stream_lines(filename):\n    with open(filename) as f:\n        for line in f:          # f iterates line by line \u2014 OS handles this\n            yield line.strip()  # pause here, return line, resume on next()\n\n# Usage: process 100GB file with 8GB RAM \u2014 works perfectly\nfor line in stream_lines(\"huge_log.txt\"):\n    process(line)  # only one line lives in memory at any time\n\n# A generator is a function that remembers WHERE IT WAS in execution.\n# \"yield\" means: return this value, pause here, resume from here next time.</pre><h4>Generator Expressions \u2014 The Lazy List Comprehension</h4><pre># List comprehension: creates all values in memory immediately\nsquares_list = [x**2 for x in range(10_000_000)]   # 80MB in memory\n\n# Generator expression: creates values on demand, same syntax with ()\nsquares_gen  = (x**2 for x in range(10_000_000))    # tiny (generator object)\n\n# Both work identically in a for loop:\ntotal = sum(x**2 for x in range(10_000_000))  # never stores all 10M values!\n\n# Rule for data engineering: use generator expression whenever you will\n# iterate through results ONCE (sum, filter, write to file, stream out).\n# Use list when you need to: index into results, iterate multiple times, or check len().</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 ETL Pipeline with Generator Chaining</div><div class=\"rich\"><h4>Real-World DE Pattern: ETL Pipeline with Generators</h4><p>The most powerful use of generators in data engineering: chaining them into a pipeline. Each stage of the ETL is a generator that reads from the previous stage. The entire pipeline runs with O(1) memory \u2014 data flows through one record at a time.</p><pre># ETL Pipeline: read \u2192 parse \u2192 filter \u2192 transform \u2192 write\n# Each function is a generator. Data flows record-by-record.\n\ndef read_csv(filepath):\n    \"\"\"Stage 1: Read file line by line.\"\"\"\n    with open(filepath) as f:\n        next(f)  # skip header\n        for line in f:\n            yield line.strip()\n\ndef parse_rows(lines):\n    \"\"\"Stage 2: Parse CSV text into dicts.\"\"\"\n    for line in lines:  # lines is itself a generator!\n        parts = line.split(\",\")\n        yield {\"user_id\": parts[0], \"amount\": float(parts[1]), \"date\": parts[2]}\n\ndef filter_active(rows):\n    \"\"\"Stage 3: Keep only recent rows.\"\"\"\n    cutoff = \"2024-01-01\"\n    for row in rows:\n        if row[\"date\"] >= cutoff:\n            yield row\n\ndef enrich(rows, lookup):\n    \"\"\"Stage 4: Add metadata from lookup table.\"\"\"\n    for row in rows:\n        row[\"region\"] = lookup.get(row[\"user_id\"], \"unknown\")\n        yield row\n\n# Chain: data flows through all stages lazily, one record at a time\nlookup = build_region_lookup()  # a dict \u2014 O(m) memory\npipeline = enrich(\n    filter_active(\n        parse_rows(\n            read_csv(\"events.csv\")  # 10GB file\n        )\n    ),\n    lookup\n)\n\nfor record in pipeline:  # triggers execution \u2014 pulls one record at a time\n    write_to_warehouse(record)\n# Total memory: O(1) for the pipeline + O(m) for the lookup dict</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Decorators + Context Managers</div><div class=\"rich\"><h4>Decorators \u2014 Functions That Wrap Functions</h4><p>A <strong>decorator</strong> is a function that takes another function as input, wraps it with additional behavior, and returns the wrapped version. The <code>@</code> syntax is shorthand for <code>func = decorator(func)</code>.</p><p>In data engineering: decorators handle retry logic, caching, metrics, logging, and access control \u2014 without cluttering the core business logic.</p><pre>import time\nimport functools\n\n# Decorator: retry an operation up to N times on failure\ndef retry(max_attempts=3, delay=1.0):\n    def decorator(func):\n        @functools.wraps(func)  # preserve original function metadata\n        def wrapper(*args, **kwargs):\n            for attempt in range(1, max_attempts + 1):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    if attempt == max_attempts:\n                        raise  # re-raise on final attempt\n                    print(f\"Attempt {attempt} failed: {e}. Retrying...\")\n                    time.sleep(delay)\n        return wrapper\n    return decorator\n\n# Usage: apply retry behavior to any function with one line\n@retry(max_attempts=5, delay=2.0)\ndef fetch_from_api(endpoint):\n    \"\"\"Fetches data from a flaky external API.\"\"\"\n    response = requests.get(endpoint)\n    response.raise_for_status()\n    return response.json()\n\n# Equivalent to: fetch_from_api = retry(5, 2.0)(fetch_from_api)</pre><h4>Context Managers \u2014 Guaranteed Resource Cleanup</h4><pre># Context manager: database connection that always closes\nfrom contextlib import contextmanager\n\n@contextmanager\ndef db_connection(conn_string):\n    conn = create_connection(conn_string)\n    try:\n        yield conn          # caller gets the connection here\n    finally:\n        conn.close()        # ALWAYS runs, even if exception occurs\n\n# Usage: connection guaranteed to close even on exception\nwith db_connection(\"postgresql://...\") as conn:\n    conn.execute(\"INSERT ...\")</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Memory Profiling + 500GB Pipeline</div><div class=\"rich\"><h4>Memory Profiling and Optimization Strategies</h4><p>In production data engineering, understanding memory at a deep level prevents outages. The three main memory killers in Python DE pipelines:</p><ul><li><strong>Loading entire files/datasets into lists</strong>: Replace with generators + streaming</li><li><strong>Multiple copies of the same string</strong>: Python interns short strings but not long ones \u2014 use string interning or category dtypes in Pandas</li><li><strong>DataFrames with wrong dtypes</strong>: a DataFrame with int64 columns that hold values 0-100 uses 8x more memory than int8</li></ul><pre># Memory profiling with memory_profiler\n# pip install memory-profiler\nfrom memory_profiler import profile\n\n@profile\ndef load_and_process():\n    data = pd.read_csv(\"large.csv\")     # baseline memory\n    # LINE 1: 2.0 GiB\n    data = optimize_dtypes(data)         # shrink dtypes\n    # LINE 2: 0.5 GiB \u2014 75% reduction!\n    return data\n\n# Dtype optimization:\ndef optimize_dtypes(df):\n    for col in df.select_dtypes(\"int64\").columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n    for col in df.select_dtypes(\"float64\").columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n    for col in df.select_dtypes(\"object\").columns:\n        if df[col].nunique() / len(df) < 0.5:  # <50% unique \u2192 category\n            df[col] = df[col].astype(\"category\")\n    return df\n\n# Processing chunks: alternative to generators for files too large for memory\nfor chunk in pd.read_csv(\"huge.csv\", chunksize=100_000):\n    process(chunk)  # process 100K rows at a time</pre><p><strong>FAANG interview question:</strong> \"Your pipeline processes 500GB of logs daily. Memory on your EC2 instance is 32GB. Walk me through how you would process this.\" Correct answer: generators + chunked reading + streaming writes, never load the full dataset simultaneously.</p></div></div></div>",
        "KeyConcepts": [
            "Generator function: uses 'yield' to pause/resume. Returns a generator object, not a list.",
            "Generator uses O(1) memory: it never holds all values at once \u2014 computes next value on demand.",
            "Generator expression: (expr for x in iterable) \u2014 identical syntax to list comp but lazy.",
            "Pipeline chaining: each ETL stage is a generator reading from the previous. Data flows one record at a time.",
            "Decorator: higher-order function that wraps another. '@retry' adds retry logic without changing core code.",
            "@functools.wraps preserves function name/docstring when writing decorators.",
            "Context manager: 'with' block guarantees cleanup (finally) even on exceptions.",
            "Memory optimization: dtype downcast (int64\u2192int8), category dtype for low-cardinality strings."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write a generator that reads a CSV file line by line and yields parsed dict rows. Compare memory usage vs reading the full file into a list.",
            "<strong>Step 2:</strong> Build a 3-stage generator pipeline: read \u2192 filter (only rows where amount > 100) \u2192 transform (add a 'revenue_tier' field). Process a 1M-row file.",
            "<strong>Step 3:</strong> Write a @retry(max_attempts=3, delay=1) decorator. Apply it to a function that simulates random network failures. Verify it retries correctly.",
            "<strong>Step 4:</strong> Write optimize_dtypes(df). Apply it to a DataFrame with int64 and object columns. Report memory usage before and after with df.memory_usage(deep=True).sum()."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Convert 100k row CSV to JSON Lines without Pandas",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Convert 100k row CSV to JSON Lines without Pandas \u2014 trace through 2-3 edge cases before writing any code.",
                "Can you iterate through results with a single for loop? \u2192 use generator (saves memory).",
                "Need to index into results, get len(), or iterate multiple times? \u2192 convert to list first.",
                "Chunked pandas: pd.read_csv('file.csv', chunksize=100_000) processes file in chunks without full load.",
                "Generators are lazy \u2014 they do NOTHING until iterated. Chaining generators costs zero compute up front."
            ]
        },
        "HardProblem": "Boss Problem (Netflix): Your nightly ETL reads from an S3 bucket containing 50,000 gzipped JSON files (total ~500GB uncompressed). Each file has user viewing events. (1) Write a generator pipeline: stream S3 object list \u2192 download each file \u2192 gunzip \u2192 parse JSON lines \u2192 filter by date \u2192 yield events. The pipeline should use <2GB RAM regardless of total file count. (2) For each event, do a dict lookup against a 20M-row user metadata table loaded in RAM. What is the maximum size this lookup dict can be? (3) If processing takes 14 hours, how do you parallelize using multiprocessing.Pool while keeping memory bounded?"
    },
    {
        "Week": 5,
        "Day": "Thursday",
        "Phase": "1. Filter (Coding)",
        "Theme": "Python Systems",
        "SpecificTopic": "Memory Management",
        "ActionItem_Deliverable": "Use sys.getsizeof() to compare List vs Generator",
        "LeetCodeProblem": "<strong>LC 141 \u2013 Linked List Cycle</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Dimensional Modeling Basics</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Generators \u2014 The Memory Problem</div><div class=\"rich\"><h4>The Memory Problem \u2014 Why Generators Exist</h4><p>Imagine you need to process a 100GB log file. The naive approach: <code>lines = open(\"log.txt\").readlines()</code>. This reads all 100GB into RAM at once. If you only have 8GB of RAM, your process crashes. Even if it doesn't crash, you've consumed 100GB of memory for a computation that might only need 100MB at any one time.</p><p>This is the fundamental problem generators solve: <strong>lazy evaluation</strong>. Instead of computing all values upfront and storing them in memory, a generator computes the NEXT value only when asked. It uses O(1) memory regardless of the total sequence size.</p><h4>Generator Functions \u2014 The yield Keyword</h4><pre># Regular function: returns a list (loads everything into memory)\ndef read_all_lines(filename):\n    return open(filename).readlines()  # 100GB in RAM if large file!\n\n# Generator function: yields one line at a time (O(1) memory)\ndef stream_lines(filename):\n    with open(filename) as f:\n        for line in f:          # f iterates line by line \u2014 OS handles this\n            yield line.strip()  # pause here, return line, resume on next()\n\n# Usage: process 100GB file with 8GB RAM \u2014 works perfectly\nfor line in stream_lines(\"huge_log.txt\"):\n    process(line)  # only one line lives in memory at any time\n\n# A generator is a function that remembers WHERE IT WAS in execution.\n# \"yield\" means: return this value, pause here, resume from here next time.</pre><h4>Generator Expressions \u2014 The Lazy List Comprehension</h4><pre># List comprehension: creates all values in memory immediately\nsquares_list = [x**2 for x in range(10_000_000)]   # 80MB in memory\n\n# Generator expression: creates values on demand, same syntax with ()\nsquares_gen  = (x**2 for x in range(10_000_000))    # tiny (generator object)\n\n# Both work identically in a for loop:\ntotal = sum(x**2 for x in range(10_000_000))  # never stores all 10M values!\n\n# Rule for data engineering: use generator expression whenever you will\n# iterate through results ONCE (sum, filter, write to file, stream out).\n# Use list when you need to: index into results, iterate multiple times, or check len().</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 ETL Pipeline with Generator Chaining</div><div class=\"rich\"><h4>Real-World DE Pattern: ETL Pipeline with Generators</h4><p>The most powerful use of generators in data engineering: chaining them into a pipeline. Each stage of the ETL is a generator that reads from the previous stage. The entire pipeline runs with O(1) memory \u2014 data flows through one record at a time.</p><pre># ETL Pipeline: read \u2192 parse \u2192 filter \u2192 transform \u2192 write\n# Each function is a generator. Data flows record-by-record.\n\ndef read_csv(filepath):\n    \"\"\"Stage 1: Read file line by line.\"\"\"\n    with open(filepath) as f:\n        next(f)  # skip header\n        for line in f:\n            yield line.strip()\n\ndef parse_rows(lines):\n    \"\"\"Stage 2: Parse CSV text into dicts.\"\"\"\n    for line in lines:  # lines is itself a generator!\n        parts = line.split(\",\")\n        yield {\"user_id\": parts[0], \"amount\": float(parts[1]), \"date\": parts[2]}\n\ndef filter_active(rows):\n    \"\"\"Stage 3: Keep only recent rows.\"\"\"\n    cutoff = \"2024-01-01\"\n    for row in rows:\n        if row[\"date\"] >= cutoff:\n            yield row\n\ndef enrich(rows, lookup):\n    \"\"\"Stage 4: Add metadata from lookup table.\"\"\"\n    for row in rows:\n        row[\"region\"] = lookup.get(row[\"user_id\"], \"unknown\")\n        yield row\n\n# Chain: data flows through all stages lazily, one record at a time\nlookup = build_region_lookup()  # a dict \u2014 O(m) memory\npipeline = enrich(\n    filter_active(\n        parse_rows(\n            read_csv(\"events.csv\")  # 10GB file\n        )\n    ),\n    lookup\n)\n\nfor record in pipeline:  # triggers execution \u2014 pulls one record at a time\n    write_to_warehouse(record)\n# Total memory: O(1) for the pipeline + O(m) for the lookup dict</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Decorators + Context Managers</div><div class=\"rich\"><h4>Decorators \u2014 Functions That Wrap Functions</h4><p>A <strong>decorator</strong> is a function that takes another function as input, wraps it with additional behavior, and returns the wrapped version. The <code>@</code> syntax is shorthand for <code>func = decorator(func)</code>.</p><p>In data engineering: decorators handle retry logic, caching, metrics, logging, and access control \u2014 without cluttering the core business logic.</p><pre>import time\nimport functools\n\n# Decorator: retry an operation up to N times on failure\ndef retry(max_attempts=3, delay=1.0):\n    def decorator(func):\n        @functools.wraps(func)  # preserve original function metadata\n        def wrapper(*args, **kwargs):\n            for attempt in range(1, max_attempts + 1):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    if attempt == max_attempts:\n                        raise  # re-raise on final attempt\n                    print(f\"Attempt {attempt} failed: {e}. Retrying...\")\n                    time.sleep(delay)\n        return wrapper\n    return decorator\n\n# Usage: apply retry behavior to any function with one line\n@retry(max_attempts=5, delay=2.0)\ndef fetch_from_api(endpoint):\n    \"\"\"Fetches data from a flaky external API.\"\"\"\n    response = requests.get(endpoint)\n    response.raise_for_status()\n    return response.json()\n\n# Equivalent to: fetch_from_api = retry(5, 2.0)(fetch_from_api)</pre><h4>Context Managers \u2014 Guaranteed Resource Cleanup</h4><pre># Context manager: database connection that always closes\nfrom contextlib import contextmanager\n\n@contextmanager\ndef db_connection(conn_string):\n    conn = create_connection(conn_string)\n    try:\n        yield conn          # caller gets the connection here\n    finally:\n        conn.close()        # ALWAYS runs, even if exception occurs\n\n# Usage: connection guaranteed to close even on exception\nwith db_connection(\"postgresql://...\") as conn:\n    conn.execute(\"INSERT ...\")</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Memory Profiling + 500GB Pipeline</div><div class=\"rich\"><h4>Memory Profiling and Optimization Strategies</h4><p>In production data engineering, understanding memory at a deep level prevents outages. The three main memory killers in Python DE pipelines:</p><ul><li><strong>Loading entire files/datasets into lists</strong>: Replace with generators + streaming</li><li><strong>Multiple copies of the same string</strong>: Python interns short strings but not long ones \u2014 use string interning or category dtypes in Pandas</li><li><strong>DataFrames with wrong dtypes</strong>: a DataFrame with int64 columns that hold values 0-100 uses 8x more memory than int8</li></ul><pre># Memory profiling with memory_profiler\n# pip install memory-profiler\nfrom memory_profiler import profile\n\n@profile\ndef load_and_process():\n    data = pd.read_csv(\"large.csv\")     # baseline memory\n    # LINE 1: 2.0 GiB\n    data = optimize_dtypes(data)         # shrink dtypes\n    # LINE 2: 0.5 GiB \u2014 75% reduction!\n    return data\n\n# Dtype optimization:\ndef optimize_dtypes(df):\n    for col in df.select_dtypes(\"int64\").columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n    for col in df.select_dtypes(\"float64\").columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n    for col in df.select_dtypes(\"object\").columns:\n        if df[col].nunique() / len(df) < 0.5:  # <50% unique \u2192 category\n            df[col] = df[col].astype(\"category\")\n    return df\n\n# Processing chunks: alternative to generators for files too large for memory\nfor chunk in pd.read_csv(\"huge.csv\", chunksize=100_000):\n    process(chunk)  # process 100K rows at a time</pre><p><strong>FAANG interview question:</strong> \"Your pipeline processes 500GB of logs daily. Memory on your EC2 instance is 32GB. Walk me through how you would process this.\" Correct answer: generators + chunked reading + streaming writes, never load the full dataset simultaneously.</p></div></div></div>",
        "KeyConcepts": [
            "Generator function: uses 'yield' to pause/resume. Returns a generator object, not a list.",
            "Generator uses O(1) memory: it never holds all values at once \u2014 computes next value on demand.",
            "Generator expression: (expr for x in iterable) \u2014 identical syntax to list comp but lazy.",
            "Pipeline chaining: each ETL stage is a generator reading from the previous. Data flows one record at a time.",
            "Decorator: higher-order function that wraps another. '@retry' adds retry logic without changing core code.",
            "@functools.wraps preserves function name/docstring when writing decorators.",
            "Context manager: 'with' block guarantees cleanup (finally) even on exceptions.",
            "Memory optimization: dtype downcast (int64\u2192int8), category dtype for low-cardinality strings."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write a generator that reads a CSV file line by line and yields parsed dict rows. Compare memory usage vs reading the full file into a list.",
            "<strong>Step 2:</strong> Build a 3-stage generator pipeline: read \u2192 filter (only rows where amount > 100) \u2192 transform (add a 'revenue_tier' field). Process a 1M-row file.",
            "<strong>Step 3:</strong> Write a @retry(max_attempts=3, delay=1) decorator. Apply it to a function that simulates random network failures. Verify it retries correctly.",
            "<strong>Step 4:</strong> Write optimize_dtypes(df). Apply it to a DataFrame with int64 and object columns. Report memory usage before and after with df.memory_usage(deep=True).sum()."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Use sys.getsizeof() to compare List vs Generator",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Use sys.getsizeof() to compare List vs Generator \u2014 trace through 2-3 edge cases before writing any code.",
                "Can you iterate through results with a single for loop? \u2192 use generator (saves memory).",
                "Need to index into results, get len(), or iterate multiple times? \u2192 convert to list first.",
                "Chunked pandas: pd.read_csv('file.csv', chunksize=100_000) processes file in chunks without full load.",
                "Generators are lazy \u2014 they do NOTHING until iterated. Chaining generators costs zero compute up front."
            ]
        },
        "HardProblem": "Boss Problem (Netflix): Your nightly ETL reads from an S3 bucket containing 50,000 gzipped JSON files (total ~500GB uncompressed). Each file has user viewing events. (1) Write a generator pipeline: stream S3 object list \u2192 download each file \u2192 gunzip \u2192 parse JSON lines \u2192 filter by date \u2192 yield events. The pipeline should use <2GB RAM regardless of total file count. (2) For each event, do a dict lookup against a 20M-row user metadata table loaded in RAM. What is the maximum size this lookup dict can be? (3) If processing takes 14 hours, how do you parallelize using multiprocessing.Pool while keeping memory bounded?"
    },
    {
        "Week": 5,
        "Day": "Friday",
        "Phase": "1. Filter (Coding)",
        "Theme": "Python Systems",
        "SpecificTopic": "Decorators",
        "ActionItem_Deliverable": "Write a @retry decorator for flaky API calls",
        "LeetCodeProblem": "<strong>LC 53 \u2013 Maximum Subarray</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>String Manipulation</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Generators \u2014 The Memory Problem</div><div class=\"rich\"><h4>The Memory Problem \u2014 Why Generators Exist</h4><p>Imagine you need to process a 100GB log file. The naive approach: <code>lines = open(\"log.txt\").readlines()</code>. This reads all 100GB into RAM at once. If you only have 8GB of RAM, your process crashes. Even if it doesn't crash, you've consumed 100GB of memory for a computation that might only need 100MB at any one time.</p><p>This is the fundamental problem generators solve: <strong>lazy evaluation</strong>. Instead of computing all values upfront and storing them in memory, a generator computes the NEXT value only when asked. It uses O(1) memory regardless of the total sequence size.</p><h4>Generator Functions \u2014 The yield Keyword</h4><pre># Regular function: returns a list (loads everything into memory)\ndef read_all_lines(filename):\n    return open(filename).readlines()  # 100GB in RAM if large file!\n\n# Generator function: yields one line at a time (O(1) memory)\ndef stream_lines(filename):\n    with open(filename) as f:\n        for line in f:          # f iterates line by line \u2014 OS handles this\n            yield line.strip()  # pause here, return line, resume on next()\n\n# Usage: process 100GB file with 8GB RAM \u2014 works perfectly\nfor line in stream_lines(\"huge_log.txt\"):\n    process(line)  # only one line lives in memory at any time\n\n# A generator is a function that remembers WHERE IT WAS in execution.\n# \"yield\" means: return this value, pause here, resume from here next time.</pre><h4>Generator Expressions \u2014 The Lazy List Comprehension</h4><pre># List comprehension: creates all values in memory immediately\nsquares_list = [x**2 for x in range(10_000_000)]   # 80MB in memory\n\n# Generator expression: creates values on demand, same syntax with ()\nsquares_gen  = (x**2 for x in range(10_000_000))    # tiny (generator object)\n\n# Both work identically in a for loop:\ntotal = sum(x**2 for x in range(10_000_000))  # never stores all 10M values!\n\n# Rule for data engineering: use generator expression whenever you will\n# iterate through results ONCE (sum, filter, write to file, stream out).\n# Use list when you need to: index into results, iterate multiple times, or check len().</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 ETL Pipeline with Generator Chaining</div><div class=\"rich\"><h4>Real-World DE Pattern: ETL Pipeline with Generators</h4><p>The most powerful use of generators in data engineering: chaining them into a pipeline. Each stage of the ETL is a generator that reads from the previous stage. The entire pipeline runs with O(1) memory \u2014 data flows through one record at a time.</p><pre># ETL Pipeline: read \u2192 parse \u2192 filter \u2192 transform \u2192 write\n# Each function is a generator. Data flows record-by-record.\n\ndef read_csv(filepath):\n    \"\"\"Stage 1: Read file line by line.\"\"\"\n    with open(filepath) as f:\n        next(f)  # skip header\n        for line in f:\n            yield line.strip()\n\ndef parse_rows(lines):\n    \"\"\"Stage 2: Parse CSV text into dicts.\"\"\"\n    for line in lines:  # lines is itself a generator!\n        parts = line.split(\",\")\n        yield {\"user_id\": parts[0], \"amount\": float(parts[1]), \"date\": parts[2]}\n\ndef filter_active(rows):\n    \"\"\"Stage 3: Keep only recent rows.\"\"\"\n    cutoff = \"2024-01-01\"\n    for row in rows:\n        if row[\"date\"] >= cutoff:\n            yield row\n\ndef enrich(rows, lookup):\n    \"\"\"Stage 4: Add metadata from lookup table.\"\"\"\n    for row in rows:\n        row[\"region\"] = lookup.get(row[\"user_id\"], \"unknown\")\n        yield row\n\n# Chain: data flows through all stages lazily, one record at a time\nlookup = build_region_lookup()  # a dict \u2014 O(m) memory\npipeline = enrich(\n    filter_active(\n        parse_rows(\n            read_csv(\"events.csv\")  # 10GB file\n        )\n    ),\n    lookup\n)\n\nfor record in pipeline:  # triggers execution \u2014 pulls one record at a time\n    write_to_warehouse(record)\n# Total memory: O(1) for the pipeline + O(m) for the lookup dict</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Decorators + Context Managers</div><div class=\"rich\"><h4>Decorators \u2014 Functions That Wrap Functions</h4><p>A <strong>decorator</strong> is a function that takes another function as input, wraps it with additional behavior, and returns the wrapped version. The <code>@</code> syntax is shorthand for <code>func = decorator(func)</code>.</p><p>In data engineering: decorators handle retry logic, caching, metrics, logging, and access control \u2014 without cluttering the core business logic.</p><pre>import time\nimport functools\n\n# Decorator: retry an operation up to N times on failure\ndef retry(max_attempts=3, delay=1.0):\n    def decorator(func):\n        @functools.wraps(func)  # preserve original function metadata\n        def wrapper(*args, **kwargs):\n            for attempt in range(1, max_attempts + 1):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    if attempt == max_attempts:\n                        raise  # re-raise on final attempt\n                    print(f\"Attempt {attempt} failed: {e}. Retrying...\")\n                    time.sleep(delay)\n        return wrapper\n    return decorator\n\n# Usage: apply retry behavior to any function with one line\n@retry(max_attempts=5, delay=2.0)\ndef fetch_from_api(endpoint):\n    \"\"\"Fetches data from a flaky external API.\"\"\"\n    response = requests.get(endpoint)\n    response.raise_for_status()\n    return response.json()\n\n# Equivalent to: fetch_from_api = retry(5, 2.0)(fetch_from_api)</pre><h4>Context Managers \u2014 Guaranteed Resource Cleanup</h4><pre># Context manager: database connection that always closes\nfrom contextlib import contextmanager\n\n@contextmanager\ndef db_connection(conn_string):\n    conn = create_connection(conn_string)\n    try:\n        yield conn          # caller gets the connection here\n    finally:\n        conn.close()        # ALWAYS runs, even if exception occurs\n\n# Usage: connection guaranteed to close even on exception\nwith db_connection(\"postgresql://...\") as conn:\n    conn.execute(\"INSERT ...\")</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Memory Profiling + 500GB Pipeline</div><div class=\"rich\"><h4>Memory Profiling and Optimization Strategies</h4><p>In production data engineering, understanding memory at a deep level prevents outages. The three main memory killers in Python DE pipelines:</p><ul><li><strong>Loading entire files/datasets into lists</strong>: Replace with generators + streaming</li><li><strong>Multiple copies of the same string</strong>: Python interns short strings but not long ones \u2014 use string interning or category dtypes in Pandas</li><li><strong>DataFrames with wrong dtypes</strong>: a DataFrame with int64 columns that hold values 0-100 uses 8x more memory than int8</li></ul><pre># Memory profiling with memory_profiler\n# pip install memory-profiler\nfrom memory_profiler import profile\n\n@profile\ndef load_and_process():\n    data = pd.read_csv(\"large.csv\")     # baseline memory\n    # LINE 1: 2.0 GiB\n    data = optimize_dtypes(data)         # shrink dtypes\n    # LINE 2: 0.5 GiB \u2014 75% reduction!\n    return data\n\n# Dtype optimization:\ndef optimize_dtypes(df):\n    for col in df.select_dtypes(\"int64\").columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n    for col in df.select_dtypes(\"float64\").columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n    for col in df.select_dtypes(\"object\").columns:\n        if df[col].nunique() / len(df) < 0.5:  # <50% unique \u2192 category\n            df[col] = df[col].astype(\"category\")\n    return df\n\n# Processing chunks: alternative to generators for files too large for memory\nfor chunk in pd.read_csv(\"huge.csv\", chunksize=100_000):\n    process(chunk)  # process 100K rows at a time</pre><p><strong>FAANG interview question:</strong> \"Your pipeline processes 500GB of logs daily. Memory on your EC2 instance is 32GB. Walk me through how you would process this.\" Correct answer: generators + chunked reading + streaming writes, never load the full dataset simultaneously.</p></div></div></div>",
        "KeyConcepts": [
            "Generator function: uses 'yield' to pause/resume. Returns a generator object, not a list.",
            "Generator uses O(1) memory: it never holds all values at once \u2014 computes next value on demand.",
            "Generator expression: (expr for x in iterable) \u2014 identical syntax to list comp but lazy.",
            "Pipeline chaining: each ETL stage is a generator reading from the previous. Data flows one record at a time.",
            "Decorator: higher-order function that wraps another. '@retry' adds retry logic without changing core code.",
            "@functools.wraps preserves function name/docstring when writing decorators.",
            "Context manager: 'with' block guarantees cleanup (finally) even on exceptions.",
            "Memory optimization: dtype downcast (int64\u2192int8), category dtype for low-cardinality strings."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write a generator that reads a CSV file line by line and yields parsed dict rows. Compare memory usage vs reading the full file into a list.",
            "<strong>Step 2:</strong> Build a 3-stage generator pipeline: read \u2192 filter (only rows where amount > 100) \u2192 transform (add a 'revenue_tier' field). Process a 1M-row file.",
            "<strong>Step 3:</strong> Write a @retry(max_attempts=3, delay=1) decorator. Apply it to a function that simulates random network failures. Verify it retries correctly.",
            "<strong>Step 4:</strong> Write optimize_dtypes(df). Apply it to a DataFrame with int64 and object columns. Report memory usage before and after with df.memory_usage(deep=True).sum()."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Write a @retry decorator for flaky API calls",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Write a @retry decorator for flaky API calls \u2014 trace through 2-3 edge cases before writing any code.",
                "Can you iterate through results with a single for loop? \u2192 use generator (saves memory).",
                "Need to index into results, get len(), or iterate multiple times? \u2192 convert to list first.",
                "Chunked pandas: pd.read_csv('file.csv', chunksize=100_000) processes file in chunks without full load.",
                "Generators are lazy \u2014 they do NOTHING until iterated. Chaining generators costs zero compute up front."
            ]
        },
        "HardProblem": "Boss Problem (Netflix): Your nightly ETL reads from an S3 bucket containing 50,000 gzipped JSON files (total ~500GB uncompressed). Each file has user viewing events. (1) Write a generator pipeline: stream S3 object list \u2192 download each file \u2192 gunzip \u2192 parse JSON lines \u2192 filter by date \u2192 yield events. The pipeline should use <2GB RAM regardless of total file count. (2) For each event, do a dict lookup against a 20M-row user metadata table loaded in RAM. What is the maximum size this lookup dict can be? (3) If processing takes 14 hours, how do you parallelize using multiprocessing.Pool while keeping memory bounded?"
    },
    {
        "Week": 5,
        "Day": "Saturday",
        "Phase": "1. Filter (Coding)",
        "Theme": "Python Systems",
        "SpecificTopic": "Mock Assessment + Mock #1",
        "ActionItem_Deliverable": "1 System Scripting Task + Full Mock Interview Loop #1",
        "LeetCodeProblem": "<strong>LC 56 \u2013 Merge Intervals</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": "NEW: Mock Interview #1 added",
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Heaps / Priority Queues</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 What Spark Is \u2014 Driver/Executor + Lazy Eval</div><div class=\"rich\"><h4>What Spark Actually Is \u2014 And Why It Exists</h4><p>Apache Spark was created in 2009 at UC Berkeley's AMPLab as a faster replacement for Hadoop MapReduce. Hadoop MapReduce wrote intermediate results to disk after every step. A 10-step pipeline wrote and read disk 10 times \u2014 massive I/O overhead. Spark keeps intermediate results <strong>in memory</strong>, passing data between stages without disk. For iterative algorithms (machine learning) and multi-step pipelines, this is 10\u2013100x faster.</p><h4>How Spark Executes Code \u2014 The Driver/Executor Model</h4><pre>Spark Cluster Architecture:\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              DRIVER (your Python code)          \u2502\n\u2502  - Runs on the master node                      \u2502\n\u2502  - Builds the execution plan                    \u2502\n\u2502  - Sends tasks to executors                     \u2502\n\u2502  - Collects results                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502 (sends serialized tasks)\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u25bc              \u25bc              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Executor \u2502  \u2502Executor \u2502  \u2502Executor \u2502\n\u2502 - Node 1\u2502  \u2502 - Node 2\u2502  \u2502 - Node 3\u2502\n\u2502 - tasks \u2502  \u2502 - tasks \u2502  \u2502 - tasks \u2502\n\u2502 - cache \u2502  \u2502 - cache \u2502  \u2502 - cache \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n# DataFrame operations run on executors in parallel\n# Driver only sees final results (unless you call .collect() \u2014 dangerous!)</pre><h4>Lazy Evaluation \u2014 Nothing Runs Until You Ask</h4><p>Spark is lazy: DataFrame transformations (filter, select, join, groupBy) do NOT execute immediately. They build a <strong>query plan</strong>. Only <strong>actions</strong> (collect(), count(), write(), show()) trigger actual execution.</p><pre>df = spark.read.parquet(\"s3://events/\")   # no I/O yet\ndf2 = df.filter(df.year == 2024)           # no I/O yet \u2014 builds plan\ndf3 = df2.groupBy(\"country\").count()       # no I/O yet \u2014 extends plan\n\ndf3.show()  # \u2190 ACTION: NOW Spark executes the entire plan\n\n# Why lazy? Spark's optimizer (Catalyst) can rearrange operations:\n# - Pushes filters before joins (read less data)\n# - Eliminates unused columns early\n# - Merges multiple transformations into single scan\n# This often makes your unoptimized code run faster automatically.</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 The Shuffle + Broadcast Joins</div><div class=\"rich\"><h4>The Shuffle \u2014 The Most Expensive Spark Operation</h4><p>A <strong>shuffle</strong> occurs when data needs to be redistributed across executors based on a key \u2014 for example, during a <code>groupBy</code>, <code>join</code>, or <code>repartition</code>. Shuffle is expensive because: (1) executors must write their partition to disk, (2) data is sent over the network, (3) receiving executors read from disk and memory. In a job with 100 executors processing 1TB, a shuffle can move terabytes over the network.</p><pre># Operations that trigger a shuffle:\ndf.groupBy(\"country\").count()            # \u2190 SHUFFLE: all rows for same country\n                                         #   must land on same executor\ndf.join(other, \"user_id\")               # \u2190 SHUFFLE: matching user_ids must\n                                         #   be co-located\ndf.repartition(200, \"country\")          # \u2190 SHUFFLE: explicit redistribution\ndf.distinct()                            # \u2190 SHUFFLE: deduplicate requires global view\n\n# Operations that do NOT shuffle (cheap \"narrow\" transformations):\ndf.filter(df.year == 2024)              # stays on same partition\ndf.select(\"user_id\", \"amount\")          # stays on same partition\ndf.withColumn(\"total\", df.a + df.b)    # stays on same partition\ndf.coalesce(10)                         # merge partitions, no redistribution\n\n# Reduce shuffles by:\n# 1. Partition data by join key BEFORE the join (pre-partition)\n# 2. Broadcast small tables (eliminates join shuffle entirely)\n# 3. Aggregate before joining (reduce data volume before shuffle)</pre><h4>Broadcast Join \u2014 Eliminating the Join Shuffle</h4><pre># Problem: joining a 100TB fact table with a 50MB dimension\n# Default join \u2192 shuffle 100TB over network (catastrophically slow)\n\n# Solution: broadcast the small table to ALL executors\n# Each executor gets a full copy \u2014 no network movement for the large table\n\nfrom pyspark.sql.functions import broadcast\n\nresult = large_df.join(\n    broadcast(small_dim_df),  # \u2190 small table copied to all executors\n    on=\"product_id\"\n)  # No shuffle of large_df! Each executor joins locally.\n\n# Spark auto-broadcasts tables < spark.sql.autoBroadcastJoinThreshold (default 10MB)\n# Increase for larger dimensions if they fit in executor memory:\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"100m\")</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Data Skew + Partitions vs Tasks</div><div class=\"rich\"><h4>Data Skew \u2014 When One Executor Does 90% of the Work</h4><p><strong>Data skew</strong> is the #1 production Spark performance problem. It occurs when one partition key value has far more rows than others. Example: a NULL user_id for all guest orders means the \"NULL\" partition gets 80% of all rows. One executor processing 8TB while others process 100GB \u2014 the job is only as fast as the slowest executor.</p><pre># Detect skew: check partition sizes\ndf.groupBy(spark_partition_id()).count().show()  # see partition row counts\n# If one partition has 100M rows and others have 1M \u2192 skew\n\n# Fix 1: Salt the key (add random suffix to distribute skewed keys)\nfrom pyspark.sql import functions as F\n\n# Add salt to the large table\nnum_salt_buckets = 10\nsalted_large = large_df.withColumn(\n    \"salted_key\",\n    F.concat(F.col(\"user_id\"), F.lit(\"_\"), (F.rand() * num_salt_buckets).cast(\"int\"))\n)\n\n# Explode the small table to match all salted keys\nsalt_values = spark.range(num_salt_buckets).withColumnRenamed(\"id\", \"salt\")\nsalted_small = small_df.crossJoin(salt_values).withColumn(\n    \"salted_key\",\n    F.concat(F.col(\"user_id\"), F.lit(\"_\"), F.col(\"salt\"))\n)\n\n# Now join on salted_key \u2014 skew is evenly distributed!\nresult = salted_large.join(salted_small, \"salted_key\")\n\n# Fix 2: Filter skewed keys and process separately\nnormal = df.filter(F.col(\"user_id\").isNotNull())\nnulls  = df.filter(F.col(\"user_id\").isNull())\n# Process each differently, then union</pre><h4>Partitions vs Tasks \u2014 Understanding Parallelism</h4><p>In Spark, a <strong>partition</strong> is a chunk of data. A <strong>task</strong> processes one partition. The number of parallel tasks = number of executor cores. If you have 100 cores and 1,000 partitions \u2192 100 tasks run simultaneously, 10 rounds of 100 tasks = 10 rounds total.</p><pre># Rule of thumb: 2-4 partitions per CPU core\n# 100 cores \u2192 200-400 partitions\n\n# Too few partitions: executors are idle (not enough work to distribute)\n# Too many partitions: per-partition overhead dominates (too many small tasks)\n\ndf.rdd.getNumPartitions()  # check current partition count\ndf.repartition(400)  # redistribute to 400 partitions (shuffle)\ndf.coalesce(10)      # reduce to 10 partitions (no shuffle, but may be uneven)\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"400\")  # for groupBy/join output</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Diagnosing a Slow Spark Job</div><div class=\"rich\"><h4>FAANG Interview: Diagnosing a Slow Spark Job</h4><p>Interview question: \"Your Spark job processes 10TB of data but takes 6 hours. Walk me through diagnosing it.\"</p><pre>Step 1 \u2014 Open the Spark UI (port 4040)\n  \u2192 DAG tab: which stage takes the most time?\n  \u2192 Stage details: which task is the slowest? (skew indicator)\n  \u2192 Storage tab: how much data is cached? Is spill occurring?\n\nStep 2 \u2014 Identify the type of slowdown:\n\n  A. One stage takes 90% of job time:\n     \u2192 Check if it contains a shuffle (groupBy, join)\n     \u2192 If so: can you reduce data before the shuffle? Broadcast the smaller side?\n\n  B. One task in a stage takes 10x longer than others:\n     \u2192 Data skew. Check: what is the key distribution?\n     \u2192 Fix: salt the key, or handle skewed keys separately\n\n  C. Spill to disk (red in Spark UI):\n     \u2192 Executor ran out of memory, wrote shuffle data to disk\n     \u2192 Fix: increase executor memory, or increase spark.sql.shuffle.partitions\n       (more partitions = smaller per-partition size = less memory per partition)\n\n  D. Many stages, each fast individually but slow overall:\n     \u2192 Wide dependency chain \u2014 hard to parallelize\n     \u2192 Fix: checkpoint intermediate results, cache frequently reused DataFrames\n\ndf.cache()  # cache in memory \u2014 avoid recomputing in subsequent actions\ndf.persist(StorageLevel.DISK_ONLY)  # persist to disk if too large for memory</pre><p>\u270d\ufe0f <strong>The Spark UI is your most important debugging tool.</strong> FAANG interviewers expect you to know how to read it and what each metric means.</p></div></div></div>",
        "KeyConcepts": [
            "Spark = distributed in-memory computing. Driver builds plan, executors execute tasks on partitions.",
            "Lazy evaluation: transformations build a plan. Actions (count/show/write) trigger execution.",
            "Catalyst optimizer: rearranges your query plan for efficiency \u2014 predicate pushdown, column pruning.",
            "Shuffle: data redistribution across network. Most expensive operation. Triggered by groupBy, join, distinct.",
            "Narrow transformation: each partition maps to exactly one output partition. No shuffle. Fast.",
            "Broadcast join: copies small table to all executors. Eliminates join shuffle for large+small joins.",
            "Data skew: one partition key has disproportionate rows. Fix: salting (randomize key) or separate processing.",
            "Partitions = data chunks. Tasks = work units (1 per partition). Target 2-4 partitions per core.",
            "spark.sql.shuffle.partitions default = 200. Tune to 2-4x your number of cores."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Read 10M rows. Chain: filter \u2192 groupBy \u2192 count. Use .explain(True) to show the physical plan. Identify which operations cause shuffles.",
            "<strong>Step 2:</strong> Join a 1B-row fact table with a 5MB lookup table. Compare: regular join vs broadcast join. Measure time difference with spark.time().",
            "<strong>Step 3:</strong> Simulate data skew: create a dataset where 80% of rows have user_id=NULL. Run a groupBy. Observe one task taking 10x longer. Apply the salt fix.",
            "<strong>Step 4:</strong> Set spark.sql.shuffle.partitions to 10, 200, 1000 on the same groupBy query. Observe partition count and execution time changes."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> 1 System Scripting Task + Full Mock Interview Loop #1",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> 1 System Scripting Task + Full Mock Interview Loop #1 \u2014 trace through 2-3 edge cases before writing any code.",
                "Seeing one stage taking 90% of job time? Check for shuffle. Can you broadcast the smaller side?",
                "One task running 10x longer than others? Data skew. Check key value distribution with groupBy + count.",
                "Spill to disk in Spark UI? Executor out of memory. Increase executor memory or spark.sql.shuffle.partitions.",
                "Cache DataFrames reused multiple times: df.cache(). Unpersist when done: df.unpersist()."
            ]
        },
        "HardProblem": "Boss Problem (Uber): Your daily Spark job aggregates 500B GPS ping events (5TB) to compute driver earnings per city. It runs in 14 hours but needs to run in 2 hours. Steps: (1) You find 3 shuffle stages dominate. How do you investigate each one? (2) One join between pings (5TB) and a driver_metadata table (2GB) causes a 3TB shuffle. How do you fix it without salting? (3) City='NULL' has 60% of all pings (GPS loss). How do you handle this skew? (4) The job reads the same 5TB raw data for 4 different aggregations. How do you avoid reading it 4 times?"
    },
    {
        "Week": 5,
        "Day": "Sunday",
        "Phase": "1. Filter (Coding)",
        "Theme": "Rest",
        "SpecificTopic": "Rest & Review",
        "ActionItem_Deliverable": "Review Python memory management",
        "LeetCodeProblem": null,
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Heaps / Priority Queues</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Active Rest \u2014 How Memory Consolidates</div>\n<div class=\"rich\">\n<h4>The Neuroscience of Spaced Repetition</h4>\n<p>Rest days are not wasted days. During rest, your brain replays what it learned \u2014 a process called <strong>memory consolidation</strong>. Research consistently shows that <em>attempting to recall from memory</em> is 3\u00d7 more effective at building durable knowledge than re-reading notes. The act of struggling to remember \u2014 even failing \u2014 strengthens the neural pathway more than passive review.</p>\n<p>Think of it like muscle recovery: a weightlifter who trains 7 days straight without rest gets weaker, not stronger. The growth happens during recovery. Your brain is the same.</p>\n<h4>Today's Protocol (4 hours, structured)</h4>\n<ol>\n  <li><strong>Active Recall (60 min):</strong> Close everything. On a blank page, write every concept, formula, and pattern from this week from memory. Then check.</li>\n  <li><strong>Error Review (60 min):</strong> Find the hardest problem you struggled with. Attempt it again completely from scratch \u2014 no notes.</li>\n  <li><strong>Teach It (45 min):</strong> Open the AI chatbot and explain Gaps & Islands to it as if it's a junior engineer. If you stumble, that's your gap.</li>\n  <li><strong>Next Week Preview (15 min):</strong> Skim next Monday's topic at headline level only \u2014 priming improves retention when you study it deeply.</li>\n</ol>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Active Recall Quiz \u2014 No Notes Allowed</div>\n<div class=\"rich\">\n<h4>Answer These From Memory. Then Check.</h4>\n<pre>Window Functions:\n\u25a1 What is the difference between ROW_NUMBER, RANK, and DENSE_RANK?\n  (Write the output for a table with two tied rows)\n\u25a1 Write the syntax for a 7-day rolling average from memory\n\u25a1 What does PARTITION BY do differently from GROUP BY?\n\u25a1 Write the complete Gaps & Islands CTE from memory\n\u25a1 What is the difference between ROWS BETWEEN and RANGE BETWEEN?\n\u25a1 LAG(col, 1, 0) vs LAG(col, 1) \u2014 what is different?\n\u25a1 In what SQL execution order step do window functions run?\n\u25a1 Why can't you write WHERE rank &lt;= 3 in the same query as ROW_NUMBER?\n\nScore yourself:\n8/8 = \ud83c\udfc6 Mastery level \u2014 you're ready for interviews on this\n5-7/8 = \ud83d\udfe1 Good \u2014 review the 1-2 concepts you missed\n&lt; 5/8 = \ud83d\udd34 Go back to the specific day's content, re-read Level 1-2</pre>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Spaced Repetition Schedule</div>\n<div class=\"rich\">\n<h4>When to Review This Week's Material</h4>\n<table>\n<tr><th>Review this content</th><th>When</th><th>How</th></tr>\n<tr><td>This week</td><td>Today</td><td>Active recall quiz above</td></tr>\n<tr><td>This week</td><td>In 3 days</td><td>Re-solve the hard problem from Day 4</td></tr>\n<tr><td>This week</td><td>In 7 days</td><td>LeetCode: solve problems 185, 180, 196</td></tr>\n<tr><td>This week</td><td>In 30 days</td><td>Explain all 4 concepts to someone else</td></tr>\n</table>\n<h4>The Feynman Technique</h4>\n<p>After studying any concept, close your notes and explain it as if you're teaching a newcomer. Use only simple words. Avoid jargon. If you stumble, that's exactly where your understanding breaks down \u2014 return to just that part. This technique built Richard Feynman's legendary ability to explain complex physics simply.</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Interview Readiness Self-Assessment</div>\n<div class=\"rich\">\n<pre>Rate yourself 1\u20135 on each topic (honest self-assessment):\n  1 = \"I need to re-read from scratch\"\n  3 = \"I understand but need to look at notes\"\n  5 = \"I can code this under pressure from memory\"\n\nROW_NUMBER / RANK / DENSE_RANK:        ___/5\nRolling windows (ROWS BETWEEN):        ___/5\nLAG/LEAD (adjacent row comparison):    ___/5\nGaps & Islands (date streaks):         ___/5\nSession detection (timestamp version): ___/5\n\nAction:\n  Score &lt;3: revisit that day's Level 1 and Level 2 content tomorrow\n  Score 3:  do one LeetCode problem on that specific topic\n  Score 5:  you're interview-ready \u2705 on this topic</pre>\n<h4>FAANG Interview Reality Check</h4>\n<p>At meta, Amazon, and Google data engineering interviews, you'll be asked to write SQL live on a shared screen without autocomplete or documentation. The most common reason candidates fail: they know the concept but freeze on the syntax under pressure. The fix: write every pattern from memory at least 5 times this week. Muscle memory is more reliable than conscious recall under stress.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Memory consolidation happens during rest \u2014 the brain replays and stabilizes learned patterns during downtime.",
            "Active recall (testing yourself) builds 3x stronger memory than re-reading notes.",
            "Spaced repetition: review at day 1, day 3, day 7, day 30 for maximum retention.",
            "The Feynman Technique: if you can't explain it simply, you don't fully understand it yet.",
            "Error analysis: the problems you got WRONG are your highest-value review items \u2014 spend the most time there."
        ],
        "Tasks": [
            "<strong>Active Recall:</strong> Write the Gaps & Islands CTE from memory on paper. Check it against Day 4.",
            "<strong>Re-attempt:</strong> Pick the hardest problem from this week and solve it in 20 minutes from scratch.",
            "<strong>Teach it:</strong> Explain RANK vs DENSE_RANK to the AI chatbot as if it's a junior. Get it right without notes.",
            "<strong>Preview:</strong> Skim next week's Monday topic for 10 minutes \u2014 just headline concepts."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Review Python memory management",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Review Python memory management \u2014 trace through 2-3 edge cases before writing any code.",
                "Use the AI chatbot to quiz yourself \u2014 ask it 'Give me a SQL window function interview question.'",
                "Write SQL from memory in a plain text editor \u2014 no autocomplete. This builds true recall.",
                "The hardest concept this week: spend 30 minutes writing a full explanation in your own words."
            ]
        },
        "HardProblem": "Connect-the-dots: Write 2-3 paragraphs explaining how ROW_NUMBER, rolling averages, LAG, and Gaps & Islands could ALL be used in a single production pipeline for a ride-sharing company. What problem does each solve? How do they complement each other? What would break if you removed any one of them?"
    },
    {
        "Week": 6,
        "Day": "Monday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Storage Internals",
        "SpecificTopic": "Row vs Columnar",
        "ActionItem_Deliverable": "Draw layout of Parquet (Header-RowGroups-Footer)",
        "LeetCodeProblem": "<strong>LC 238 \u2013 Product of Array Except Self</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Sets & Deduplication</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Row vs Columnar \u2014 The Layout Trade-Off</div><div class=\"rich\"><h4>The Fundamental Trade-Off in Storage Layout</h4><p>When you store data on disk, you have a fundamental choice: store all columns of one row together (row-oriented) or store all values of one column together (column-oriented). This decision affects compression ratios, query speed, write speed, and how much data you scan for any given query.</p><h4>Row-Oriented Storage \u2014 How It Works</h4><p>In a row-oriented format (CSV, MySQL, PostgreSQL by default), all columns of Row 1 are stored together on disk, then all columns of Row 2, etc.</p><pre>Disk layout for a 3-row, 4-column table:\n\nRow 1: [user_id=1][name=Alice][age=30][country=US]\nRow 2: [user_id=2][name=Bob][age=25][country=UK]\nRow 3: [user_id=3][name=Carol][age=28][country=US]\n\nQuery: SELECT country, SUM(age) GROUP BY country\nWhat disk must read: ALL of [user_id, name, age, country] for every row\nEven though user_id and name are not needed at all!\n\nFor 100 columns, analytical queries typically need 3-5.\nRow storage reads 100 columns to use 5 = 95% wasted I/O.</pre><h4>Column-Oriented Storage \u2014 How It Works</h4><p>In a column-oriented format (Parquet, ORC), all values of column 1 are stored together, then all of column 2, etc.</p><pre>Disk layout (same data, columnar):\n\nColumn user_id: [1][2][3]\nColumn name:    [Alice][Bob][Carol]\nColumn age:     [30][25][28]\nColumn country: [US][UK][US]\n\nQuery: SELECT country, SUM(age) GROUP BY country\nWhat disk must read: ONLY [age] and [country] column blocks\nuser_id and name are not read at all \u2014 they live in different disk blocks!\n\nFor a 100-column table querying 5 columns: reads 5% of the data.</pre><p>\u270d\ufe0f <strong>Rule:</strong> Row storage is optimal for OLTP (write one complete row, read one complete row). Column storage is optimal for OLAP (read few columns across all rows, aggregate).</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Compression, Formats, Parquet Internals</div><div class=\"rich\"><h4>Why Columnar Format Compresses So Well</h4><p>Compression works by finding patterns and encoding them more efficiently. Column storage makes compression dramatically more effective because <strong>values in the same column have the same type and similar values</strong>.</p><ul><li><strong>country column:</strong> [\"US\",\"UK\",\"US\",\"US\",\"CA\",\"US\",\"UK\"] \u2014 only a few unique values repeat. Dictionary encoding: US=1, UK=2, CA=3 \u2192 stores [1,2,1,1,3,1,2] as integers. 90%+ compression.</li><li><strong>age column:</strong> [30,25,28,31,22,25,28] \u2014 small integers, all similar range. Delta encoding: store [30, -5, +3, +3, -9, +3, +3]. Small deltas compress extremely well.</li><li><strong>timestamp column:</strong> [1704067200, 1704067800, 1704068400] \u2014 always increasing, constant difference. RLE encodes as: start=1704067200, step=600, count=3.</li></ul><table><tr><th>Format</th><td>Compression</td><td>Read speed</td><td>Write speed</td><td>Best for</td></tr><tr><th>CSV</th><td>None (text)</td><td>Slow (parse text)</td><td>Fast</td><td>Data exchange, debugging</td></tr><tr><th>JSON</th><td>None</td><td>Very slow (parse tree)</td><td>Medium</td><td>APIs, nested data</td></tr><tr><th>Avro</th><td>Medium</td><td>Medium (row-oriented)</td><td>Very fast</td><td>Kafka messages, write-heavy</td></tr><tr><th>Parquet</th><td>High (columnar)</td><td>Very fast for queries</td><td>Slower</td><td>Analytics, data warehouses</td></tr><tr><th>ORC</th><td>High (columnar)</td><td>Very fast (Hive/Spark)</td><td>Slower</td><td>Hive workloads, transactional</td></tr><tr><th>Delta Lake</th><td>High + ACID</td><td>Very fast + time-travel</td><td>Medium</td><td>Lakehouse: ACID + analytics</td></tr></table><h4>Parquet File Structure \u2014 What's Inside</h4><pre>Parquet file internals:\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  MAGIC BYTES: \"PAR1\"                        \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Row Group 1 (e.g., 128MB of rows)          \u2502\n\u2502    Column chunk: user_id   [encoded values] \u2502\n\u2502    Column chunk: age       [encoded values] \u2502\n\u2502    Column chunk: country   [encoded values] \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Row Group 2 (next 128MB of rows)           \u2502\n\u2502    Column chunks...                         \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Footer: schema + row group offsets         \u2502\n\u2502    min/max statistics per column per chunk  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n# Statistics in footer enable predicate pushdown:\n# WHERE country = \"US\" \u2192 check if \"US\" is within [min,max] of each row group\n# Skip entire row groups that cannot contain the value \u2192 zero bytes read!</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 S3 Partitioning + Small File Problem</div><div class=\"rich\"><h4>S3 Partitioning \u2014 Hive-Style Directory Layout</h4><p>When storing large datasets on S3 (or HDFS), the directory structure acts as a physical partition. Spark/Hive reads ONLY partition directories that match the query filter, completely skipping all other directories.</p><pre># Good partition structure (by year/month/day)\ns3://my-bucket/events/\n    year=2024/\n        month=01/\n            day=01/\n                part-00000.parquet\n                part-00001.parquet\n            day=02/\n                ...\n        month=02/...\n\n# Query: WHERE year=2024 AND month=01 AND day=01\n# Reads: ONLY s3://my-bucket/events/year=2024/month=01/day=01/\n# Skips: all other months and days entirely\n\n# Spark reads partition from path automatically:\ndf = spark.read.parquet(\"s3://my-bucket/events/\")\ndf.filter((df.year==2024) & (df.month==1)).show()\n# Spark sees partition filter \u2192 reads only matching directories</pre><h4>The Small File Problem \u2014 The Silent Performance Killer</h4><p>Data lakes accumulate thousands of tiny files (1-10MB each) over time, typically from: streaming jobs writing every minute, or daily partitions with many sub-partitions.</p><p>The problem: Parquet/HDFS has per-file overhead. 10,000 files of 5MB each reads SLOWER than 50 files of 1GB each \u2014 even though it's the same total data. Every file requires one filesystem API call, one file open, reading the footer, and metadata validation.</p><pre># Detect small file problem in Spark:\ndf.rdd.getNumPartitions()  # if this is > 10,000, you have too many small files\n\n# Fix: coalesce (narrow transform) or repartition (full shuffle)\ndf.coalesce(200).write.parquet(\"output/\")  # merge to 200 files \u2014 no shuffle\ndf.repartition(200).write.parquet(\"output/\")  # redistribute evenly \u2014 full shuffle\n\n# Rule: target file size 128MB\u20131GB. coalesce < 200 files is usually fine.</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Delta Lake \u2014 ACID on Data Lakes</div><div class=\"rich\"><h4>Delta Lake \u2014 ACID Transactions on a Data Lake</h4><p>Traditional data lakes (S3 + Parquet) have no transaction support: if you DELETE or UPDATE records, you have to rewrite entire Parquet files. If your job fails halfway through, you have partial data written \u2014 corrupted state. <strong>Delta Lake</strong> adds ACID transactions and time travel to any Parquet-based data lake.</p><pre># Delta Lake stores data as Parquet + a transaction log (_delta_log/)\ns3://my-bucket/orders/\n    _delta_log/\n        00000000000000000000.json  # commit 0: initial data\n        00000000000000000001.json  # commit 1: added rows\n        00000000000000000002.json  # commit 2: deleted rows\n    part-00000.parquet\n    part-00001.parquet\n\n# ACID operations:\nfrom delta.tables import DeltaTable\ndt = DeltaTable.forPath(spark, \"s3://my-bucket/orders/\")\n\n# DELETE with conditions (records are marked deleted in log, not physically removed)\ndt.delete(\"order_status = 'CANCELLED'\")\n\n# UPSERT (MERGE INTO): insert new rows, update existing ones\ndt.alias(\"target\").merge(\n    updates_df.alias(\"source\"),\n    \"target.order_id = source.order_id\"\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n\n# Time travel: query data as it was at a previous point\ndf_yesterday = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-01-01\").load(...)</pre><p><strong>FAANG interview:</strong> \"When would you choose Delta Lake over plain Parquet?\"</p><ul><li>You need to DELETE or UPDATE individual records (GDPR right-to-be-forgotten)</li><li>You need ACID guarantees: multiple writers, no partial writes</li><li>You need time travel: audit trail, debugging, rollbacks</li><li>You process late-arriving data that needs to be merged into historical partitions</li><li>Plain Parquet is better: write-once append-only workloads where simplicity is valued</li></ul></div></div></div>",
        "KeyConcepts": [
            "Row storage: all columns of one row together. Fast for full-row reads/writes (OLTP).",
            "Columnar storage: all values of one column together. Fast for analytical queries (scan few columns of many rows).",
            "Columnar compression: same-type values in a block compress 5\u201310x better than mixed row data.",
            "Parquet internals: row groups (128MB), column chunks within groups, statistics in footer.",
            "Footer statistics enable predicate pushdown: skip entire row groups that can't match WHERE condition.",
            "S3 partitioning: Hive-style year=/month=/day= directories. Spark reads only matching partitions.",
            "Small file problem: 10K 5MB files read slower than 50 1GB files. Target 128MB\u20131GB per file.",
            "Delta Lake: adds ACID + time travel to Parquet. Use when you need DELETE/UPDATE/upsert on a data lake."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create a 1M-row DataFrame. Write it as CSV and as Parquet. Compare file sizes on disk. Then query only 2 of 10 columns \u2014 compare read times.",
            "<strong>Step 2:</strong> Write the same DataFrame partitioned by year and month. Confirm Spark uses partition pruning by checking the query plan with .explain().",
            "<strong>Step 3:</strong> Simulate small file problem: write 1,000 small Parquet files. Then coalesce to 10 files. Compare read performance.",
            "<strong>Step 4:</strong> Try a Delta Lake MERGE (upsert): start with 1M orders, receive 10K updates and 5K new rows. Use .merge() to apply changes. Read back and verify."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Draw layout of Parquet (Header-RowGroups-Footer)",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Draw layout of Parquet (Header-RowGroups-Footer) \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: 'Why Parquet over CSV?' \u2192 columnar compression, predicate pushdown, schema enforcement.",
                "Footer statistics in Parquet: min/max per column chunk enable row group skipping \u2014 huge for filtered reads.",
                "Small file fix: df.coalesce(200) (no shuffle, reduce only) or df.repartition(200) (shuffle, redistribute evenly).",
                "Delta Lake vs Parquet: Parquet = simple, immutable. Delta = ACID + time travel + schema enforcement."
            ]
        },
        "HardProblem": "Boss Problem (Databricks): You manage a data lake with 3TB of daily event data, stored as Parquet, partitioned by date. Problems: (1) 18 months of data \u2014 540 date partitions \u00d7 2,000 small files each = 1.08M files. Query times degraded from 2min to 45min. (2) GDPR requires deleting all data for specific user_ids within 30 days. (3) Late-arriving data lands 3-5 days after the event date, breaking partition assumptions. Design the migration plan to Delta Lake that fixes all three problems."
    },
    {
        "Week": 6,
        "Day": "Tuesday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Storage Internals",
        "SpecificTopic": "Compression",
        "ActionItem_Deliverable": "Research Splittability of GZIP vs Snappy",
        "LeetCodeProblem": "<strong>LC 271 \u2013 Encode & Decode Strings</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Recursive Parsing</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Row vs Columnar \u2014 The Layout Trade-Off</div><div class=\"rich\"><h4>The Fundamental Trade-Off in Storage Layout</h4><p>When you store data on disk, you have a fundamental choice: store all columns of one row together (row-oriented) or store all values of one column together (column-oriented). This decision affects compression ratios, query speed, write speed, and how much data you scan for any given query.</p><h4>Row-Oriented Storage \u2014 How It Works</h4><p>In a row-oriented format (CSV, MySQL, PostgreSQL by default), all columns of Row 1 are stored together on disk, then all columns of Row 2, etc.</p><pre>Disk layout for a 3-row, 4-column table:\n\nRow 1: [user_id=1][name=Alice][age=30][country=US]\nRow 2: [user_id=2][name=Bob][age=25][country=UK]\nRow 3: [user_id=3][name=Carol][age=28][country=US]\n\nQuery: SELECT country, SUM(age) GROUP BY country\nWhat disk must read: ALL of [user_id, name, age, country] for every row\nEven though user_id and name are not needed at all!\n\nFor 100 columns, analytical queries typically need 3-5.\nRow storage reads 100 columns to use 5 = 95% wasted I/O.</pre><h4>Column-Oriented Storage \u2014 How It Works</h4><p>In a column-oriented format (Parquet, ORC), all values of column 1 are stored together, then all of column 2, etc.</p><pre>Disk layout (same data, columnar):\n\nColumn user_id: [1][2][3]\nColumn name:    [Alice][Bob][Carol]\nColumn age:     [30][25][28]\nColumn country: [US][UK][US]\n\nQuery: SELECT country, SUM(age) GROUP BY country\nWhat disk must read: ONLY [age] and [country] column blocks\nuser_id and name are not read at all \u2014 they live in different disk blocks!\n\nFor a 100-column table querying 5 columns: reads 5% of the data.</pre><p>\u270d\ufe0f <strong>Rule:</strong> Row storage is optimal for OLTP (write one complete row, read one complete row). Column storage is optimal for OLAP (read few columns across all rows, aggregate).</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Compression, Formats, Parquet Internals</div><div class=\"rich\"><h4>Why Columnar Format Compresses So Well</h4><p>Compression works by finding patterns and encoding them more efficiently. Column storage makes compression dramatically more effective because <strong>values in the same column have the same type and similar values</strong>.</p><ul><li><strong>country column:</strong> [\"US\",\"UK\",\"US\",\"US\",\"CA\",\"US\",\"UK\"] \u2014 only a few unique values repeat. Dictionary encoding: US=1, UK=2, CA=3 \u2192 stores [1,2,1,1,3,1,2] as integers. 90%+ compression.</li><li><strong>age column:</strong> [30,25,28,31,22,25,28] \u2014 small integers, all similar range. Delta encoding: store [30, -5, +3, +3, -9, +3, +3]. Small deltas compress extremely well.</li><li><strong>timestamp column:</strong> [1704067200, 1704067800, 1704068400] \u2014 always increasing, constant difference. RLE encodes as: start=1704067200, step=600, count=3.</li></ul><table><tr><th>Format</th><td>Compression</td><td>Read speed</td><td>Write speed</td><td>Best for</td></tr><tr><th>CSV</th><td>None (text)</td><td>Slow (parse text)</td><td>Fast</td><td>Data exchange, debugging</td></tr><tr><th>JSON</th><td>None</td><td>Very slow (parse tree)</td><td>Medium</td><td>APIs, nested data</td></tr><tr><th>Avro</th><td>Medium</td><td>Medium (row-oriented)</td><td>Very fast</td><td>Kafka messages, write-heavy</td></tr><tr><th>Parquet</th><td>High (columnar)</td><td>Very fast for queries</td><td>Slower</td><td>Analytics, data warehouses</td></tr><tr><th>ORC</th><td>High (columnar)</td><td>Very fast (Hive/Spark)</td><td>Slower</td><td>Hive workloads, transactional</td></tr><tr><th>Delta Lake</th><td>High + ACID</td><td>Very fast + time-travel</td><td>Medium</td><td>Lakehouse: ACID + analytics</td></tr></table><h4>Parquet File Structure \u2014 What's Inside</h4><pre>Parquet file internals:\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  MAGIC BYTES: \"PAR1\"                        \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Row Group 1 (e.g., 128MB of rows)          \u2502\n\u2502    Column chunk: user_id   [encoded values] \u2502\n\u2502    Column chunk: age       [encoded values] \u2502\n\u2502    Column chunk: country   [encoded values] \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Row Group 2 (next 128MB of rows)           \u2502\n\u2502    Column chunks...                         \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Footer: schema + row group offsets         \u2502\n\u2502    min/max statistics per column per chunk  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n# Statistics in footer enable predicate pushdown:\n# WHERE country = \"US\" \u2192 check if \"US\" is within [min,max] of each row group\n# Skip entire row groups that cannot contain the value \u2192 zero bytes read!</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 S3 Partitioning + Small File Problem</div><div class=\"rich\"><h4>S3 Partitioning \u2014 Hive-Style Directory Layout</h4><p>When storing large datasets on S3 (or HDFS), the directory structure acts as a physical partition. Spark/Hive reads ONLY partition directories that match the query filter, completely skipping all other directories.</p><pre># Good partition structure (by year/month/day)\ns3://my-bucket/events/\n    year=2024/\n        month=01/\n            day=01/\n                part-00000.parquet\n                part-00001.parquet\n            day=02/\n                ...\n        month=02/...\n\n# Query: WHERE year=2024 AND month=01 AND day=01\n# Reads: ONLY s3://my-bucket/events/year=2024/month=01/day=01/\n# Skips: all other months and days entirely\n\n# Spark reads partition from path automatically:\ndf = spark.read.parquet(\"s3://my-bucket/events/\")\ndf.filter((df.year==2024) & (df.month==1)).show()\n# Spark sees partition filter \u2192 reads only matching directories</pre><h4>The Small File Problem \u2014 The Silent Performance Killer</h4><p>Data lakes accumulate thousands of tiny files (1-10MB each) over time, typically from: streaming jobs writing every minute, or daily partitions with many sub-partitions.</p><p>The problem: Parquet/HDFS has per-file overhead. 10,000 files of 5MB each reads SLOWER than 50 files of 1GB each \u2014 even though it's the same total data. Every file requires one filesystem API call, one file open, reading the footer, and metadata validation.</p><pre># Detect small file problem in Spark:\ndf.rdd.getNumPartitions()  # if this is > 10,000, you have too many small files\n\n# Fix: coalesce (narrow transform) or repartition (full shuffle)\ndf.coalesce(200).write.parquet(\"output/\")  # merge to 200 files \u2014 no shuffle\ndf.repartition(200).write.parquet(\"output/\")  # redistribute evenly \u2014 full shuffle\n\n# Rule: target file size 128MB\u20131GB. coalesce < 200 files is usually fine.</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Delta Lake \u2014 ACID on Data Lakes</div><div class=\"rich\"><h4>Delta Lake \u2014 ACID Transactions on a Data Lake</h4><p>Traditional data lakes (S3 + Parquet) have no transaction support: if you DELETE or UPDATE records, you have to rewrite entire Parquet files. If your job fails halfway through, you have partial data written \u2014 corrupted state. <strong>Delta Lake</strong> adds ACID transactions and time travel to any Parquet-based data lake.</p><pre># Delta Lake stores data as Parquet + a transaction log (_delta_log/)\ns3://my-bucket/orders/\n    _delta_log/\n        00000000000000000000.json  # commit 0: initial data\n        00000000000000000001.json  # commit 1: added rows\n        00000000000000000002.json  # commit 2: deleted rows\n    part-00000.parquet\n    part-00001.parquet\n\n# ACID operations:\nfrom delta.tables import DeltaTable\ndt = DeltaTable.forPath(spark, \"s3://my-bucket/orders/\")\n\n# DELETE with conditions (records are marked deleted in log, not physically removed)\ndt.delete(\"order_status = 'CANCELLED'\")\n\n# UPSERT (MERGE INTO): insert new rows, update existing ones\ndt.alias(\"target\").merge(\n    updates_df.alias(\"source\"),\n    \"target.order_id = source.order_id\"\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n\n# Time travel: query data as it was at a previous point\ndf_yesterday = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-01-01\").load(...)</pre><p><strong>FAANG interview:</strong> \"When would you choose Delta Lake over plain Parquet?\"</p><ul><li>You need to DELETE or UPDATE individual records (GDPR right-to-be-forgotten)</li><li>You need ACID guarantees: multiple writers, no partial writes</li><li>You need time travel: audit trail, debugging, rollbacks</li><li>You process late-arriving data that needs to be merged into historical partitions</li><li>Plain Parquet is better: write-once append-only workloads where simplicity is valued</li></ul></div></div></div>",
        "KeyConcepts": [
            "Row storage: all columns of one row together. Fast for full-row reads/writes (OLTP).",
            "Columnar storage: all values of one column together. Fast for analytical queries (scan few columns of many rows).",
            "Columnar compression: same-type values in a block compress 5\u201310x better than mixed row data.",
            "Parquet internals: row groups (128MB), column chunks within groups, statistics in footer.",
            "Footer statistics enable predicate pushdown: skip entire row groups that can't match WHERE condition.",
            "S3 partitioning: Hive-style year=/month=/day= directories. Spark reads only matching partitions.",
            "Small file problem: 10K 5MB files read slower than 50 1GB files. Target 128MB\u20131GB per file.",
            "Delta Lake: adds ACID + time travel to Parquet. Use when you need DELETE/UPDATE/upsert on a data lake."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create a 1M-row DataFrame. Write it as CSV and as Parquet. Compare file sizes on disk. Then query only 2 of 10 columns \u2014 compare read times.",
            "<strong>Step 2:</strong> Write the same DataFrame partitioned by year and month. Confirm Spark uses partition pruning by checking the query plan with .explain().",
            "<strong>Step 3:</strong> Simulate small file problem: write 1,000 small Parquet files. Then coalesce to 10 files. Compare read performance.",
            "<strong>Step 4:</strong> Try a Delta Lake MERGE (upsert): start with 1M orders, receive 10K updates and 5K new rows. Use .merge() to apply changes. Read back and verify."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Research Splittability of GZIP vs Snappy",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Research Splittability of GZIP vs Snappy \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: 'Why Parquet over CSV?' \u2192 columnar compression, predicate pushdown, schema enforcement.",
                "Footer statistics in Parquet: min/max per column chunk enable row group skipping \u2014 huge for filtered reads.",
                "Small file fix: df.coalesce(200) (no shuffle, reduce only) or df.repartition(200) (shuffle, redistribute evenly).",
                "Delta Lake vs Parquet: Parquet = simple, immutable. Delta = ACID + time travel + schema enforcement."
            ]
        },
        "HardProblem": "Boss Problem (Databricks): You manage a data lake with 3TB of daily event data, stored as Parquet, partitioned by date. Problems: (1) 18 months of data \u2014 540 date partitions \u00d7 2,000 small files each = 1.08M files. Query times degraded from 2min to 45min. (2) GDPR requires deleting all data for specific user_ids within 30 days. (3) Late-arriving data lands 3-5 days after the event date, breaking partition assumptions. Design the migration plan to Delta Lake that fixes all three problems."
    },
    {
        "Week": 6,
        "Day": "Wednesday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Storage Internals",
        "SpecificTopic": "S3 Partitioning",
        "ActionItem_Deliverable": "Calculate file count for 10 years of hourly data",
        "LeetCodeProblem": "<strong>LC 146 \u2013 LRU Cache</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Recursive CTEs</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Row vs Columnar \u2014 The Layout Trade-Off</div><div class=\"rich\"><h4>The Fundamental Trade-Off in Storage Layout</h4><p>When you store data on disk, you have a fundamental choice: store all columns of one row together (row-oriented) or store all values of one column together (column-oriented). This decision affects compression ratios, query speed, write speed, and how much data you scan for any given query.</p><h4>Row-Oriented Storage \u2014 How It Works</h4><p>In a row-oriented format (CSV, MySQL, PostgreSQL by default), all columns of Row 1 are stored together on disk, then all columns of Row 2, etc.</p><pre>Disk layout for a 3-row, 4-column table:\n\nRow 1: [user_id=1][name=Alice][age=30][country=US]\nRow 2: [user_id=2][name=Bob][age=25][country=UK]\nRow 3: [user_id=3][name=Carol][age=28][country=US]\n\nQuery: SELECT country, SUM(age) GROUP BY country\nWhat disk must read: ALL of [user_id, name, age, country] for every row\nEven though user_id and name are not needed at all!\n\nFor 100 columns, analytical queries typically need 3-5.\nRow storage reads 100 columns to use 5 = 95% wasted I/O.</pre><h4>Column-Oriented Storage \u2014 How It Works</h4><p>In a column-oriented format (Parquet, ORC), all values of column 1 are stored together, then all of column 2, etc.</p><pre>Disk layout (same data, columnar):\n\nColumn user_id: [1][2][3]\nColumn name:    [Alice][Bob][Carol]\nColumn age:     [30][25][28]\nColumn country: [US][UK][US]\n\nQuery: SELECT country, SUM(age) GROUP BY country\nWhat disk must read: ONLY [age] and [country] column blocks\nuser_id and name are not read at all \u2014 they live in different disk blocks!\n\nFor a 100-column table querying 5 columns: reads 5% of the data.</pre><p>\u270d\ufe0f <strong>Rule:</strong> Row storage is optimal for OLTP (write one complete row, read one complete row). Column storage is optimal for OLAP (read few columns across all rows, aggregate).</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Compression, Formats, Parquet Internals</div><div class=\"rich\"><h4>Why Columnar Format Compresses So Well</h4><p>Compression works by finding patterns and encoding them more efficiently. Column storage makes compression dramatically more effective because <strong>values in the same column have the same type and similar values</strong>.</p><ul><li><strong>country column:</strong> [\"US\",\"UK\",\"US\",\"US\",\"CA\",\"US\",\"UK\"] \u2014 only a few unique values repeat. Dictionary encoding: US=1, UK=2, CA=3 \u2192 stores [1,2,1,1,3,1,2] as integers. 90%+ compression.</li><li><strong>age column:</strong> [30,25,28,31,22,25,28] \u2014 small integers, all similar range. Delta encoding: store [30, -5, +3, +3, -9, +3, +3]. Small deltas compress extremely well.</li><li><strong>timestamp column:</strong> [1704067200, 1704067800, 1704068400] \u2014 always increasing, constant difference. RLE encodes as: start=1704067200, step=600, count=3.</li></ul><table><tr><th>Format</th><td>Compression</td><td>Read speed</td><td>Write speed</td><td>Best for</td></tr><tr><th>CSV</th><td>None (text)</td><td>Slow (parse text)</td><td>Fast</td><td>Data exchange, debugging</td></tr><tr><th>JSON</th><td>None</td><td>Very slow (parse tree)</td><td>Medium</td><td>APIs, nested data</td></tr><tr><th>Avro</th><td>Medium</td><td>Medium (row-oriented)</td><td>Very fast</td><td>Kafka messages, write-heavy</td></tr><tr><th>Parquet</th><td>High (columnar)</td><td>Very fast for queries</td><td>Slower</td><td>Analytics, data warehouses</td></tr><tr><th>ORC</th><td>High (columnar)</td><td>Very fast (Hive/Spark)</td><td>Slower</td><td>Hive workloads, transactional</td></tr><tr><th>Delta Lake</th><td>High + ACID</td><td>Very fast + time-travel</td><td>Medium</td><td>Lakehouse: ACID + analytics</td></tr></table><h4>Parquet File Structure \u2014 What's Inside</h4><pre>Parquet file internals:\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  MAGIC BYTES: \"PAR1\"                        \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Row Group 1 (e.g., 128MB of rows)          \u2502\n\u2502    Column chunk: user_id   [encoded values] \u2502\n\u2502    Column chunk: age       [encoded values] \u2502\n\u2502    Column chunk: country   [encoded values] \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Row Group 2 (next 128MB of rows)           \u2502\n\u2502    Column chunks...                         \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Footer: schema + row group offsets         \u2502\n\u2502    min/max statistics per column per chunk  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n# Statistics in footer enable predicate pushdown:\n# WHERE country = \"US\" \u2192 check if \"US\" is within [min,max] of each row group\n# Skip entire row groups that cannot contain the value \u2192 zero bytes read!</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 S3 Partitioning + Small File Problem</div><div class=\"rich\"><h4>S3 Partitioning \u2014 Hive-Style Directory Layout</h4><p>When storing large datasets on S3 (or HDFS), the directory structure acts as a physical partition. Spark/Hive reads ONLY partition directories that match the query filter, completely skipping all other directories.</p><pre># Good partition structure (by year/month/day)\ns3://my-bucket/events/\n    year=2024/\n        month=01/\n            day=01/\n                part-00000.parquet\n                part-00001.parquet\n            day=02/\n                ...\n        month=02/...\n\n# Query: WHERE year=2024 AND month=01 AND day=01\n# Reads: ONLY s3://my-bucket/events/year=2024/month=01/day=01/\n# Skips: all other months and days entirely\n\n# Spark reads partition from path automatically:\ndf = spark.read.parquet(\"s3://my-bucket/events/\")\ndf.filter((df.year==2024) & (df.month==1)).show()\n# Spark sees partition filter \u2192 reads only matching directories</pre><h4>The Small File Problem \u2014 The Silent Performance Killer</h4><p>Data lakes accumulate thousands of tiny files (1-10MB each) over time, typically from: streaming jobs writing every minute, or daily partitions with many sub-partitions.</p><p>The problem: Parquet/HDFS has per-file overhead. 10,000 files of 5MB each reads SLOWER than 50 files of 1GB each \u2014 even though it's the same total data. Every file requires one filesystem API call, one file open, reading the footer, and metadata validation.</p><pre># Detect small file problem in Spark:\ndf.rdd.getNumPartitions()  # if this is > 10,000, you have too many small files\n\n# Fix: coalesce (narrow transform) or repartition (full shuffle)\ndf.coalesce(200).write.parquet(\"output/\")  # merge to 200 files \u2014 no shuffle\ndf.repartition(200).write.parquet(\"output/\")  # redistribute evenly \u2014 full shuffle\n\n# Rule: target file size 128MB\u20131GB. coalesce < 200 files is usually fine.</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Delta Lake \u2014 ACID on Data Lakes</div><div class=\"rich\"><h4>Delta Lake \u2014 ACID Transactions on a Data Lake</h4><p>Traditional data lakes (S3 + Parquet) have no transaction support: if you DELETE or UPDATE records, you have to rewrite entire Parquet files. If your job fails halfway through, you have partial data written \u2014 corrupted state. <strong>Delta Lake</strong> adds ACID transactions and time travel to any Parquet-based data lake.</p><pre># Delta Lake stores data as Parquet + a transaction log (_delta_log/)\ns3://my-bucket/orders/\n    _delta_log/\n        00000000000000000000.json  # commit 0: initial data\n        00000000000000000001.json  # commit 1: added rows\n        00000000000000000002.json  # commit 2: deleted rows\n    part-00000.parquet\n    part-00001.parquet\n\n# ACID operations:\nfrom delta.tables import DeltaTable\ndt = DeltaTable.forPath(spark, \"s3://my-bucket/orders/\")\n\n# DELETE with conditions (records are marked deleted in log, not physically removed)\ndt.delete(\"order_status = 'CANCELLED'\")\n\n# UPSERT (MERGE INTO): insert new rows, update existing ones\ndt.alias(\"target\").merge(\n    updates_df.alias(\"source\"),\n    \"target.order_id = source.order_id\"\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n\n# Time travel: query data as it was at a previous point\ndf_yesterday = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-01-01\").load(...)</pre><p><strong>FAANG interview:</strong> \"When would you choose Delta Lake over plain Parquet?\"</p><ul><li>You need to DELETE or UPDATE individual records (GDPR right-to-be-forgotten)</li><li>You need ACID guarantees: multiple writers, no partial writes</li><li>You need time travel: audit trail, debugging, rollbacks</li><li>You process late-arriving data that needs to be merged into historical partitions</li><li>Plain Parquet is better: write-once append-only workloads where simplicity is valued</li></ul></div></div></div>",
        "KeyConcepts": [
            "Row storage: all columns of one row together. Fast for full-row reads/writes (OLTP).",
            "Columnar storage: all values of one column together. Fast for analytical queries (scan few columns of many rows).",
            "Columnar compression: same-type values in a block compress 5\u201310x better than mixed row data.",
            "Parquet internals: row groups (128MB), column chunks within groups, statistics in footer.",
            "Footer statistics enable predicate pushdown: skip entire row groups that can't match WHERE condition.",
            "S3 partitioning: Hive-style year=/month=/day= directories. Spark reads only matching partitions.",
            "Small file problem: 10K 5MB files read slower than 50 1GB files. Target 128MB\u20131GB per file.",
            "Delta Lake: adds ACID + time travel to Parquet. Use when you need DELETE/UPDATE/upsert on a data lake."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create a 1M-row DataFrame. Write it as CSV and as Parquet. Compare file sizes on disk. Then query only 2 of 10 columns \u2014 compare read times.",
            "<strong>Step 2:</strong> Write the same DataFrame partitioned by year and month. Confirm Spark uses partition pruning by checking the query plan with .explain().",
            "<strong>Step 3:</strong> Simulate small file problem: write 1,000 small Parquet files. Then coalesce to 10 files. Compare read performance.",
            "<strong>Step 4:</strong> Try a Delta Lake MERGE (upsert): start with 1M orders, receive 10K updates and 5K new rows. Use .merge() to apply changes. Read back and verify."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Calculate file count for 10 years of hourly data",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Calculate file count for 10 years of hourly data \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: 'Why Parquet over CSV?' \u2192 columnar compression, predicate pushdown, schema enforcement.",
                "Footer statistics in Parquet: min/max per column chunk enable row group skipping \u2014 huge for filtered reads.",
                "Small file fix: df.coalesce(200) (no shuffle, reduce only) or df.repartition(200) (shuffle, redistribute evenly).",
                "Delta Lake vs Parquet: Parquet = simple, immutable. Delta = ACID + time travel + schema enforcement."
            ]
        },
        "HardProblem": "Boss Problem (Databricks): You manage a data lake with 3TB of daily event data, stored as Parquet, partitioned by date. Problems: (1) 18 months of data \u2014 540 date partitions \u00d7 2,000 small files each = 1.08M files. Query times degraded from 2min to 45min. (2) GDPR requires deleting all data for specific user_ids within 30 days. (3) Late-arriving data lands 3-5 days after the event date, breaking partition assumptions. Design the migration plan to Delta Lake that fixes all three problems."
    },
    {
        "Week": 6,
        "Day": "Thursday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Storage Internals",
        "SpecificTopic": "Small File Problem",
        "ActionItem_Deliverable": "Write logic to compact small files in Spark",
        "LeetCodeProblem": "<strong>LC 295 \u2013 Find Median from Data Stream</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Joins Deep Dive</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Row vs Columnar \u2014 The Layout Trade-Off</div><div class=\"rich\"><h4>The Fundamental Trade-Off in Storage Layout</h4><p>When you store data on disk, you have a fundamental choice: store all columns of one row together (row-oriented) or store all values of one column together (column-oriented). This decision affects compression ratios, query speed, write speed, and how much data you scan for any given query.</p><h4>Row-Oriented Storage \u2014 How It Works</h4><p>In a row-oriented format (CSV, MySQL, PostgreSQL by default), all columns of Row 1 are stored together on disk, then all columns of Row 2, etc.</p><pre>Disk layout for a 3-row, 4-column table:\n\nRow 1: [user_id=1][name=Alice][age=30][country=US]\nRow 2: [user_id=2][name=Bob][age=25][country=UK]\nRow 3: [user_id=3][name=Carol][age=28][country=US]\n\nQuery: SELECT country, SUM(age) GROUP BY country\nWhat disk must read: ALL of [user_id, name, age, country] for every row\nEven though user_id and name are not needed at all!\n\nFor 100 columns, analytical queries typically need 3-5.\nRow storage reads 100 columns to use 5 = 95% wasted I/O.</pre><h4>Column-Oriented Storage \u2014 How It Works</h4><p>In a column-oriented format (Parquet, ORC), all values of column 1 are stored together, then all of column 2, etc.</p><pre>Disk layout (same data, columnar):\n\nColumn user_id: [1][2][3]\nColumn name:    [Alice][Bob][Carol]\nColumn age:     [30][25][28]\nColumn country: [US][UK][US]\n\nQuery: SELECT country, SUM(age) GROUP BY country\nWhat disk must read: ONLY [age] and [country] column blocks\nuser_id and name are not read at all \u2014 they live in different disk blocks!\n\nFor a 100-column table querying 5 columns: reads 5% of the data.</pre><p>\u270d\ufe0f <strong>Rule:</strong> Row storage is optimal for OLTP (write one complete row, read one complete row). Column storage is optimal for OLAP (read few columns across all rows, aggregate).</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Compression, Formats, Parquet Internals</div><div class=\"rich\"><h4>Why Columnar Format Compresses So Well</h4><p>Compression works by finding patterns and encoding them more efficiently. Column storage makes compression dramatically more effective because <strong>values in the same column have the same type and similar values</strong>.</p><ul><li><strong>country column:</strong> [\"US\",\"UK\",\"US\",\"US\",\"CA\",\"US\",\"UK\"] \u2014 only a few unique values repeat. Dictionary encoding: US=1, UK=2, CA=3 \u2192 stores [1,2,1,1,3,1,2] as integers. 90%+ compression.</li><li><strong>age column:</strong> [30,25,28,31,22,25,28] \u2014 small integers, all similar range. Delta encoding: store [30, -5, +3, +3, -9, +3, +3]. Small deltas compress extremely well.</li><li><strong>timestamp column:</strong> [1704067200, 1704067800, 1704068400] \u2014 always increasing, constant difference. RLE encodes as: start=1704067200, step=600, count=3.</li></ul><table><tr><th>Format</th><td>Compression</td><td>Read speed</td><td>Write speed</td><td>Best for</td></tr><tr><th>CSV</th><td>None (text)</td><td>Slow (parse text)</td><td>Fast</td><td>Data exchange, debugging</td></tr><tr><th>JSON</th><td>None</td><td>Very slow (parse tree)</td><td>Medium</td><td>APIs, nested data</td></tr><tr><th>Avro</th><td>Medium</td><td>Medium (row-oriented)</td><td>Very fast</td><td>Kafka messages, write-heavy</td></tr><tr><th>Parquet</th><td>High (columnar)</td><td>Very fast for queries</td><td>Slower</td><td>Analytics, data warehouses</td></tr><tr><th>ORC</th><td>High (columnar)</td><td>Very fast (Hive/Spark)</td><td>Slower</td><td>Hive workloads, transactional</td></tr><tr><th>Delta Lake</th><td>High + ACID</td><td>Very fast + time-travel</td><td>Medium</td><td>Lakehouse: ACID + analytics</td></tr></table><h4>Parquet File Structure \u2014 What's Inside</h4><pre>Parquet file internals:\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  MAGIC BYTES: \"PAR1\"                        \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Row Group 1 (e.g., 128MB of rows)          \u2502\n\u2502    Column chunk: user_id   [encoded values] \u2502\n\u2502    Column chunk: age       [encoded values] \u2502\n\u2502    Column chunk: country   [encoded values] \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Row Group 2 (next 128MB of rows)           \u2502\n\u2502    Column chunks...                         \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Footer: schema + row group offsets         \u2502\n\u2502    min/max statistics per column per chunk  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n# Statistics in footer enable predicate pushdown:\n# WHERE country = \"US\" \u2192 check if \"US\" is within [min,max] of each row group\n# Skip entire row groups that cannot contain the value \u2192 zero bytes read!</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 S3 Partitioning + Small File Problem</div><div class=\"rich\"><h4>S3 Partitioning \u2014 Hive-Style Directory Layout</h4><p>When storing large datasets on S3 (or HDFS), the directory structure acts as a physical partition. Spark/Hive reads ONLY partition directories that match the query filter, completely skipping all other directories.</p><pre># Good partition structure (by year/month/day)\ns3://my-bucket/events/\n    year=2024/\n        month=01/\n            day=01/\n                part-00000.parquet\n                part-00001.parquet\n            day=02/\n                ...\n        month=02/...\n\n# Query: WHERE year=2024 AND month=01 AND day=01\n# Reads: ONLY s3://my-bucket/events/year=2024/month=01/day=01/\n# Skips: all other months and days entirely\n\n# Spark reads partition from path automatically:\ndf = spark.read.parquet(\"s3://my-bucket/events/\")\ndf.filter((df.year==2024) & (df.month==1)).show()\n# Spark sees partition filter \u2192 reads only matching directories</pre><h4>The Small File Problem \u2014 The Silent Performance Killer</h4><p>Data lakes accumulate thousands of tiny files (1-10MB each) over time, typically from: streaming jobs writing every minute, or daily partitions with many sub-partitions.</p><p>The problem: Parquet/HDFS has per-file overhead. 10,000 files of 5MB each reads SLOWER than 50 files of 1GB each \u2014 even though it's the same total data. Every file requires one filesystem API call, one file open, reading the footer, and metadata validation.</p><pre># Detect small file problem in Spark:\ndf.rdd.getNumPartitions()  # if this is > 10,000, you have too many small files\n\n# Fix: coalesce (narrow transform) or repartition (full shuffle)\ndf.coalesce(200).write.parquet(\"output/\")  # merge to 200 files \u2014 no shuffle\ndf.repartition(200).write.parquet(\"output/\")  # redistribute evenly \u2014 full shuffle\n\n# Rule: target file size 128MB\u20131GB. coalesce < 200 files is usually fine.</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Delta Lake \u2014 ACID on Data Lakes</div><div class=\"rich\"><h4>Delta Lake \u2014 ACID Transactions on a Data Lake</h4><p>Traditional data lakes (S3 + Parquet) have no transaction support: if you DELETE or UPDATE records, you have to rewrite entire Parquet files. If your job fails halfway through, you have partial data written \u2014 corrupted state. <strong>Delta Lake</strong> adds ACID transactions and time travel to any Parquet-based data lake.</p><pre># Delta Lake stores data as Parquet + a transaction log (_delta_log/)\ns3://my-bucket/orders/\n    _delta_log/\n        00000000000000000000.json  # commit 0: initial data\n        00000000000000000001.json  # commit 1: added rows\n        00000000000000000002.json  # commit 2: deleted rows\n    part-00000.parquet\n    part-00001.parquet\n\n# ACID operations:\nfrom delta.tables import DeltaTable\ndt = DeltaTable.forPath(spark, \"s3://my-bucket/orders/\")\n\n# DELETE with conditions (records are marked deleted in log, not physically removed)\ndt.delete(\"order_status = 'CANCELLED'\")\n\n# UPSERT (MERGE INTO): insert new rows, update existing ones\ndt.alias(\"target\").merge(\n    updates_df.alias(\"source\"),\n    \"target.order_id = source.order_id\"\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n\n# Time travel: query data as it was at a previous point\ndf_yesterday = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-01-01\").load(...)</pre><p><strong>FAANG interview:</strong> \"When would you choose Delta Lake over plain Parquet?\"</p><ul><li>You need to DELETE or UPDATE individual records (GDPR right-to-be-forgotten)</li><li>You need ACID guarantees: multiple writers, no partial writes</li><li>You need time travel: audit trail, debugging, rollbacks</li><li>You process late-arriving data that needs to be merged into historical partitions</li><li>Plain Parquet is better: write-once append-only workloads where simplicity is valued</li></ul></div></div></div>",
        "KeyConcepts": [
            "Row storage: all columns of one row together. Fast for full-row reads/writes (OLTP).",
            "Columnar storage: all values of one column together. Fast for analytical queries (scan few columns of many rows).",
            "Columnar compression: same-type values in a block compress 5\u201310x better than mixed row data.",
            "Parquet internals: row groups (128MB), column chunks within groups, statistics in footer.",
            "Footer statistics enable predicate pushdown: skip entire row groups that can't match WHERE condition.",
            "S3 partitioning: Hive-style year=/month=/day= directories. Spark reads only matching partitions.",
            "Small file problem: 10K 5MB files read slower than 50 1GB files. Target 128MB\u20131GB per file.",
            "Delta Lake: adds ACID + time travel to Parquet. Use when you need DELETE/UPDATE/upsert on a data lake."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create a 1M-row DataFrame. Write it as CSV and as Parquet. Compare file sizes on disk. Then query only 2 of 10 columns \u2014 compare read times.",
            "<strong>Step 2:</strong> Write the same DataFrame partitioned by year and month. Confirm Spark uses partition pruning by checking the query plan with .explain().",
            "<strong>Step 3:</strong> Simulate small file problem: write 1,000 small Parquet files. Then coalesce to 10 files. Compare read performance.",
            "<strong>Step 4:</strong> Try a Delta Lake MERGE (upsert): start with 1M orders, receive 10K updates and 5K new rows. Use .merge() to apply changes. Read back and verify."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Write logic to compact small files in Spark",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Write logic to compact small files in Spark \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: 'Why Parquet over CSV?' \u2192 columnar compression, predicate pushdown, schema enforcement.",
                "Footer statistics in Parquet: min/max per column chunk enable row group skipping \u2014 huge for filtered reads.",
                "Small file fix: df.coalesce(200) (no shuffle, reduce only) or df.repartition(200) (shuffle, redistribute evenly).",
                "Delta Lake vs Parquet: Parquet = simple, immutable. Delta = ACID + time travel + schema enforcement."
            ]
        },
        "HardProblem": "Boss Problem (Databricks): You manage a data lake with 3TB of daily event data, stored as Parquet, partitioned by date. Problems: (1) 18 months of data \u2014 540 date partitions \u00d7 2,000 small files each = 1.08M files. Query times degraded from 2min to 45min. (2) GDPR requires deleting all data for specific user_ids within 30 days. (3) Late-arriving data lands 3-5 days after the event date, breaking partition assumptions. Design the migration plan to Delta Lake that fixes all three problems."
    },
    {
        "Week": 6,
        "Day": "Friday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Storage Internals",
        "SpecificTopic": "Data Formats",
        "ActionItem_Deliverable": "Explain Avro (Kafka) vs Parquet (Lake) use cases",
        "LeetCodeProblem": "<strong>LC 981 \u2013 Time Based Key-Value Store</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Mock Assessment + Mock #1</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Row vs Columnar \u2014 The Layout Trade-Off</div><div class=\"rich\"><h4>The Fundamental Trade-Off in Storage Layout</h4><p>When you store data on disk, you have a fundamental choice: store all columns of one row together (row-oriented) or store all values of one column together (column-oriented). This decision affects compression ratios, query speed, write speed, and how much data you scan for any given query.</p><h4>Row-Oriented Storage \u2014 How It Works</h4><p>In a row-oriented format (CSV, MySQL, PostgreSQL by default), all columns of Row 1 are stored together on disk, then all columns of Row 2, etc.</p><pre>Disk layout for a 3-row, 4-column table:\n\nRow 1: [user_id=1][name=Alice][age=30][country=US]\nRow 2: [user_id=2][name=Bob][age=25][country=UK]\nRow 3: [user_id=3][name=Carol][age=28][country=US]\n\nQuery: SELECT country, SUM(age) GROUP BY country\nWhat disk must read: ALL of [user_id, name, age, country] for every row\nEven though user_id and name are not needed at all!\n\nFor 100 columns, analytical queries typically need 3-5.\nRow storage reads 100 columns to use 5 = 95% wasted I/O.</pre><h4>Column-Oriented Storage \u2014 How It Works</h4><p>In a column-oriented format (Parquet, ORC), all values of column 1 are stored together, then all of column 2, etc.</p><pre>Disk layout (same data, columnar):\n\nColumn user_id: [1][2][3]\nColumn name:    [Alice][Bob][Carol]\nColumn age:     [30][25][28]\nColumn country: [US][UK][US]\n\nQuery: SELECT country, SUM(age) GROUP BY country\nWhat disk must read: ONLY [age] and [country] column blocks\nuser_id and name are not read at all \u2014 they live in different disk blocks!\n\nFor a 100-column table querying 5 columns: reads 5% of the data.</pre><p>\u270d\ufe0f <strong>Rule:</strong> Row storage is optimal for OLTP (write one complete row, read one complete row). Column storage is optimal for OLAP (read few columns across all rows, aggregate).</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Compression, Formats, Parquet Internals</div><div class=\"rich\"><h4>Why Columnar Format Compresses So Well</h4><p>Compression works by finding patterns and encoding them more efficiently. Column storage makes compression dramatically more effective because <strong>values in the same column have the same type and similar values</strong>.</p><ul><li><strong>country column:</strong> [\"US\",\"UK\",\"US\",\"US\",\"CA\",\"US\",\"UK\"] \u2014 only a few unique values repeat. Dictionary encoding: US=1, UK=2, CA=3 \u2192 stores [1,2,1,1,3,1,2] as integers. 90%+ compression.</li><li><strong>age column:</strong> [30,25,28,31,22,25,28] \u2014 small integers, all similar range. Delta encoding: store [30, -5, +3, +3, -9, +3, +3]. Small deltas compress extremely well.</li><li><strong>timestamp column:</strong> [1704067200, 1704067800, 1704068400] \u2014 always increasing, constant difference. RLE encodes as: start=1704067200, step=600, count=3.</li></ul><table><tr><th>Format</th><td>Compression</td><td>Read speed</td><td>Write speed</td><td>Best for</td></tr><tr><th>CSV</th><td>None (text)</td><td>Slow (parse text)</td><td>Fast</td><td>Data exchange, debugging</td></tr><tr><th>JSON</th><td>None</td><td>Very slow (parse tree)</td><td>Medium</td><td>APIs, nested data</td></tr><tr><th>Avro</th><td>Medium</td><td>Medium (row-oriented)</td><td>Very fast</td><td>Kafka messages, write-heavy</td></tr><tr><th>Parquet</th><td>High (columnar)</td><td>Very fast for queries</td><td>Slower</td><td>Analytics, data warehouses</td></tr><tr><th>ORC</th><td>High (columnar)</td><td>Very fast (Hive/Spark)</td><td>Slower</td><td>Hive workloads, transactional</td></tr><tr><th>Delta Lake</th><td>High + ACID</td><td>Very fast + time-travel</td><td>Medium</td><td>Lakehouse: ACID + analytics</td></tr></table><h4>Parquet File Structure \u2014 What's Inside</h4><pre>Parquet file internals:\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  MAGIC BYTES: \"PAR1\"                        \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Row Group 1 (e.g., 128MB of rows)          \u2502\n\u2502    Column chunk: user_id   [encoded values] \u2502\n\u2502    Column chunk: age       [encoded values] \u2502\n\u2502    Column chunk: country   [encoded values] \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Row Group 2 (next 128MB of rows)           \u2502\n\u2502    Column chunks...                         \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Footer: schema + row group offsets         \u2502\n\u2502    min/max statistics per column per chunk  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n# Statistics in footer enable predicate pushdown:\n# WHERE country = \"US\" \u2192 check if \"US\" is within [min,max] of each row group\n# Skip entire row groups that cannot contain the value \u2192 zero bytes read!</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 S3 Partitioning + Small File Problem</div><div class=\"rich\"><h4>S3 Partitioning \u2014 Hive-Style Directory Layout</h4><p>When storing large datasets on S3 (or HDFS), the directory structure acts as a physical partition. Spark/Hive reads ONLY partition directories that match the query filter, completely skipping all other directories.</p><pre># Good partition structure (by year/month/day)\ns3://my-bucket/events/\n    year=2024/\n        month=01/\n            day=01/\n                part-00000.parquet\n                part-00001.parquet\n            day=02/\n                ...\n        month=02/...\n\n# Query: WHERE year=2024 AND month=01 AND day=01\n# Reads: ONLY s3://my-bucket/events/year=2024/month=01/day=01/\n# Skips: all other months and days entirely\n\n# Spark reads partition from path automatically:\ndf = spark.read.parquet(\"s3://my-bucket/events/\")\ndf.filter((df.year==2024) & (df.month==1)).show()\n# Spark sees partition filter \u2192 reads only matching directories</pre><h4>The Small File Problem \u2014 The Silent Performance Killer</h4><p>Data lakes accumulate thousands of tiny files (1-10MB each) over time, typically from: streaming jobs writing every minute, or daily partitions with many sub-partitions.</p><p>The problem: Parquet/HDFS has per-file overhead. 10,000 files of 5MB each reads SLOWER than 50 files of 1GB each \u2014 even though it's the same total data. Every file requires one filesystem API call, one file open, reading the footer, and metadata validation.</p><pre># Detect small file problem in Spark:\ndf.rdd.getNumPartitions()  # if this is > 10,000, you have too many small files\n\n# Fix: coalesce (narrow transform) or repartition (full shuffle)\ndf.coalesce(200).write.parquet(\"output/\")  # merge to 200 files \u2014 no shuffle\ndf.repartition(200).write.parquet(\"output/\")  # redistribute evenly \u2014 full shuffle\n\n# Rule: target file size 128MB\u20131GB. coalesce < 200 files is usually fine.</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Delta Lake \u2014 ACID on Data Lakes</div><div class=\"rich\"><h4>Delta Lake \u2014 ACID Transactions on a Data Lake</h4><p>Traditional data lakes (S3 + Parquet) have no transaction support: if you DELETE or UPDATE records, you have to rewrite entire Parquet files. If your job fails halfway through, you have partial data written \u2014 corrupted state. <strong>Delta Lake</strong> adds ACID transactions and time travel to any Parquet-based data lake.</p><pre># Delta Lake stores data as Parquet + a transaction log (_delta_log/)\ns3://my-bucket/orders/\n    _delta_log/\n        00000000000000000000.json  # commit 0: initial data\n        00000000000000000001.json  # commit 1: added rows\n        00000000000000000002.json  # commit 2: deleted rows\n    part-00000.parquet\n    part-00001.parquet\n\n# ACID operations:\nfrom delta.tables import DeltaTable\ndt = DeltaTable.forPath(spark, \"s3://my-bucket/orders/\")\n\n# DELETE with conditions (records are marked deleted in log, not physically removed)\ndt.delete(\"order_status = 'CANCELLED'\")\n\n# UPSERT (MERGE INTO): insert new rows, update existing ones\ndt.alias(\"target\").merge(\n    updates_df.alias(\"source\"),\n    \"target.order_id = source.order_id\"\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n\n# Time travel: query data as it was at a previous point\ndf_yesterday = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-01-01\").load(...)</pre><p><strong>FAANG interview:</strong> \"When would you choose Delta Lake over plain Parquet?\"</p><ul><li>You need to DELETE or UPDATE individual records (GDPR right-to-be-forgotten)</li><li>You need ACID guarantees: multiple writers, no partial writes</li><li>You need time travel: audit trail, debugging, rollbacks</li><li>You process late-arriving data that needs to be merged into historical partitions</li><li>Plain Parquet is better: write-once append-only workloads where simplicity is valued</li></ul></div></div></div>",
        "KeyConcepts": [
            "Row storage: all columns of one row together. Fast for full-row reads/writes (OLTP).",
            "Columnar storage: all values of one column together. Fast for analytical queries (scan few columns of many rows).",
            "Columnar compression: same-type values in a block compress 5\u201310x better than mixed row data.",
            "Parquet internals: row groups (128MB), column chunks within groups, statistics in footer.",
            "Footer statistics enable predicate pushdown: skip entire row groups that can't match WHERE condition.",
            "S3 partitioning: Hive-style year=/month=/day= directories. Spark reads only matching partitions.",
            "Small file problem: 10K 5MB files read slower than 50 1GB files. Target 128MB\u20131GB per file.",
            "Delta Lake: adds ACID + time travel to Parquet. Use when you need DELETE/UPDATE/upsert on a data lake."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create a 1M-row DataFrame. Write it as CSV and as Parquet. Compare file sizes on disk. Then query only 2 of 10 columns \u2014 compare read times.",
            "<strong>Step 2:</strong> Write the same DataFrame partitioned by year and month. Confirm Spark uses partition pruning by checking the query plan with .explain().",
            "<strong>Step 3:</strong> Simulate small file problem: write 1,000 small Parquet files. Then coalesce to 10 files. Compare read performance.",
            "<strong>Step 4:</strong> Try a Delta Lake MERGE (upsert): start with 1M orders, receive 10K updates and 5K new rows. Use .merge() to apply changes. Read back and verify."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Explain Avro (Kafka) vs Parquet (Lake) use cases",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Explain Avro (Kafka) vs Parquet (Lake) use cases \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: 'Why Parquet over CSV?' \u2192 columnar compression, predicate pushdown, schema enforcement.",
                "Footer statistics in Parquet: min/max per column chunk enable row group skipping \u2014 huge for filtered reads.",
                "Small file fix: df.coalesce(200) (no shuffle, reduce only) or df.repartition(200) (shuffle, redistribute evenly).",
                "Delta Lake vs Parquet: Parquet = simple, immutable. Delta = ACID + time travel + schema enforcement."
            ]
        },
        "HardProblem": "Boss Problem (Databricks): You manage a data lake with 3TB of daily event data, stored as Parquet, partitioned by date. Problems: (1) 18 months of data \u2014 540 date partitions \u00d7 2,000 small files each = 1.08M files. Query times degraded from 2min to 45min. (2) GDPR requires deleting all data for specific user_ids within 30 days. (3) Late-arriving data lands 3-5 days after the event date, breaking partition assumptions. Design the migration plan to Delta Lake that fixes all three problems."
    },
    {
        "Week": 6,
        "Day": "Saturday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Storage Internals",
        "SpecificTopic": "Delta Lake / Iceberg",
        "ActionItem_Deliverable": "Read Delta Lake vs Iceberg internals; draw architecture",
        "LeetCodeProblem": "<strong>LC 380 \u2013 Insert Delete GetRandom O(1)</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": "NEW: Replaces Netflix talk -- OSS table formats",
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Sliding Window</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Row vs Columnar \u2014 The Layout Trade-Off</div><div class=\"rich\"><h4>The Fundamental Trade-Off in Storage Layout</h4><p>When you store data on disk, you have a fundamental choice: store all columns of one row together (row-oriented) or store all values of one column together (column-oriented). This decision affects compression ratios, query speed, write speed, and how much data you scan for any given query.</p><h4>Row-Oriented Storage \u2014 How It Works</h4><p>In a row-oriented format (CSV, MySQL, PostgreSQL by default), all columns of Row 1 are stored together on disk, then all columns of Row 2, etc.</p><pre>Disk layout for a 3-row, 4-column table:\n\nRow 1: [user_id=1][name=Alice][age=30][country=US]\nRow 2: [user_id=2][name=Bob][age=25][country=UK]\nRow 3: [user_id=3][name=Carol][age=28][country=US]\n\nQuery: SELECT country, SUM(age) GROUP BY country\nWhat disk must read: ALL of [user_id, name, age, country] for every row\nEven though user_id and name are not needed at all!\n\nFor 100 columns, analytical queries typically need 3-5.\nRow storage reads 100 columns to use 5 = 95% wasted I/O.</pre><h4>Column-Oriented Storage \u2014 How It Works</h4><p>In a column-oriented format (Parquet, ORC), all values of column 1 are stored together, then all of column 2, etc.</p><pre>Disk layout (same data, columnar):\n\nColumn user_id: [1][2][3]\nColumn name:    [Alice][Bob][Carol]\nColumn age:     [30][25][28]\nColumn country: [US][UK][US]\n\nQuery: SELECT country, SUM(age) GROUP BY country\nWhat disk must read: ONLY [age] and [country] column blocks\nuser_id and name are not read at all \u2014 they live in different disk blocks!\n\nFor a 100-column table querying 5 columns: reads 5% of the data.</pre><p>\u270d\ufe0f <strong>Rule:</strong> Row storage is optimal for OLTP (write one complete row, read one complete row). Column storage is optimal for OLAP (read few columns across all rows, aggregate).</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Compression, Formats, Parquet Internals</div><div class=\"rich\"><h4>Why Columnar Format Compresses So Well</h4><p>Compression works by finding patterns and encoding them more efficiently. Column storage makes compression dramatically more effective because <strong>values in the same column have the same type and similar values</strong>.</p><ul><li><strong>country column:</strong> [\"US\",\"UK\",\"US\",\"US\",\"CA\",\"US\",\"UK\"] \u2014 only a few unique values repeat. Dictionary encoding: US=1, UK=2, CA=3 \u2192 stores [1,2,1,1,3,1,2] as integers. 90%+ compression.</li><li><strong>age column:</strong> [30,25,28,31,22,25,28] \u2014 small integers, all similar range. Delta encoding: store [30, -5, +3, +3, -9, +3, +3]. Small deltas compress extremely well.</li><li><strong>timestamp column:</strong> [1704067200, 1704067800, 1704068400] \u2014 always increasing, constant difference. RLE encodes as: start=1704067200, step=600, count=3.</li></ul><table><tr><th>Format</th><td>Compression</td><td>Read speed</td><td>Write speed</td><td>Best for</td></tr><tr><th>CSV</th><td>None (text)</td><td>Slow (parse text)</td><td>Fast</td><td>Data exchange, debugging</td></tr><tr><th>JSON</th><td>None</td><td>Very slow (parse tree)</td><td>Medium</td><td>APIs, nested data</td></tr><tr><th>Avro</th><td>Medium</td><td>Medium (row-oriented)</td><td>Very fast</td><td>Kafka messages, write-heavy</td></tr><tr><th>Parquet</th><td>High (columnar)</td><td>Very fast for queries</td><td>Slower</td><td>Analytics, data warehouses</td></tr><tr><th>ORC</th><td>High (columnar)</td><td>Very fast (Hive/Spark)</td><td>Slower</td><td>Hive workloads, transactional</td></tr><tr><th>Delta Lake</th><td>High + ACID</td><td>Very fast + time-travel</td><td>Medium</td><td>Lakehouse: ACID + analytics</td></tr></table><h4>Parquet File Structure \u2014 What's Inside</h4><pre>Parquet file internals:\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  MAGIC BYTES: \"PAR1\"                        \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Row Group 1 (e.g., 128MB of rows)          \u2502\n\u2502    Column chunk: user_id   [encoded values] \u2502\n\u2502    Column chunk: age       [encoded values] \u2502\n\u2502    Column chunk: country   [encoded values] \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Row Group 2 (next 128MB of rows)           \u2502\n\u2502    Column chunks...                         \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Footer: schema + row group offsets         \u2502\n\u2502    min/max statistics per column per chunk  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n# Statistics in footer enable predicate pushdown:\n# WHERE country = \"US\" \u2192 check if \"US\" is within [min,max] of each row group\n# Skip entire row groups that cannot contain the value \u2192 zero bytes read!</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 S3 Partitioning + Small File Problem</div><div class=\"rich\"><h4>S3 Partitioning \u2014 Hive-Style Directory Layout</h4><p>When storing large datasets on S3 (or HDFS), the directory structure acts as a physical partition. Spark/Hive reads ONLY partition directories that match the query filter, completely skipping all other directories.</p><pre># Good partition structure (by year/month/day)\ns3://my-bucket/events/\n    year=2024/\n        month=01/\n            day=01/\n                part-00000.parquet\n                part-00001.parquet\n            day=02/\n                ...\n        month=02/...\n\n# Query: WHERE year=2024 AND month=01 AND day=01\n# Reads: ONLY s3://my-bucket/events/year=2024/month=01/day=01/\n# Skips: all other months and days entirely\n\n# Spark reads partition from path automatically:\ndf = spark.read.parquet(\"s3://my-bucket/events/\")\ndf.filter((df.year==2024) & (df.month==1)).show()\n# Spark sees partition filter \u2192 reads only matching directories</pre><h4>The Small File Problem \u2014 The Silent Performance Killer</h4><p>Data lakes accumulate thousands of tiny files (1-10MB each) over time, typically from: streaming jobs writing every minute, or daily partitions with many sub-partitions.</p><p>The problem: Parquet/HDFS has per-file overhead. 10,000 files of 5MB each reads SLOWER than 50 files of 1GB each \u2014 even though it's the same total data. Every file requires one filesystem API call, one file open, reading the footer, and metadata validation.</p><pre># Detect small file problem in Spark:\ndf.rdd.getNumPartitions()  # if this is > 10,000, you have too many small files\n\n# Fix: coalesce (narrow transform) or repartition (full shuffle)\ndf.coalesce(200).write.parquet(\"output/\")  # merge to 200 files \u2014 no shuffle\ndf.repartition(200).write.parquet(\"output/\")  # redistribute evenly \u2014 full shuffle\n\n# Rule: target file size 128MB\u20131GB. coalesce < 200 files is usually fine.</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Delta Lake \u2014 ACID on Data Lakes</div><div class=\"rich\"><h4>Delta Lake \u2014 ACID Transactions on a Data Lake</h4><p>Traditional data lakes (S3 + Parquet) have no transaction support: if you DELETE or UPDATE records, you have to rewrite entire Parquet files. If your job fails halfway through, you have partial data written \u2014 corrupted state. <strong>Delta Lake</strong> adds ACID transactions and time travel to any Parquet-based data lake.</p><pre># Delta Lake stores data as Parquet + a transaction log (_delta_log/)\ns3://my-bucket/orders/\n    _delta_log/\n        00000000000000000000.json  # commit 0: initial data\n        00000000000000000001.json  # commit 1: added rows\n        00000000000000000002.json  # commit 2: deleted rows\n    part-00000.parquet\n    part-00001.parquet\n\n# ACID operations:\nfrom delta.tables import DeltaTable\ndt = DeltaTable.forPath(spark, \"s3://my-bucket/orders/\")\n\n# DELETE with conditions (records are marked deleted in log, not physically removed)\ndt.delete(\"order_status = 'CANCELLED'\")\n\n# UPSERT (MERGE INTO): insert new rows, update existing ones\ndt.alias(\"target\").merge(\n    updates_df.alias(\"source\"),\n    \"target.order_id = source.order_id\"\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n\n# Time travel: query data as it was at a previous point\ndf_yesterday = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-01-01\").load(...)</pre><p><strong>FAANG interview:</strong> \"When would you choose Delta Lake over plain Parquet?\"</p><ul><li>You need to DELETE or UPDATE individual records (GDPR right-to-be-forgotten)</li><li>You need ACID guarantees: multiple writers, no partial writes</li><li>You need time travel: audit trail, debugging, rollbacks</li><li>You process late-arriving data that needs to be merged into historical partitions</li><li>Plain Parquet is better: write-once append-only workloads where simplicity is valued</li></ul></div></div></div>",
        "KeyConcepts": [
            "Row storage: all columns of one row together. Fast for full-row reads/writes (OLTP).",
            "Columnar storage: all values of one column together. Fast for analytical queries (scan few columns of many rows).",
            "Columnar compression: same-type values in a block compress 5\u201310x better than mixed row data.",
            "Parquet internals: row groups (128MB), column chunks within groups, statistics in footer.",
            "Footer statistics enable predicate pushdown: skip entire row groups that can't match WHERE condition.",
            "S3 partitioning: Hive-style year=/month=/day= directories. Spark reads only matching partitions.",
            "Small file problem: 10K 5MB files read slower than 50 1GB files. Target 128MB\u20131GB per file.",
            "Delta Lake: adds ACID + time travel to Parquet. Use when you need DELETE/UPDATE/upsert on a data lake."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create a 1M-row DataFrame. Write it as CSV and as Parquet. Compare file sizes on disk. Then query only 2 of 10 columns \u2014 compare read times.",
            "<strong>Step 2:</strong> Write the same DataFrame partitioned by year and month. Confirm Spark uses partition pruning by checking the query plan with .explain().",
            "<strong>Step 3:</strong> Simulate small file problem: write 1,000 small Parquet files. Then coalesce to 10 files. Compare read performance.",
            "<strong>Step 4:</strong> Try a Delta Lake MERGE (upsert): start with 1M orders, receive 10K updates and 5K new rows. Use .merge() to apply changes. Read back and verify."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Read Delta Lake vs Iceberg internals; draw architecture",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Read Delta Lake vs Iceberg internals; draw architecture \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: 'Why Parquet over CSV?' \u2192 columnar compression, predicate pushdown, schema enforcement.",
                "Footer statistics in Parquet: min/max per column chunk enable row group skipping \u2014 huge for filtered reads.",
                "Small file fix: df.coalesce(200) (no shuffle, reduce only) or df.repartition(200) (shuffle, redistribute evenly).",
                "Delta Lake vs Parquet: Parquet = simple, immutable. Delta = ACID + time travel + schema enforcement."
            ]
        },
        "HardProblem": "Boss Problem (Databricks): You manage a data lake with 3TB of daily event data, stored as Parquet, partitioned by date. Problems: (1) 18 months of data \u2014 540 date partitions \u00d7 2,000 small files each = 1.08M files. Query times degraded from 2min to 45min. (2) GDPR requires deleting all data for specific user_ids within 30 days. (3) Late-arriving data lands 3-5 days after the event date, breaking partition assumptions. Design the migration plan to Delta Lake that fixes all three problems."
    },
    {
        "Week": 6,
        "Day": "Sunday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Rest",
        "SpecificTopic": "Rest & Review",
        "ActionItem_Deliverable": "Review Storage Classes in S3",
        "LeetCodeProblem": null,
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>S3 Partitioning</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Active Rest \u2014 How Memory Consolidates</div>\n<div class=\"rich\">\n<h4>The Neuroscience of Spaced Repetition</h4>\n<p>Rest days are not wasted days. During rest, your brain replays what it learned \u2014 a process called <strong>memory consolidation</strong>. Research consistently shows that <em>attempting to recall from memory</em> is 3\u00d7 more effective at building durable knowledge than re-reading notes. The act of struggling to remember \u2014 even failing \u2014 strengthens the neural pathway more than passive review.</p>\n<p>Think of it like muscle recovery: a weightlifter who trains 7 days straight without rest gets weaker, not stronger. The growth happens during recovery. Your brain is the same.</p>\n<h4>Today's Protocol (4 hours, structured)</h4>\n<ol>\n  <li><strong>Active Recall (60 min):</strong> Close everything. On a blank page, write every concept, formula, and pattern from this week from memory. Then check.</li>\n  <li><strong>Error Review (60 min):</strong> Find the hardest problem you struggled with. Attempt it again completely from scratch \u2014 no notes.</li>\n  <li><strong>Teach It (45 min):</strong> Open the AI chatbot and explain Gaps & Islands to it as if it's a junior engineer. If you stumble, that's your gap.</li>\n  <li><strong>Next Week Preview (15 min):</strong> Skim next Monday's topic at headline level only \u2014 priming improves retention when you study it deeply.</li>\n</ol>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Active Recall Quiz \u2014 No Notes Allowed</div>\n<div class=\"rich\">\n<h4>Answer These From Memory. Then Check.</h4>\n<pre>Window Functions:\n\u25a1 What is the difference between ROW_NUMBER, RANK, and DENSE_RANK?\n  (Write the output for a table with two tied rows)\n\u25a1 Write the syntax for a 7-day rolling average from memory\n\u25a1 What does PARTITION BY do differently from GROUP BY?\n\u25a1 Write the complete Gaps & Islands CTE from memory\n\u25a1 What is the difference between ROWS BETWEEN and RANGE BETWEEN?\n\u25a1 LAG(col, 1, 0) vs LAG(col, 1) \u2014 what is different?\n\u25a1 In what SQL execution order step do window functions run?\n\u25a1 Why can't you write WHERE rank &lt;= 3 in the same query as ROW_NUMBER?\n\nScore yourself:\n8/8 = \ud83c\udfc6 Mastery level \u2014 you're ready for interviews on this\n5-7/8 = \ud83d\udfe1 Good \u2014 review the 1-2 concepts you missed\n&lt; 5/8 = \ud83d\udd34 Go back to the specific day's content, re-read Level 1-2</pre>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Spaced Repetition Schedule</div>\n<div class=\"rich\">\n<h4>When to Review This Week's Material</h4>\n<table>\n<tr><th>Review this content</th><th>When</th><th>How</th></tr>\n<tr><td>This week</td><td>Today</td><td>Active recall quiz above</td></tr>\n<tr><td>This week</td><td>In 3 days</td><td>Re-solve the hard problem from Day 4</td></tr>\n<tr><td>This week</td><td>In 7 days</td><td>LeetCode: solve problems 185, 180, 196</td></tr>\n<tr><td>This week</td><td>In 30 days</td><td>Explain all 4 concepts to someone else</td></tr>\n</table>\n<h4>The Feynman Technique</h4>\n<p>After studying any concept, close your notes and explain it as if you're teaching a newcomer. Use only simple words. Avoid jargon. If you stumble, that's exactly where your understanding breaks down \u2014 return to just that part. This technique built Richard Feynman's legendary ability to explain complex physics simply.</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Interview Readiness Self-Assessment</div>\n<div class=\"rich\">\n<pre>Rate yourself 1\u20135 on each topic (honest self-assessment):\n  1 = \"I need to re-read from scratch\"\n  3 = \"I understand but need to look at notes\"\n  5 = \"I can code this under pressure from memory\"\n\nROW_NUMBER / RANK / DENSE_RANK:        ___/5\nRolling windows (ROWS BETWEEN):        ___/5\nLAG/LEAD (adjacent row comparison):    ___/5\nGaps & Islands (date streaks):         ___/5\nSession detection (timestamp version): ___/5\n\nAction:\n  Score &lt;3: revisit that day's Level 1 and Level 2 content tomorrow\n  Score 3:  do one LeetCode problem on that specific topic\n  Score 5:  you're interview-ready \u2705 on this topic</pre>\n<h4>FAANG Interview Reality Check</h4>\n<p>At meta, Amazon, and Google data engineering interviews, you'll be asked to write SQL live on a shared screen without autocomplete or documentation. The most common reason candidates fail: they know the concept but freeze on the syntax under pressure. The fix: write every pattern from memory at least 5 times this week. Muscle memory is more reliable than conscious recall under stress.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Memory consolidation happens during rest \u2014 the brain replays and stabilizes learned patterns during downtime.",
            "Active recall (testing yourself) builds 3x stronger memory than re-reading notes.",
            "Spaced repetition: review at day 1, day 3, day 7, day 30 for maximum retention.",
            "The Feynman Technique: if you can't explain it simply, you don't fully understand it yet.",
            "Error analysis: the problems you got WRONG are your highest-value review items \u2014 spend the most time there."
        ],
        "Tasks": [
            "<strong>Active Recall:</strong> Write the Gaps & Islands CTE from memory on paper. Check it against Day 4.",
            "<strong>Re-attempt:</strong> Pick the hardest problem from this week and solve it in 20 minutes from scratch.",
            "<strong>Teach it:</strong> Explain RANK vs DENSE_RANK to the AI chatbot as if it's a junior. Get it right without notes.",
            "<strong>Preview:</strong> Skim next week's Monday topic for 10 minutes \u2014 just headline concepts."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Review Storage Classes in S3",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Review Storage Classes in S3 \u2014 trace through 2-3 edge cases before writing any code.",
                "Use the AI chatbot to quiz yourself \u2014 ask it 'Give me a SQL window function interview question.'",
                "Write SQL from memory in a plain text editor \u2014 no autocomplete. This builds true recall.",
                "The hardest concept this week: spend 30 minutes writing a full explanation in your own words."
            ]
        },
        "HardProblem": "Connect-the-dots: Write 2-3 paragraphs explaining how ROW_NUMBER, rolling averages, LAG, and Gaps & Islands could ALL be used in a single production pipeline for a ride-sharing company. What problem does each solve? How do they complement each other? What would break if you removed any one of them?"
    },
    {
        "Week": 7,
        "Day": "Monday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Spark Internals",
        "SpecificTopic": "Logical Plan",
        "ActionItem_Deliverable": "Explain Predicate Pushdown with example",
        "LeetCodeProblem": "<strong>LC 322 \u2013 Coin Change</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>NoSQL Modeling</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 What Spark Is \u2014 Driver/Executor + Lazy Eval</div><div class=\"rich\"><h4>What Spark Actually Is \u2014 And Why It Exists</h4><p>Apache Spark was created in 2009 at UC Berkeley's AMPLab as a faster replacement for Hadoop MapReduce. Hadoop MapReduce wrote intermediate results to disk after every step. A 10-step pipeline wrote and read disk 10 times \u2014 massive I/O overhead. Spark keeps intermediate results <strong>in memory</strong>, passing data between stages without disk. For iterative algorithms (machine learning) and multi-step pipelines, this is 10\u2013100x faster.</p><h4>How Spark Executes Code \u2014 The Driver/Executor Model</h4><pre>Spark Cluster Architecture:\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              DRIVER (your Python code)          \u2502\n\u2502  - Runs on the master node                      \u2502\n\u2502  - Builds the execution plan                    \u2502\n\u2502  - Sends tasks to executors                     \u2502\n\u2502  - Collects results                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502 (sends serialized tasks)\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u25bc              \u25bc              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Executor \u2502  \u2502Executor \u2502  \u2502Executor \u2502\n\u2502 - Node 1\u2502  \u2502 - Node 2\u2502  \u2502 - Node 3\u2502\n\u2502 - tasks \u2502  \u2502 - tasks \u2502  \u2502 - tasks \u2502\n\u2502 - cache \u2502  \u2502 - cache \u2502  \u2502 - cache \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n# DataFrame operations run on executors in parallel\n# Driver only sees final results (unless you call .collect() \u2014 dangerous!)</pre><h4>Lazy Evaluation \u2014 Nothing Runs Until You Ask</h4><p>Spark is lazy: DataFrame transformations (filter, select, join, groupBy) do NOT execute immediately. They build a <strong>query plan</strong>. Only <strong>actions</strong> (collect(), count(), write(), show()) trigger actual execution.</p><pre>df = spark.read.parquet(\"s3://events/\")   # no I/O yet\ndf2 = df.filter(df.year == 2024)           # no I/O yet \u2014 builds plan\ndf3 = df2.groupBy(\"country\").count()       # no I/O yet \u2014 extends plan\n\ndf3.show()  # \u2190 ACTION: NOW Spark executes the entire plan\n\n# Why lazy? Spark's optimizer (Catalyst) can rearrange operations:\n# - Pushes filters before joins (read less data)\n# - Eliminates unused columns early\n# - Merges multiple transformations into single scan\n# This often makes your unoptimized code run faster automatically.</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 The Shuffle + Broadcast Joins</div><div class=\"rich\"><h4>The Shuffle \u2014 The Most Expensive Spark Operation</h4><p>A <strong>shuffle</strong> occurs when data needs to be redistributed across executors based on a key \u2014 for example, during a <code>groupBy</code>, <code>join</code>, or <code>repartition</code>. Shuffle is expensive because: (1) executors must write their partition to disk, (2) data is sent over the network, (3) receiving executors read from disk and memory. In a job with 100 executors processing 1TB, a shuffle can move terabytes over the network.</p><pre># Operations that trigger a shuffle:\ndf.groupBy(\"country\").count()            # \u2190 SHUFFLE: all rows for same country\n                                         #   must land on same executor\ndf.join(other, \"user_id\")               # \u2190 SHUFFLE: matching user_ids must\n                                         #   be co-located\ndf.repartition(200, \"country\")          # \u2190 SHUFFLE: explicit redistribution\ndf.distinct()                            # \u2190 SHUFFLE: deduplicate requires global view\n\n# Operations that do NOT shuffle (cheap \"narrow\" transformations):\ndf.filter(df.year == 2024)              # stays on same partition\ndf.select(\"user_id\", \"amount\")          # stays on same partition\ndf.withColumn(\"total\", df.a + df.b)    # stays on same partition\ndf.coalesce(10)                         # merge partitions, no redistribution\n\n# Reduce shuffles by:\n# 1. Partition data by join key BEFORE the join (pre-partition)\n# 2. Broadcast small tables (eliminates join shuffle entirely)\n# 3. Aggregate before joining (reduce data volume before shuffle)</pre><h4>Broadcast Join \u2014 Eliminating the Join Shuffle</h4><pre># Problem: joining a 100TB fact table with a 50MB dimension\n# Default join \u2192 shuffle 100TB over network (catastrophically slow)\n\n# Solution: broadcast the small table to ALL executors\n# Each executor gets a full copy \u2014 no network movement for the large table\n\nfrom pyspark.sql.functions import broadcast\n\nresult = large_df.join(\n    broadcast(small_dim_df),  # \u2190 small table copied to all executors\n    on=\"product_id\"\n)  # No shuffle of large_df! Each executor joins locally.\n\n# Spark auto-broadcasts tables < spark.sql.autoBroadcastJoinThreshold (default 10MB)\n# Increase for larger dimensions if they fit in executor memory:\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"100m\")</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Data Skew + Partitions vs Tasks</div><div class=\"rich\"><h4>Data Skew \u2014 When One Executor Does 90% of the Work</h4><p><strong>Data skew</strong> is the #1 production Spark performance problem. It occurs when one partition key value has far more rows than others. Example: a NULL user_id for all guest orders means the \"NULL\" partition gets 80% of all rows. One executor processing 8TB while others process 100GB \u2014 the job is only as fast as the slowest executor.</p><pre># Detect skew: check partition sizes\ndf.groupBy(spark_partition_id()).count().show()  # see partition row counts\n# If one partition has 100M rows and others have 1M \u2192 skew\n\n# Fix 1: Salt the key (add random suffix to distribute skewed keys)\nfrom pyspark.sql import functions as F\n\n# Add salt to the large table\nnum_salt_buckets = 10\nsalted_large = large_df.withColumn(\n    \"salted_key\",\n    F.concat(F.col(\"user_id\"), F.lit(\"_\"), (F.rand() * num_salt_buckets).cast(\"int\"))\n)\n\n# Explode the small table to match all salted keys\nsalt_values = spark.range(num_salt_buckets).withColumnRenamed(\"id\", \"salt\")\nsalted_small = small_df.crossJoin(salt_values).withColumn(\n    \"salted_key\",\n    F.concat(F.col(\"user_id\"), F.lit(\"_\"), F.col(\"salt\"))\n)\n\n# Now join on salted_key \u2014 skew is evenly distributed!\nresult = salted_large.join(salted_small, \"salted_key\")\n\n# Fix 2: Filter skewed keys and process separately\nnormal = df.filter(F.col(\"user_id\").isNotNull())\nnulls  = df.filter(F.col(\"user_id\").isNull())\n# Process each differently, then union</pre><h4>Partitions vs Tasks \u2014 Understanding Parallelism</h4><p>In Spark, a <strong>partition</strong> is a chunk of data. A <strong>task</strong> processes one partition. The number of parallel tasks = number of executor cores. If you have 100 cores and 1,000 partitions \u2192 100 tasks run simultaneously, 10 rounds of 100 tasks = 10 rounds total.</p><pre># Rule of thumb: 2-4 partitions per CPU core\n# 100 cores \u2192 200-400 partitions\n\n# Too few partitions: executors are idle (not enough work to distribute)\n# Too many partitions: per-partition overhead dominates (too many small tasks)\n\ndf.rdd.getNumPartitions()  # check current partition count\ndf.repartition(400)  # redistribute to 400 partitions (shuffle)\ndf.coalesce(10)      # reduce to 10 partitions (no shuffle, but may be uneven)\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"400\")  # for groupBy/join output</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Diagnosing a Slow Spark Job</div><div class=\"rich\"><h4>FAANG Interview: Diagnosing a Slow Spark Job</h4><p>Interview question: \"Your Spark job processes 10TB of data but takes 6 hours. Walk me through diagnosing it.\"</p><pre>Step 1 \u2014 Open the Spark UI (port 4040)\n  \u2192 DAG tab: which stage takes the most time?\n  \u2192 Stage details: which task is the slowest? (skew indicator)\n  \u2192 Storage tab: how much data is cached? Is spill occurring?\n\nStep 2 \u2014 Identify the type of slowdown:\n\n  A. One stage takes 90% of job time:\n     \u2192 Check if it contains a shuffle (groupBy, join)\n     \u2192 If so: can you reduce data before the shuffle? Broadcast the smaller side?\n\n  B. One task in a stage takes 10x longer than others:\n     \u2192 Data skew. Check: what is the key distribution?\n     \u2192 Fix: salt the key, or handle skewed keys separately\n\n  C. Spill to disk (red in Spark UI):\n     \u2192 Executor ran out of memory, wrote shuffle data to disk\n     \u2192 Fix: increase executor memory, or increase spark.sql.shuffle.partitions\n       (more partitions = smaller per-partition size = less memory per partition)\n\n  D. Many stages, each fast individually but slow overall:\n     \u2192 Wide dependency chain \u2014 hard to parallelize\n     \u2192 Fix: checkpoint intermediate results, cache frequently reused DataFrames\n\ndf.cache()  # cache in memory \u2014 avoid recomputing in subsequent actions\ndf.persist(StorageLevel.DISK_ONLY)  # persist to disk if too large for memory</pre><p>\u270d\ufe0f <strong>The Spark UI is your most important debugging tool.</strong> FAANG interviewers expect you to know how to read it and what each metric means.</p></div></div></div>",
        "KeyConcepts": [
            "Spark = distributed in-memory computing. Driver builds plan, executors execute tasks on partitions.",
            "Lazy evaluation: transformations build a plan. Actions (count/show/write) trigger execution.",
            "Catalyst optimizer: rearranges your query plan for efficiency \u2014 predicate pushdown, column pruning.",
            "Shuffle: data redistribution across network. Most expensive operation. Triggered by groupBy, join, distinct.",
            "Narrow transformation: each partition maps to exactly one output partition. No shuffle. Fast.",
            "Broadcast join: copies small table to all executors. Eliminates join shuffle for large+small joins.",
            "Data skew: one partition key has disproportionate rows. Fix: salting (randomize key) or separate processing.",
            "Partitions = data chunks. Tasks = work units (1 per partition). Target 2-4 partitions per core.",
            "spark.sql.shuffle.partitions default = 200. Tune to 2-4x your number of cores."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Read 10M rows. Chain: filter \u2192 groupBy \u2192 count. Use .explain(True) to show the physical plan. Identify which operations cause shuffles.",
            "<strong>Step 2:</strong> Join a 1B-row fact table with a 5MB lookup table. Compare: regular join vs broadcast join. Measure time difference with spark.time().",
            "<strong>Step 3:</strong> Simulate data skew: create a dataset where 80% of rows have user_id=NULL. Run a groupBy. Observe one task taking 10x longer. Apply the salt fix.",
            "<strong>Step 4:</strong> Set spark.sql.shuffle.partitions to 10, 200, 1000 on the same groupBy query. Observe partition count and execution time changes."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Explain Predicate Pushdown with example",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Explain Predicate Pushdown with example \u2014 trace through 2-3 edge cases before writing any code.",
                "Seeing one stage taking 90% of job time? Check for shuffle. Can you broadcast the smaller side?",
                "One task running 10x longer than others? Data skew. Check key value distribution with groupBy + count.",
                "Spill to disk in Spark UI? Executor out of memory. Increase executor memory or spark.sql.shuffle.partitions.",
                "Cache DataFrames reused multiple times: df.cache(). Unpersist when done: df.unpersist()."
            ]
        },
        "HardProblem": "Boss Problem (Uber): Your daily Spark job aggregates 500B GPS ping events (5TB) to compute driver earnings per city. It runs in 14 hours but needs to run in 2 hours. Steps: (1) You find 3 shuffle stages dominate. How do you investigate each one? (2) One join between pings (5TB) and a driver_metadata table (2GB) causes a 3TB shuffle. How do you fix it without salting? (3) City='NULL' has 60% of all pings (GPS loss). How do you handle this skew? (4) The job reads the same 5TB raw data for 4 different aggregations. How do you avoid reading it 4 times?"
    },
    {
        "Week": 7,
        "Day": "Tuesday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Spark Internals",
        "SpecificTopic": "The Shuffle",
        "ActionItem_Deliverable": "Draw what happens during groupByKey",
        "LeetCodeProblem": "<strong>LC 139 \u2013 Word Break</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>String Manipulation</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 What Spark Is \u2014 Driver/Executor + Lazy Eval</div><div class=\"rich\"><h4>What Spark Actually Is \u2014 And Why It Exists</h4><p>Apache Spark was created in 2009 at UC Berkeley's AMPLab as a faster replacement for Hadoop MapReduce. Hadoop MapReduce wrote intermediate results to disk after every step. A 10-step pipeline wrote and read disk 10 times \u2014 massive I/O overhead. Spark keeps intermediate results <strong>in memory</strong>, passing data between stages without disk. For iterative algorithms (machine learning) and multi-step pipelines, this is 10\u2013100x faster.</p><h4>How Spark Executes Code \u2014 The Driver/Executor Model</h4><pre>Spark Cluster Architecture:\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              DRIVER (your Python code)          \u2502\n\u2502  - Runs on the master node                      \u2502\n\u2502  - Builds the execution plan                    \u2502\n\u2502  - Sends tasks to executors                     \u2502\n\u2502  - Collects results                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502 (sends serialized tasks)\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u25bc              \u25bc              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Executor \u2502  \u2502Executor \u2502  \u2502Executor \u2502\n\u2502 - Node 1\u2502  \u2502 - Node 2\u2502  \u2502 - Node 3\u2502\n\u2502 - tasks \u2502  \u2502 - tasks \u2502  \u2502 - tasks \u2502\n\u2502 - cache \u2502  \u2502 - cache \u2502  \u2502 - cache \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n# DataFrame operations run on executors in parallel\n# Driver only sees final results (unless you call .collect() \u2014 dangerous!)</pre><h4>Lazy Evaluation \u2014 Nothing Runs Until You Ask</h4><p>Spark is lazy: DataFrame transformations (filter, select, join, groupBy) do NOT execute immediately. They build a <strong>query plan</strong>. Only <strong>actions</strong> (collect(), count(), write(), show()) trigger actual execution.</p><pre>df = spark.read.parquet(\"s3://events/\")   # no I/O yet\ndf2 = df.filter(df.year == 2024)           # no I/O yet \u2014 builds plan\ndf3 = df2.groupBy(\"country\").count()       # no I/O yet \u2014 extends plan\n\ndf3.show()  # \u2190 ACTION: NOW Spark executes the entire plan\n\n# Why lazy? Spark's optimizer (Catalyst) can rearrange operations:\n# - Pushes filters before joins (read less data)\n# - Eliminates unused columns early\n# - Merges multiple transformations into single scan\n# This often makes your unoptimized code run faster automatically.</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 The Shuffle + Broadcast Joins</div><div class=\"rich\"><h4>The Shuffle \u2014 The Most Expensive Spark Operation</h4><p>A <strong>shuffle</strong> occurs when data needs to be redistributed across executors based on a key \u2014 for example, during a <code>groupBy</code>, <code>join</code>, or <code>repartition</code>. Shuffle is expensive because: (1) executors must write their partition to disk, (2) data is sent over the network, (3) receiving executors read from disk and memory. In a job with 100 executors processing 1TB, a shuffle can move terabytes over the network.</p><pre># Operations that trigger a shuffle:\ndf.groupBy(\"country\").count()            # \u2190 SHUFFLE: all rows for same country\n                                         #   must land on same executor\ndf.join(other, \"user_id\")               # \u2190 SHUFFLE: matching user_ids must\n                                         #   be co-located\ndf.repartition(200, \"country\")          # \u2190 SHUFFLE: explicit redistribution\ndf.distinct()                            # \u2190 SHUFFLE: deduplicate requires global view\n\n# Operations that do NOT shuffle (cheap \"narrow\" transformations):\ndf.filter(df.year == 2024)              # stays on same partition\ndf.select(\"user_id\", \"amount\")          # stays on same partition\ndf.withColumn(\"total\", df.a + df.b)    # stays on same partition\ndf.coalesce(10)                         # merge partitions, no redistribution\n\n# Reduce shuffles by:\n# 1. Partition data by join key BEFORE the join (pre-partition)\n# 2. Broadcast small tables (eliminates join shuffle entirely)\n# 3. Aggregate before joining (reduce data volume before shuffle)</pre><h4>Broadcast Join \u2014 Eliminating the Join Shuffle</h4><pre># Problem: joining a 100TB fact table with a 50MB dimension\n# Default join \u2192 shuffle 100TB over network (catastrophically slow)\n\n# Solution: broadcast the small table to ALL executors\n# Each executor gets a full copy \u2014 no network movement for the large table\n\nfrom pyspark.sql.functions import broadcast\n\nresult = large_df.join(\n    broadcast(small_dim_df),  # \u2190 small table copied to all executors\n    on=\"product_id\"\n)  # No shuffle of large_df! Each executor joins locally.\n\n# Spark auto-broadcasts tables < spark.sql.autoBroadcastJoinThreshold (default 10MB)\n# Increase for larger dimensions if they fit in executor memory:\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"100m\")</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Data Skew + Partitions vs Tasks</div><div class=\"rich\"><h4>Data Skew \u2014 When One Executor Does 90% of the Work</h4><p><strong>Data skew</strong> is the #1 production Spark performance problem. It occurs when one partition key value has far more rows than others. Example: a NULL user_id for all guest orders means the \"NULL\" partition gets 80% of all rows. One executor processing 8TB while others process 100GB \u2014 the job is only as fast as the slowest executor.</p><pre># Detect skew: check partition sizes\ndf.groupBy(spark_partition_id()).count().show()  # see partition row counts\n# If one partition has 100M rows and others have 1M \u2192 skew\n\n# Fix 1: Salt the key (add random suffix to distribute skewed keys)\nfrom pyspark.sql import functions as F\n\n# Add salt to the large table\nnum_salt_buckets = 10\nsalted_large = large_df.withColumn(\n    \"salted_key\",\n    F.concat(F.col(\"user_id\"), F.lit(\"_\"), (F.rand() * num_salt_buckets).cast(\"int\"))\n)\n\n# Explode the small table to match all salted keys\nsalt_values = spark.range(num_salt_buckets).withColumnRenamed(\"id\", \"salt\")\nsalted_small = small_df.crossJoin(salt_values).withColumn(\n    \"salted_key\",\n    F.concat(F.col(\"user_id\"), F.lit(\"_\"), F.col(\"salt\"))\n)\n\n# Now join on salted_key \u2014 skew is evenly distributed!\nresult = salted_large.join(salted_small, \"salted_key\")\n\n# Fix 2: Filter skewed keys and process separately\nnormal = df.filter(F.col(\"user_id\").isNotNull())\nnulls  = df.filter(F.col(\"user_id\").isNull())\n# Process each differently, then union</pre><h4>Partitions vs Tasks \u2014 Understanding Parallelism</h4><p>In Spark, a <strong>partition</strong> is a chunk of data. A <strong>task</strong> processes one partition. The number of parallel tasks = number of executor cores. If you have 100 cores and 1,000 partitions \u2192 100 tasks run simultaneously, 10 rounds of 100 tasks = 10 rounds total.</p><pre># Rule of thumb: 2-4 partitions per CPU core\n# 100 cores \u2192 200-400 partitions\n\n# Too few partitions: executors are idle (not enough work to distribute)\n# Too many partitions: per-partition overhead dominates (too many small tasks)\n\ndf.rdd.getNumPartitions()  # check current partition count\ndf.repartition(400)  # redistribute to 400 partitions (shuffle)\ndf.coalesce(10)      # reduce to 10 partitions (no shuffle, but may be uneven)\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"400\")  # for groupBy/join output</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Diagnosing a Slow Spark Job</div><div class=\"rich\"><h4>FAANG Interview: Diagnosing a Slow Spark Job</h4><p>Interview question: \"Your Spark job processes 10TB of data but takes 6 hours. Walk me through diagnosing it.\"</p><pre>Step 1 \u2014 Open the Spark UI (port 4040)\n  \u2192 DAG tab: which stage takes the most time?\n  \u2192 Stage details: which task is the slowest? (skew indicator)\n  \u2192 Storage tab: how much data is cached? Is spill occurring?\n\nStep 2 \u2014 Identify the type of slowdown:\n\n  A. One stage takes 90% of job time:\n     \u2192 Check if it contains a shuffle (groupBy, join)\n     \u2192 If so: can you reduce data before the shuffle? Broadcast the smaller side?\n\n  B. One task in a stage takes 10x longer than others:\n     \u2192 Data skew. Check: what is the key distribution?\n     \u2192 Fix: salt the key, or handle skewed keys separately\n\n  C. Spill to disk (red in Spark UI):\n     \u2192 Executor ran out of memory, wrote shuffle data to disk\n     \u2192 Fix: increase executor memory, or increase spark.sql.shuffle.partitions\n       (more partitions = smaller per-partition size = less memory per partition)\n\n  D. Many stages, each fast individually but slow overall:\n     \u2192 Wide dependency chain \u2014 hard to parallelize\n     \u2192 Fix: checkpoint intermediate results, cache frequently reused DataFrames\n\ndf.cache()  # cache in memory \u2014 avoid recomputing in subsequent actions\ndf.persist(StorageLevel.DISK_ONLY)  # persist to disk if too large for memory</pre><p>\u270d\ufe0f <strong>The Spark UI is your most important debugging tool.</strong> FAANG interviewers expect you to know how to read it and what each metric means.</p></div></div></div>",
        "KeyConcepts": [
            "Spark = distributed in-memory computing. Driver builds plan, executors execute tasks on partitions.",
            "Lazy evaluation: transformations build a plan. Actions (count/show/write) trigger execution.",
            "Catalyst optimizer: rearranges your query plan for efficiency \u2014 predicate pushdown, column pruning.",
            "Shuffle: data redistribution across network. Most expensive operation. Triggered by groupBy, join, distinct.",
            "Narrow transformation: each partition maps to exactly one output partition. No shuffle. Fast.",
            "Broadcast join: copies small table to all executors. Eliminates join shuffle for large+small joins.",
            "Data skew: one partition key has disproportionate rows. Fix: salting (randomize key) or separate processing.",
            "Partitions = data chunks. Tasks = work units (1 per partition). Target 2-4 partitions per core.",
            "spark.sql.shuffle.partitions default = 200. Tune to 2-4x your number of cores."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Read 10M rows. Chain: filter \u2192 groupBy \u2192 count. Use .explain(True) to show the physical plan. Identify which operations cause shuffles.",
            "<strong>Step 2:</strong> Join a 1B-row fact table with a 5MB lookup table. Compare: regular join vs broadcast join. Measure time difference with spark.time().",
            "<strong>Step 3:</strong> Simulate data skew: create a dataset where 80% of rows have user_id=NULL. Run a groupBy. Observe one task taking 10x longer. Apply the salt fix.",
            "<strong>Step 4:</strong> Set spark.sql.shuffle.partitions to 10, 200, 1000 on the same groupBy query. Observe partition count and execution time changes."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Draw what happens during groupByKey",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Draw what happens during groupByKey \u2014 trace through 2-3 edge cases before writing any code.",
                "Seeing one stage taking 90% of job time? Check for shuffle. Can you broadcast the smaller side?",
                "One task running 10x longer than others? Data skew. Check key value distribution with groupBy + count.",
                "Spill to disk in Spark UI? Executor out of memory. Increase executor memory or spark.sql.shuffle.partitions.",
                "Cache DataFrames reused multiple times: df.cache(). Unpersist when done: df.unpersist()."
            ]
        },
        "HardProblem": "Boss Problem (Uber): Your daily Spark job aggregates 500B GPS ping events (5TB) to compute driver earnings per city. It runs in 14 hours but needs to run in 2 hours. Steps: (1) You find 3 shuffle stages dominate. How do you investigate each one? (2) One join between pings (5TB) and a driver_metadata table (2GB) causes a 3TB shuffle. How do you fix it without salting? (3) City='NULL' has 60% of all pings (GPS loss). How do you handle this skew? (4) The job reads the same 5TB raw data for 4 different aggregations. How do you avoid reading it 4 times?"
    },
    {
        "Week": 7,
        "Day": "Wednesday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Spark Internals",
        "SpecificTopic": "Partitions vs Tasks",
        "ActionItem_Deliverable": "Debug Why 200 partitions for 10MB data?",
        "LeetCodeProblem": "<strong>LC 300 \u2013 Longest Increasing Subsequence</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Gaps & Islands (Logic)</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 What Spark Is \u2014 Driver/Executor + Lazy Eval</div><div class=\"rich\"><h4>What Spark Actually Is \u2014 And Why It Exists</h4><p>Apache Spark was created in 2009 at UC Berkeley's AMPLab as a faster replacement for Hadoop MapReduce. Hadoop MapReduce wrote intermediate results to disk after every step. A 10-step pipeline wrote and read disk 10 times \u2014 massive I/O overhead. Spark keeps intermediate results <strong>in memory</strong>, passing data between stages without disk. For iterative algorithms (machine learning) and multi-step pipelines, this is 10\u2013100x faster.</p><h4>How Spark Executes Code \u2014 The Driver/Executor Model</h4><pre>Spark Cluster Architecture:\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              DRIVER (your Python code)          \u2502\n\u2502  - Runs on the master node                      \u2502\n\u2502  - Builds the execution plan                    \u2502\n\u2502  - Sends tasks to executors                     \u2502\n\u2502  - Collects results                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502 (sends serialized tasks)\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u25bc              \u25bc              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Executor \u2502  \u2502Executor \u2502  \u2502Executor \u2502\n\u2502 - Node 1\u2502  \u2502 - Node 2\u2502  \u2502 - Node 3\u2502\n\u2502 - tasks \u2502  \u2502 - tasks \u2502  \u2502 - tasks \u2502\n\u2502 - cache \u2502  \u2502 - cache \u2502  \u2502 - cache \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n# DataFrame operations run on executors in parallel\n# Driver only sees final results (unless you call .collect() \u2014 dangerous!)</pre><h4>Lazy Evaluation \u2014 Nothing Runs Until You Ask</h4><p>Spark is lazy: DataFrame transformations (filter, select, join, groupBy) do NOT execute immediately. They build a <strong>query plan</strong>. Only <strong>actions</strong> (collect(), count(), write(), show()) trigger actual execution.</p><pre>df = spark.read.parquet(\"s3://events/\")   # no I/O yet\ndf2 = df.filter(df.year == 2024)           # no I/O yet \u2014 builds plan\ndf3 = df2.groupBy(\"country\").count()       # no I/O yet \u2014 extends plan\n\ndf3.show()  # \u2190 ACTION: NOW Spark executes the entire plan\n\n# Why lazy? Spark's optimizer (Catalyst) can rearrange operations:\n# - Pushes filters before joins (read less data)\n# - Eliminates unused columns early\n# - Merges multiple transformations into single scan\n# This often makes your unoptimized code run faster automatically.</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 The Shuffle + Broadcast Joins</div><div class=\"rich\"><h4>The Shuffle \u2014 The Most Expensive Spark Operation</h4><p>A <strong>shuffle</strong> occurs when data needs to be redistributed across executors based on a key \u2014 for example, during a <code>groupBy</code>, <code>join</code>, or <code>repartition</code>. Shuffle is expensive because: (1) executors must write their partition to disk, (2) data is sent over the network, (3) receiving executors read from disk and memory. In a job with 100 executors processing 1TB, a shuffle can move terabytes over the network.</p><pre># Operations that trigger a shuffle:\ndf.groupBy(\"country\").count()            # \u2190 SHUFFLE: all rows for same country\n                                         #   must land on same executor\ndf.join(other, \"user_id\")               # \u2190 SHUFFLE: matching user_ids must\n                                         #   be co-located\ndf.repartition(200, \"country\")          # \u2190 SHUFFLE: explicit redistribution\ndf.distinct()                            # \u2190 SHUFFLE: deduplicate requires global view\n\n# Operations that do NOT shuffle (cheap \"narrow\" transformations):\ndf.filter(df.year == 2024)              # stays on same partition\ndf.select(\"user_id\", \"amount\")          # stays on same partition\ndf.withColumn(\"total\", df.a + df.b)    # stays on same partition\ndf.coalesce(10)                         # merge partitions, no redistribution\n\n# Reduce shuffles by:\n# 1. Partition data by join key BEFORE the join (pre-partition)\n# 2. Broadcast small tables (eliminates join shuffle entirely)\n# 3. Aggregate before joining (reduce data volume before shuffle)</pre><h4>Broadcast Join \u2014 Eliminating the Join Shuffle</h4><pre># Problem: joining a 100TB fact table with a 50MB dimension\n# Default join \u2192 shuffle 100TB over network (catastrophically slow)\n\n# Solution: broadcast the small table to ALL executors\n# Each executor gets a full copy \u2014 no network movement for the large table\n\nfrom pyspark.sql.functions import broadcast\n\nresult = large_df.join(\n    broadcast(small_dim_df),  # \u2190 small table copied to all executors\n    on=\"product_id\"\n)  # No shuffle of large_df! Each executor joins locally.\n\n# Spark auto-broadcasts tables < spark.sql.autoBroadcastJoinThreshold (default 10MB)\n# Increase for larger dimensions if they fit in executor memory:\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"100m\")</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Data Skew + Partitions vs Tasks</div><div class=\"rich\"><h4>Data Skew \u2014 When One Executor Does 90% of the Work</h4><p><strong>Data skew</strong> is the #1 production Spark performance problem. It occurs when one partition key value has far more rows than others. Example: a NULL user_id for all guest orders means the \"NULL\" partition gets 80% of all rows. One executor processing 8TB while others process 100GB \u2014 the job is only as fast as the slowest executor.</p><pre># Detect skew: check partition sizes\ndf.groupBy(spark_partition_id()).count().show()  # see partition row counts\n# If one partition has 100M rows and others have 1M \u2192 skew\n\n# Fix 1: Salt the key (add random suffix to distribute skewed keys)\nfrom pyspark.sql import functions as F\n\n# Add salt to the large table\nnum_salt_buckets = 10\nsalted_large = large_df.withColumn(\n    \"salted_key\",\n    F.concat(F.col(\"user_id\"), F.lit(\"_\"), (F.rand() * num_salt_buckets).cast(\"int\"))\n)\n\n# Explode the small table to match all salted keys\nsalt_values = spark.range(num_salt_buckets).withColumnRenamed(\"id\", \"salt\")\nsalted_small = small_df.crossJoin(salt_values).withColumn(\n    \"salted_key\",\n    F.concat(F.col(\"user_id\"), F.lit(\"_\"), F.col(\"salt\"))\n)\n\n# Now join on salted_key \u2014 skew is evenly distributed!\nresult = salted_large.join(salted_small, \"salted_key\")\n\n# Fix 2: Filter skewed keys and process separately\nnormal = df.filter(F.col(\"user_id\").isNotNull())\nnulls  = df.filter(F.col(\"user_id\").isNull())\n# Process each differently, then union</pre><h4>Partitions vs Tasks \u2014 Understanding Parallelism</h4><p>In Spark, a <strong>partition</strong> is a chunk of data. A <strong>task</strong> processes one partition. The number of parallel tasks = number of executor cores. If you have 100 cores and 1,000 partitions \u2192 100 tasks run simultaneously, 10 rounds of 100 tasks = 10 rounds total.</p><pre># Rule of thumb: 2-4 partitions per CPU core\n# 100 cores \u2192 200-400 partitions\n\n# Too few partitions: executors are idle (not enough work to distribute)\n# Too many partitions: per-partition overhead dominates (too many small tasks)\n\ndf.rdd.getNumPartitions()  # check current partition count\ndf.repartition(400)  # redistribute to 400 partitions (shuffle)\ndf.coalesce(10)      # reduce to 10 partitions (no shuffle, but may be uneven)\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"400\")  # for groupBy/join output</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Diagnosing a Slow Spark Job</div><div class=\"rich\"><h4>FAANG Interview: Diagnosing a Slow Spark Job</h4><p>Interview question: \"Your Spark job processes 10TB of data but takes 6 hours. Walk me through diagnosing it.\"</p><pre>Step 1 \u2014 Open the Spark UI (port 4040)\n  \u2192 DAG tab: which stage takes the most time?\n  \u2192 Stage details: which task is the slowest? (skew indicator)\n  \u2192 Storage tab: how much data is cached? Is spill occurring?\n\nStep 2 \u2014 Identify the type of slowdown:\n\n  A. One stage takes 90% of job time:\n     \u2192 Check if it contains a shuffle (groupBy, join)\n     \u2192 If so: can you reduce data before the shuffle? Broadcast the smaller side?\n\n  B. One task in a stage takes 10x longer than others:\n     \u2192 Data skew. Check: what is the key distribution?\n     \u2192 Fix: salt the key, or handle skewed keys separately\n\n  C. Spill to disk (red in Spark UI):\n     \u2192 Executor ran out of memory, wrote shuffle data to disk\n     \u2192 Fix: increase executor memory, or increase spark.sql.shuffle.partitions\n       (more partitions = smaller per-partition size = less memory per partition)\n\n  D. Many stages, each fast individually but slow overall:\n     \u2192 Wide dependency chain \u2014 hard to parallelize\n     \u2192 Fix: checkpoint intermediate results, cache frequently reused DataFrames\n\ndf.cache()  # cache in memory \u2014 avoid recomputing in subsequent actions\ndf.persist(StorageLevel.DISK_ONLY)  # persist to disk if too large for memory</pre><p>\u270d\ufe0f <strong>The Spark UI is your most important debugging tool.</strong> FAANG interviewers expect you to know how to read it and what each metric means.</p></div></div></div>",
        "KeyConcepts": [
            "Spark = distributed in-memory computing. Driver builds plan, executors execute tasks on partitions.",
            "Lazy evaluation: transformations build a plan. Actions (count/show/write) trigger execution.",
            "Catalyst optimizer: rearranges your query plan for efficiency \u2014 predicate pushdown, column pruning.",
            "Shuffle: data redistribution across network. Most expensive operation. Triggered by groupBy, join, distinct.",
            "Narrow transformation: each partition maps to exactly one output partition. No shuffle. Fast.",
            "Broadcast join: copies small table to all executors. Eliminates join shuffle for large+small joins.",
            "Data skew: one partition key has disproportionate rows. Fix: salting (randomize key) or separate processing.",
            "Partitions = data chunks. Tasks = work units (1 per partition). Target 2-4 partitions per core.",
            "spark.sql.shuffle.partitions default = 200. Tune to 2-4x your number of cores."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Read 10M rows. Chain: filter \u2192 groupBy \u2192 count. Use .explain(True) to show the physical plan. Identify which operations cause shuffles.",
            "<strong>Step 2:</strong> Join a 1B-row fact table with a 5MB lookup table. Compare: regular join vs broadcast join. Measure time difference with spark.time().",
            "<strong>Step 3:</strong> Simulate data skew: create a dataset where 80% of rows have user_id=NULL. Run a groupBy. Observe one task taking 10x longer. Apply the salt fix.",
            "<strong>Step 4:</strong> Set spark.sql.shuffle.partitions to 10, 200, 1000 on the same groupBy query. Observe partition count and execution time changes."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Debug Why 200 partitions for 10MB data?",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Debug Why 200 partitions for 10MB data? \u2014 trace through 2-3 edge cases before writing any code.",
                "Seeing one stage taking 90% of job time? Check for shuffle. Can you broadcast the smaller side?",
                "One task running 10x longer than others? Data skew. Check key value distribution with groupBy + count.",
                "Spill to disk in Spark UI? Executor out of memory. Increase executor memory or spark.sql.shuffle.partitions.",
                "Cache DataFrames reused multiple times: df.cache(). Unpersist when done: df.unpersist()."
            ]
        },
        "HardProblem": "Boss Problem (Uber): Your daily Spark job aggregates 500B GPS ping events (5TB) to compute driver earnings per city. It runs in 14 hours but needs to run in 2 hours. Steps: (1) You find 3 shuffle stages dominate. How do you investigate each one? (2) One join between pings (5TB) and a driver_metadata table (2GB) causes a 3TB shuffle. How do you fix it without salting? (3) City='NULL' has 60% of all pings (GPS loss). How do you handle this skew? (4) The job reads the same 5TB raw data for 4 different aggregations. How do you avoid reading it 4 times?"
    },
    {
        "Week": 7,
        "Day": "Thursday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Spark Internals",
        "SpecificTopic": "Broadcasting",
        "ActionItem_Deliverable": "Explain Broadcast Hash Join & when it fails",
        "LeetCodeProblem": "<strong>LC 416 \u2013 Partition Equal Subset Sum</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Decorators</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 What Spark Is \u2014 Driver/Executor + Lazy Eval</div><div class=\"rich\"><h4>What Spark Actually Is \u2014 And Why It Exists</h4><p>Apache Spark was created in 2009 at UC Berkeley's AMPLab as a faster replacement for Hadoop MapReduce. Hadoop MapReduce wrote intermediate results to disk after every step. A 10-step pipeline wrote and read disk 10 times \u2014 massive I/O overhead. Spark keeps intermediate results <strong>in memory</strong>, passing data between stages without disk. For iterative algorithms (machine learning) and multi-step pipelines, this is 10\u2013100x faster.</p><h4>How Spark Executes Code \u2014 The Driver/Executor Model</h4><pre>Spark Cluster Architecture:\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              DRIVER (your Python code)          \u2502\n\u2502  - Runs on the master node                      \u2502\n\u2502  - Builds the execution plan                    \u2502\n\u2502  - Sends tasks to executors                     \u2502\n\u2502  - Collects results                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502 (sends serialized tasks)\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u25bc              \u25bc              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Executor \u2502  \u2502Executor \u2502  \u2502Executor \u2502\n\u2502 - Node 1\u2502  \u2502 - Node 2\u2502  \u2502 - Node 3\u2502\n\u2502 - tasks \u2502  \u2502 - tasks \u2502  \u2502 - tasks \u2502\n\u2502 - cache \u2502  \u2502 - cache \u2502  \u2502 - cache \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n# DataFrame operations run on executors in parallel\n# Driver only sees final results (unless you call .collect() \u2014 dangerous!)</pre><h4>Lazy Evaluation \u2014 Nothing Runs Until You Ask</h4><p>Spark is lazy: DataFrame transformations (filter, select, join, groupBy) do NOT execute immediately. They build a <strong>query plan</strong>. Only <strong>actions</strong> (collect(), count(), write(), show()) trigger actual execution.</p><pre>df = spark.read.parquet(\"s3://events/\")   # no I/O yet\ndf2 = df.filter(df.year == 2024)           # no I/O yet \u2014 builds plan\ndf3 = df2.groupBy(\"country\").count()       # no I/O yet \u2014 extends plan\n\ndf3.show()  # \u2190 ACTION: NOW Spark executes the entire plan\n\n# Why lazy? Spark's optimizer (Catalyst) can rearrange operations:\n# - Pushes filters before joins (read less data)\n# - Eliminates unused columns early\n# - Merges multiple transformations into single scan\n# This often makes your unoptimized code run faster automatically.</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 The Shuffle + Broadcast Joins</div><div class=\"rich\"><h4>The Shuffle \u2014 The Most Expensive Spark Operation</h4><p>A <strong>shuffle</strong> occurs when data needs to be redistributed across executors based on a key \u2014 for example, during a <code>groupBy</code>, <code>join</code>, or <code>repartition</code>. Shuffle is expensive because: (1) executors must write their partition to disk, (2) data is sent over the network, (3) receiving executors read from disk and memory. In a job with 100 executors processing 1TB, a shuffle can move terabytes over the network.</p><pre># Operations that trigger a shuffle:\ndf.groupBy(\"country\").count()            # \u2190 SHUFFLE: all rows for same country\n                                         #   must land on same executor\ndf.join(other, \"user_id\")               # \u2190 SHUFFLE: matching user_ids must\n                                         #   be co-located\ndf.repartition(200, \"country\")          # \u2190 SHUFFLE: explicit redistribution\ndf.distinct()                            # \u2190 SHUFFLE: deduplicate requires global view\n\n# Operations that do NOT shuffle (cheap \"narrow\" transformations):\ndf.filter(df.year == 2024)              # stays on same partition\ndf.select(\"user_id\", \"amount\")          # stays on same partition\ndf.withColumn(\"total\", df.a + df.b)    # stays on same partition\ndf.coalesce(10)                         # merge partitions, no redistribution\n\n# Reduce shuffles by:\n# 1. Partition data by join key BEFORE the join (pre-partition)\n# 2. Broadcast small tables (eliminates join shuffle entirely)\n# 3. Aggregate before joining (reduce data volume before shuffle)</pre><h4>Broadcast Join \u2014 Eliminating the Join Shuffle</h4><pre># Problem: joining a 100TB fact table with a 50MB dimension\n# Default join \u2192 shuffle 100TB over network (catastrophically slow)\n\n# Solution: broadcast the small table to ALL executors\n# Each executor gets a full copy \u2014 no network movement for the large table\n\nfrom pyspark.sql.functions import broadcast\n\nresult = large_df.join(\n    broadcast(small_dim_df),  # \u2190 small table copied to all executors\n    on=\"product_id\"\n)  # No shuffle of large_df! Each executor joins locally.\n\n# Spark auto-broadcasts tables < spark.sql.autoBroadcastJoinThreshold (default 10MB)\n# Increase for larger dimensions if they fit in executor memory:\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"100m\")</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Data Skew + Partitions vs Tasks</div><div class=\"rich\"><h4>Data Skew \u2014 When One Executor Does 90% of the Work</h4><p><strong>Data skew</strong> is the #1 production Spark performance problem. It occurs when one partition key value has far more rows than others. Example: a NULL user_id for all guest orders means the \"NULL\" partition gets 80% of all rows. One executor processing 8TB while others process 100GB \u2014 the job is only as fast as the slowest executor.</p><pre># Detect skew: check partition sizes\ndf.groupBy(spark_partition_id()).count().show()  # see partition row counts\n# If one partition has 100M rows and others have 1M \u2192 skew\n\n# Fix 1: Salt the key (add random suffix to distribute skewed keys)\nfrom pyspark.sql import functions as F\n\n# Add salt to the large table\nnum_salt_buckets = 10\nsalted_large = large_df.withColumn(\n    \"salted_key\",\n    F.concat(F.col(\"user_id\"), F.lit(\"_\"), (F.rand() * num_salt_buckets).cast(\"int\"))\n)\n\n# Explode the small table to match all salted keys\nsalt_values = spark.range(num_salt_buckets).withColumnRenamed(\"id\", \"salt\")\nsalted_small = small_df.crossJoin(salt_values).withColumn(\n    \"salted_key\",\n    F.concat(F.col(\"user_id\"), F.lit(\"_\"), F.col(\"salt\"))\n)\n\n# Now join on salted_key \u2014 skew is evenly distributed!\nresult = salted_large.join(salted_small, \"salted_key\")\n\n# Fix 2: Filter skewed keys and process separately\nnormal = df.filter(F.col(\"user_id\").isNotNull())\nnulls  = df.filter(F.col(\"user_id\").isNull())\n# Process each differently, then union</pre><h4>Partitions vs Tasks \u2014 Understanding Parallelism</h4><p>In Spark, a <strong>partition</strong> is a chunk of data. A <strong>task</strong> processes one partition. The number of parallel tasks = number of executor cores. If you have 100 cores and 1,000 partitions \u2192 100 tasks run simultaneously, 10 rounds of 100 tasks = 10 rounds total.</p><pre># Rule of thumb: 2-4 partitions per CPU core\n# 100 cores \u2192 200-400 partitions\n\n# Too few partitions: executors are idle (not enough work to distribute)\n# Too many partitions: per-partition overhead dominates (too many small tasks)\n\ndf.rdd.getNumPartitions()  # check current partition count\ndf.repartition(400)  # redistribute to 400 partitions (shuffle)\ndf.coalesce(10)      # reduce to 10 partitions (no shuffle, but may be uneven)\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"400\")  # for groupBy/join output</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Diagnosing a Slow Spark Job</div><div class=\"rich\"><h4>FAANG Interview: Diagnosing a Slow Spark Job</h4><p>Interview question: \"Your Spark job processes 10TB of data but takes 6 hours. Walk me through diagnosing it.\"</p><pre>Step 1 \u2014 Open the Spark UI (port 4040)\n  \u2192 DAG tab: which stage takes the most time?\n  \u2192 Stage details: which task is the slowest? (skew indicator)\n  \u2192 Storage tab: how much data is cached? Is spill occurring?\n\nStep 2 \u2014 Identify the type of slowdown:\n\n  A. One stage takes 90% of job time:\n     \u2192 Check if it contains a shuffle (groupBy, join)\n     \u2192 If so: can you reduce data before the shuffle? Broadcast the smaller side?\n\n  B. One task in a stage takes 10x longer than others:\n     \u2192 Data skew. Check: what is the key distribution?\n     \u2192 Fix: salt the key, or handle skewed keys separately\n\n  C. Spill to disk (red in Spark UI):\n     \u2192 Executor ran out of memory, wrote shuffle data to disk\n     \u2192 Fix: increase executor memory, or increase spark.sql.shuffle.partitions\n       (more partitions = smaller per-partition size = less memory per partition)\n\n  D. Many stages, each fast individually but slow overall:\n     \u2192 Wide dependency chain \u2014 hard to parallelize\n     \u2192 Fix: checkpoint intermediate results, cache frequently reused DataFrames\n\ndf.cache()  # cache in memory \u2014 avoid recomputing in subsequent actions\ndf.persist(StorageLevel.DISK_ONLY)  # persist to disk if too large for memory</pre><p>\u270d\ufe0f <strong>The Spark UI is your most important debugging tool.</strong> FAANG interviewers expect you to know how to read it and what each metric means.</p></div></div></div>",
        "KeyConcepts": [
            "Spark = distributed in-memory computing. Driver builds plan, executors execute tasks on partitions.",
            "Lazy evaluation: transformations build a plan. Actions (count/show/write) trigger execution.",
            "Catalyst optimizer: rearranges your query plan for efficiency \u2014 predicate pushdown, column pruning.",
            "Shuffle: data redistribution across network. Most expensive operation. Triggered by groupBy, join, distinct.",
            "Narrow transformation: each partition maps to exactly one output partition. No shuffle. Fast.",
            "Broadcast join: copies small table to all executors. Eliminates join shuffle for large+small joins.",
            "Data skew: one partition key has disproportionate rows. Fix: salting (randomize key) or separate processing.",
            "Partitions = data chunks. Tasks = work units (1 per partition). Target 2-4 partitions per core.",
            "spark.sql.shuffle.partitions default = 200. Tune to 2-4x your number of cores."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Read 10M rows. Chain: filter \u2192 groupBy \u2192 count. Use .explain(True) to show the physical plan. Identify which operations cause shuffles.",
            "<strong>Step 2:</strong> Join a 1B-row fact table with a 5MB lookup table. Compare: regular join vs broadcast join. Measure time difference with spark.time().",
            "<strong>Step 3:</strong> Simulate data skew: create a dataset where 80% of rows have user_id=NULL. Run a groupBy. Observe one task taking 10x longer. Apply the salt fix.",
            "<strong>Step 4:</strong> Set spark.sql.shuffle.partitions to 10, 200, 1000 on the same groupBy query. Observe partition count and execution time changes."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Explain Broadcast Hash Join & when it fails",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Explain Broadcast Hash Join & when it fails \u2014 trace through 2-3 edge cases before writing any code.",
                "Seeing one stage taking 90% of job time? Check for shuffle. Can you broadcast the smaller side?",
                "One task running 10x longer than others? Data skew. Check key value distribution with groupBy + count.",
                "Spill to disk in Spark UI? Executor out of memory. Increase executor memory or spark.sql.shuffle.partitions.",
                "Cache DataFrames reused multiple times: df.cache(). Unpersist when done: df.unpersist()."
            ]
        },
        "HardProblem": "Boss Problem (Uber): Your daily Spark job aggregates 500B GPS ping events (5TB) to compute driver earnings per city. It runs in 14 hours but needs to run in 2 hours. Steps: (1) You find 3 shuffle stages dominate. How do you investigate each one? (2) One join between pings (5TB) and a driver_metadata table (2GB) causes a 3TB shuffle. How do you fix it without salting? (3) City='NULL' has 60% of all pings (GPS loss). How do you handle this skew? (4) The job reads the same 5TB raw data for 4 different aggregations. How do you avoid reading it 4 times?"
    },
    {
        "Week": 7,
        "Day": "Friday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Spark Internals",
        "SpecificTopic": "Memory Management",
        "ActionItem_Deliverable": "Explain OOM: Java Heap Space vs Overhead",
        "LeetCodeProblem": "<strong>LC 91 \u2013 Decode Ways</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Sets & Deduplication</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Generators \u2014 The Memory Problem</div><div class=\"rich\"><h4>The Memory Problem \u2014 Why Generators Exist</h4><p>Imagine you need to process a 100GB log file. The naive approach: <code>lines = open(\"log.txt\").readlines()</code>. This reads all 100GB into RAM at once. If you only have 8GB of RAM, your process crashes. Even if it doesn't crash, you've consumed 100GB of memory for a computation that might only need 100MB at any one time.</p><p>This is the fundamental problem generators solve: <strong>lazy evaluation</strong>. Instead of computing all values upfront and storing them in memory, a generator computes the NEXT value only when asked. It uses O(1) memory regardless of the total sequence size.</p><h4>Generator Functions \u2014 The yield Keyword</h4><pre># Regular function: returns a list (loads everything into memory)\ndef read_all_lines(filename):\n    return open(filename).readlines()  # 100GB in RAM if large file!\n\n# Generator function: yields one line at a time (O(1) memory)\ndef stream_lines(filename):\n    with open(filename) as f:\n        for line in f:          # f iterates line by line \u2014 OS handles this\n            yield line.strip()  # pause here, return line, resume on next()\n\n# Usage: process 100GB file with 8GB RAM \u2014 works perfectly\nfor line in stream_lines(\"huge_log.txt\"):\n    process(line)  # only one line lives in memory at any time\n\n# A generator is a function that remembers WHERE IT WAS in execution.\n# \"yield\" means: return this value, pause here, resume from here next time.</pre><h4>Generator Expressions \u2014 The Lazy List Comprehension</h4><pre># List comprehension: creates all values in memory immediately\nsquares_list = [x**2 for x in range(10_000_000)]   # 80MB in memory\n\n# Generator expression: creates values on demand, same syntax with ()\nsquares_gen  = (x**2 for x in range(10_000_000))    # tiny (generator object)\n\n# Both work identically in a for loop:\ntotal = sum(x**2 for x in range(10_000_000))  # never stores all 10M values!\n\n# Rule for data engineering: use generator expression whenever you will\n# iterate through results ONCE (sum, filter, write to file, stream out).\n# Use list when you need to: index into results, iterate multiple times, or check len().</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 ETL Pipeline with Generator Chaining</div><div class=\"rich\"><h4>Real-World DE Pattern: ETL Pipeline with Generators</h4><p>The most powerful use of generators in data engineering: chaining them into a pipeline. Each stage of the ETL is a generator that reads from the previous stage. The entire pipeline runs with O(1) memory \u2014 data flows through one record at a time.</p><pre># ETL Pipeline: read \u2192 parse \u2192 filter \u2192 transform \u2192 write\n# Each function is a generator. Data flows record-by-record.\n\ndef read_csv(filepath):\n    \"\"\"Stage 1: Read file line by line.\"\"\"\n    with open(filepath) as f:\n        next(f)  # skip header\n        for line in f:\n            yield line.strip()\n\ndef parse_rows(lines):\n    \"\"\"Stage 2: Parse CSV text into dicts.\"\"\"\n    for line in lines:  # lines is itself a generator!\n        parts = line.split(\",\")\n        yield {\"user_id\": parts[0], \"amount\": float(parts[1]), \"date\": parts[2]}\n\ndef filter_active(rows):\n    \"\"\"Stage 3: Keep only recent rows.\"\"\"\n    cutoff = \"2024-01-01\"\n    for row in rows:\n        if row[\"date\"] >= cutoff:\n            yield row\n\ndef enrich(rows, lookup):\n    \"\"\"Stage 4: Add metadata from lookup table.\"\"\"\n    for row in rows:\n        row[\"region\"] = lookup.get(row[\"user_id\"], \"unknown\")\n        yield row\n\n# Chain: data flows through all stages lazily, one record at a time\nlookup = build_region_lookup()  # a dict \u2014 O(m) memory\npipeline = enrich(\n    filter_active(\n        parse_rows(\n            read_csv(\"events.csv\")  # 10GB file\n        )\n    ),\n    lookup\n)\n\nfor record in pipeline:  # triggers execution \u2014 pulls one record at a time\n    write_to_warehouse(record)\n# Total memory: O(1) for the pipeline + O(m) for the lookup dict</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Decorators + Context Managers</div><div class=\"rich\"><h4>Decorators \u2014 Functions That Wrap Functions</h4><p>A <strong>decorator</strong> is a function that takes another function as input, wraps it with additional behavior, and returns the wrapped version. The <code>@</code> syntax is shorthand for <code>func = decorator(func)</code>.</p><p>In data engineering: decorators handle retry logic, caching, metrics, logging, and access control \u2014 without cluttering the core business logic.</p><pre>import time\nimport functools\n\n# Decorator: retry an operation up to N times on failure\ndef retry(max_attempts=3, delay=1.0):\n    def decorator(func):\n        @functools.wraps(func)  # preserve original function metadata\n        def wrapper(*args, **kwargs):\n            for attempt in range(1, max_attempts + 1):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    if attempt == max_attempts:\n                        raise  # re-raise on final attempt\n                    print(f\"Attempt {attempt} failed: {e}. Retrying...\")\n                    time.sleep(delay)\n        return wrapper\n    return decorator\n\n# Usage: apply retry behavior to any function with one line\n@retry(max_attempts=5, delay=2.0)\ndef fetch_from_api(endpoint):\n    \"\"\"Fetches data from a flaky external API.\"\"\"\n    response = requests.get(endpoint)\n    response.raise_for_status()\n    return response.json()\n\n# Equivalent to: fetch_from_api = retry(5, 2.0)(fetch_from_api)</pre><h4>Context Managers \u2014 Guaranteed Resource Cleanup</h4><pre># Context manager: database connection that always closes\nfrom contextlib import contextmanager\n\n@contextmanager\ndef db_connection(conn_string):\n    conn = create_connection(conn_string)\n    try:\n        yield conn          # caller gets the connection here\n    finally:\n        conn.close()        # ALWAYS runs, even if exception occurs\n\n# Usage: connection guaranteed to close even on exception\nwith db_connection(\"postgresql://...\") as conn:\n    conn.execute(\"INSERT ...\")</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Memory Profiling + 500GB Pipeline</div><div class=\"rich\"><h4>Memory Profiling and Optimization Strategies</h4><p>In production data engineering, understanding memory at a deep level prevents outages. The three main memory killers in Python DE pipelines:</p><ul><li><strong>Loading entire files/datasets into lists</strong>: Replace with generators + streaming</li><li><strong>Multiple copies of the same string</strong>: Python interns short strings but not long ones \u2014 use string interning or category dtypes in Pandas</li><li><strong>DataFrames with wrong dtypes</strong>: a DataFrame with int64 columns that hold values 0-100 uses 8x more memory than int8</li></ul><pre># Memory profiling with memory_profiler\n# pip install memory-profiler\nfrom memory_profiler import profile\n\n@profile\ndef load_and_process():\n    data = pd.read_csv(\"large.csv\")     # baseline memory\n    # LINE 1: 2.0 GiB\n    data = optimize_dtypes(data)         # shrink dtypes\n    # LINE 2: 0.5 GiB \u2014 75% reduction!\n    return data\n\n# Dtype optimization:\ndef optimize_dtypes(df):\n    for col in df.select_dtypes(\"int64\").columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n    for col in df.select_dtypes(\"float64\").columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n    for col in df.select_dtypes(\"object\").columns:\n        if df[col].nunique() / len(df) < 0.5:  # <50% unique \u2192 category\n            df[col] = df[col].astype(\"category\")\n    return df\n\n# Processing chunks: alternative to generators for files too large for memory\nfor chunk in pd.read_csv(\"huge.csv\", chunksize=100_000):\n    process(chunk)  # process 100K rows at a time</pre><p><strong>FAANG interview question:</strong> \"Your pipeline processes 500GB of logs daily. Memory on your EC2 instance is 32GB. Walk me through how you would process this.\" Correct answer: generators + chunked reading + streaming writes, never load the full dataset simultaneously.</p></div></div></div>",
        "KeyConcepts": [
            "Generator function: uses 'yield' to pause/resume. Returns a generator object, not a list.",
            "Generator uses O(1) memory: it never holds all values at once \u2014 computes next value on demand.",
            "Generator expression: (expr for x in iterable) \u2014 identical syntax to list comp but lazy.",
            "Pipeline chaining: each ETL stage is a generator reading from the previous. Data flows one record at a time.",
            "Decorator: higher-order function that wraps another. '@retry' adds retry logic without changing core code.",
            "@functools.wraps preserves function name/docstring when writing decorators.",
            "Context manager: 'with' block guarantees cleanup (finally) even on exceptions.",
            "Memory optimization: dtype downcast (int64\u2192int8), category dtype for low-cardinality strings."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write a generator that reads a CSV file line by line and yields parsed dict rows. Compare memory usage vs reading the full file into a list.",
            "<strong>Step 2:</strong> Build a 3-stage generator pipeline: read \u2192 filter (only rows where amount > 100) \u2192 transform (add a 'revenue_tier' field). Process a 1M-row file.",
            "<strong>Step 3:</strong> Write a @retry(max_attempts=3, delay=1) decorator. Apply it to a function that simulates random network failures. Verify it retries correctly.",
            "<strong>Step 4:</strong> Write optimize_dtypes(df). Apply it to a DataFrame with int64 and object columns. Report memory usage before and after with df.memory_usage(deep=True).sum()."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Explain OOM: Java Heap Space vs Overhead",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Explain OOM: Java Heap Space vs Overhead \u2014 trace through 2-3 edge cases before writing any code.",
                "Can you iterate through results with a single for loop? \u2192 use generator (saves memory).",
                "Need to index into results, get len(), or iterate multiple times? \u2192 convert to list first.",
                "Chunked pandas: pd.read_csv('file.csv', chunksize=100_000) processes file in chunks without full load.",
                "Generators are lazy \u2014 they do NOTHING until iterated. Chaining generators costs zero compute up front."
            ]
        },
        "HardProblem": "Boss Problem (Netflix): Your nightly ETL reads from an S3 bucket containing 50,000 gzipped JSON files (total ~500GB uncompressed). Each file has user viewing events. (1) Write a generator pipeline: stream S3 object list \u2192 download each file \u2192 gunzip \u2192 parse JSON lines \u2192 filter by date \u2192 yield events. The pipeline should use <2GB RAM regardless of total file count. (2) For each event, do a dict lookup against a 20M-row user metadata table loaded in RAM. What is the maximum size this lookup dict can be? (3) If processing takes 14 hours, how do you parallelize using multiprocessing.Pool while keeping memory bounded?"
    },
    {
        "Week": 7,
        "Day": "Saturday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Spark Internals",
        "SpecificTopic": "Lab Day",
        "ActionItem_Deliverable": "Crash a Databricks notebook on purpose to see logs",
        "LeetCodeProblem": "<strong>LC 79 \u2013 Word Search</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Mock Assessment + Mock #1</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 What Spark Is \u2014 Driver/Executor + Lazy Eval</div><div class=\"rich\"><h4>What Spark Actually Is \u2014 And Why It Exists</h4><p>Apache Spark was created in 2009 at UC Berkeley's AMPLab as a faster replacement for Hadoop MapReduce. Hadoop MapReduce wrote intermediate results to disk after every step. A 10-step pipeline wrote and read disk 10 times \u2014 massive I/O overhead. Spark keeps intermediate results <strong>in memory</strong>, passing data between stages without disk. For iterative algorithms (machine learning) and multi-step pipelines, this is 10\u2013100x faster.</p><h4>How Spark Executes Code \u2014 The Driver/Executor Model</h4><pre>Spark Cluster Architecture:\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              DRIVER (your Python code)          \u2502\n\u2502  - Runs on the master node                      \u2502\n\u2502  - Builds the execution plan                    \u2502\n\u2502  - Sends tasks to executors                     \u2502\n\u2502  - Collects results                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502 (sends serialized tasks)\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u25bc              \u25bc              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Executor \u2502  \u2502Executor \u2502  \u2502Executor \u2502\n\u2502 - Node 1\u2502  \u2502 - Node 2\u2502  \u2502 - Node 3\u2502\n\u2502 - tasks \u2502  \u2502 - tasks \u2502  \u2502 - tasks \u2502\n\u2502 - cache \u2502  \u2502 - cache \u2502  \u2502 - cache \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n# DataFrame operations run on executors in parallel\n# Driver only sees final results (unless you call .collect() \u2014 dangerous!)</pre><h4>Lazy Evaluation \u2014 Nothing Runs Until You Ask</h4><p>Spark is lazy: DataFrame transformations (filter, select, join, groupBy) do NOT execute immediately. They build a <strong>query plan</strong>. Only <strong>actions</strong> (collect(), count(), write(), show()) trigger actual execution.</p><pre>df = spark.read.parquet(\"s3://events/\")   # no I/O yet\ndf2 = df.filter(df.year == 2024)           # no I/O yet \u2014 builds plan\ndf3 = df2.groupBy(\"country\").count()       # no I/O yet \u2014 extends plan\n\ndf3.show()  # \u2190 ACTION: NOW Spark executes the entire plan\n\n# Why lazy? Spark's optimizer (Catalyst) can rearrange operations:\n# - Pushes filters before joins (read less data)\n# - Eliminates unused columns early\n# - Merges multiple transformations into single scan\n# This often makes your unoptimized code run faster automatically.</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 The Shuffle + Broadcast Joins</div><div class=\"rich\"><h4>The Shuffle \u2014 The Most Expensive Spark Operation</h4><p>A <strong>shuffle</strong> occurs when data needs to be redistributed across executors based on a key \u2014 for example, during a <code>groupBy</code>, <code>join</code>, or <code>repartition</code>. Shuffle is expensive because: (1) executors must write their partition to disk, (2) data is sent over the network, (3) receiving executors read from disk and memory. In a job with 100 executors processing 1TB, a shuffle can move terabytes over the network.</p><pre># Operations that trigger a shuffle:\ndf.groupBy(\"country\").count()            # \u2190 SHUFFLE: all rows for same country\n                                         #   must land on same executor\ndf.join(other, \"user_id\")               # \u2190 SHUFFLE: matching user_ids must\n                                         #   be co-located\ndf.repartition(200, \"country\")          # \u2190 SHUFFLE: explicit redistribution\ndf.distinct()                            # \u2190 SHUFFLE: deduplicate requires global view\n\n# Operations that do NOT shuffle (cheap \"narrow\" transformations):\ndf.filter(df.year == 2024)              # stays on same partition\ndf.select(\"user_id\", \"amount\")          # stays on same partition\ndf.withColumn(\"total\", df.a + df.b)    # stays on same partition\ndf.coalesce(10)                         # merge partitions, no redistribution\n\n# Reduce shuffles by:\n# 1. Partition data by join key BEFORE the join (pre-partition)\n# 2. Broadcast small tables (eliminates join shuffle entirely)\n# 3. Aggregate before joining (reduce data volume before shuffle)</pre><h4>Broadcast Join \u2014 Eliminating the Join Shuffle</h4><pre># Problem: joining a 100TB fact table with a 50MB dimension\n# Default join \u2192 shuffle 100TB over network (catastrophically slow)\n\n# Solution: broadcast the small table to ALL executors\n# Each executor gets a full copy \u2014 no network movement for the large table\n\nfrom pyspark.sql.functions import broadcast\n\nresult = large_df.join(\n    broadcast(small_dim_df),  # \u2190 small table copied to all executors\n    on=\"product_id\"\n)  # No shuffle of large_df! Each executor joins locally.\n\n# Spark auto-broadcasts tables < spark.sql.autoBroadcastJoinThreshold (default 10MB)\n# Increase for larger dimensions if they fit in executor memory:\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"100m\")</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Data Skew + Partitions vs Tasks</div><div class=\"rich\"><h4>Data Skew \u2014 When One Executor Does 90% of the Work</h4><p><strong>Data skew</strong> is the #1 production Spark performance problem. It occurs when one partition key value has far more rows than others. Example: a NULL user_id for all guest orders means the \"NULL\" partition gets 80% of all rows. One executor processing 8TB while others process 100GB \u2014 the job is only as fast as the slowest executor.</p><pre># Detect skew: check partition sizes\ndf.groupBy(spark_partition_id()).count().show()  # see partition row counts\n# If one partition has 100M rows and others have 1M \u2192 skew\n\n# Fix 1: Salt the key (add random suffix to distribute skewed keys)\nfrom pyspark.sql import functions as F\n\n# Add salt to the large table\nnum_salt_buckets = 10\nsalted_large = large_df.withColumn(\n    \"salted_key\",\n    F.concat(F.col(\"user_id\"), F.lit(\"_\"), (F.rand() * num_salt_buckets).cast(\"int\"))\n)\n\n# Explode the small table to match all salted keys\nsalt_values = spark.range(num_salt_buckets).withColumnRenamed(\"id\", \"salt\")\nsalted_small = small_df.crossJoin(salt_values).withColumn(\n    \"salted_key\",\n    F.concat(F.col(\"user_id\"), F.lit(\"_\"), F.col(\"salt\"))\n)\n\n# Now join on salted_key \u2014 skew is evenly distributed!\nresult = salted_large.join(salted_small, \"salted_key\")\n\n# Fix 2: Filter skewed keys and process separately\nnormal = df.filter(F.col(\"user_id\").isNotNull())\nnulls  = df.filter(F.col(\"user_id\").isNull())\n# Process each differently, then union</pre><h4>Partitions vs Tasks \u2014 Understanding Parallelism</h4><p>In Spark, a <strong>partition</strong> is a chunk of data. A <strong>task</strong> processes one partition. The number of parallel tasks = number of executor cores. If you have 100 cores and 1,000 partitions \u2192 100 tasks run simultaneously, 10 rounds of 100 tasks = 10 rounds total.</p><pre># Rule of thumb: 2-4 partitions per CPU core\n# 100 cores \u2192 200-400 partitions\n\n# Too few partitions: executors are idle (not enough work to distribute)\n# Too many partitions: per-partition overhead dominates (too many small tasks)\n\ndf.rdd.getNumPartitions()  # check current partition count\ndf.repartition(400)  # redistribute to 400 partitions (shuffle)\ndf.coalesce(10)      # reduce to 10 partitions (no shuffle, but may be uneven)\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"400\")  # for groupBy/join output</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Diagnosing a Slow Spark Job</div><div class=\"rich\"><h4>FAANG Interview: Diagnosing a Slow Spark Job</h4><p>Interview question: \"Your Spark job processes 10TB of data but takes 6 hours. Walk me through diagnosing it.\"</p><pre>Step 1 \u2014 Open the Spark UI (port 4040)\n  \u2192 DAG tab: which stage takes the most time?\n  \u2192 Stage details: which task is the slowest? (skew indicator)\n  \u2192 Storage tab: how much data is cached? Is spill occurring?\n\nStep 2 \u2014 Identify the type of slowdown:\n\n  A. One stage takes 90% of job time:\n     \u2192 Check if it contains a shuffle (groupBy, join)\n     \u2192 If so: can you reduce data before the shuffle? Broadcast the smaller side?\n\n  B. One task in a stage takes 10x longer than others:\n     \u2192 Data skew. Check: what is the key distribution?\n     \u2192 Fix: salt the key, or handle skewed keys separately\n\n  C. Spill to disk (red in Spark UI):\n     \u2192 Executor ran out of memory, wrote shuffle data to disk\n     \u2192 Fix: increase executor memory, or increase spark.sql.shuffle.partitions\n       (more partitions = smaller per-partition size = less memory per partition)\n\n  D. Many stages, each fast individually but slow overall:\n     \u2192 Wide dependency chain \u2014 hard to parallelize\n     \u2192 Fix: checkpoint intermediate results, cache frequently reused DataFrames\n\ndf.cache()  # cache in memory \u2014 avoid recomputing in subsequent actions\ndf.persist(StorageLevel.DISK_ONLY)  # persist to disk if too large for memory</pre><p>\u270d\ufe0f <strong>The Spark UI is your most important debugging tool.</strong> FAANG interviewers expect you to know how to read it and what each metric means.</p></div></div></div>",
        "KeyConcepts": [
            "Spark = distributed in-memory computing. Driver builds plan, executors execute tasks on partitions.",
            "Lazy evaluation: transformations build a plan. Actions (count/show/write) trigger execution.",
            "Catalyst optimizer: rearranges your query plan for efficiency \u2014 predicate pushdown, column pruning.",
            "Shuffle: data redistribution across network. Most expensive operation. Triggered by groupBy, join, distinct.",
            "Narrow transformation: each partition maps to exactly one output partition. No shuffle. Fast.",
            "Broadcast join: copies small table to all executors. Eliminates join shuffle for large+small joins.",
            "Data skew: one partition key has disproportionate rows. Fix: salting (randomize key) or separate processing.",
            "Partitions = data chunks. Tasks = work units (1 per partition). Target 2-4 partitions per core.",
            "spark.sql.shuffle.partitions default = 200. Tune to 2-4x your number of cores."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Read 10M rows. Chain: filter \u2192 groupBy \u2192 count. Use .explain(True) to show the physical plan. Identify which operations cause shuffles.",
            "<strong>Step 2:</strong> Join a 1B-row fact table with a 5MB lookup table. Compare: regular join vs broadcast join. Measure time difference with spark.time().",
            "<strong>Step 3:</strong> Simulate data skew: create a dataset where 80% of rows have user_id=NULL. Run a groupBy. Observe one task taking 10x longer. Apply the salt fix.",
            "<strong>Step 4:</strong> Set spark.sql.shuffle.partitions to 10, 200, 1000 on the same groupBy query. Observe partition count and execution time changes."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Crash a Databricks notebook on purpose to see logs",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Crash a Databricks notebook on purpose to see logs \u2014 trace through 2-3 edge cases before writing any code.",
                "Seeing one stage taking 90% of job time? Check for shuffle. Can you broadcast the smaller side?",
                "One task running 10x longer than others? Data skew. Check key value distribution with groupBy + count.",
                "Spill to disk in Spark UI? Executor out of memory. Increase executor memory or spark.sql.shuffle.partitions.",
                "Cache DataFrames reused multiple times: df.cache(). Unpersist when done: df.unpersist()."
            ]
        },
        "HardProblem": "Boss Problem (Uber): Your daily Spark job aggregates 500B GPS ping events (5TB) to compute driver earnings per city. It runs in 14 hours but needs to run in 2 hours. Steps: (1) You find 3 shuffle stages dominate. How do you investigate each one? (2) One join between pings (5TB) and a driver_metadata table (2GB) causes a 3TB shuffle. How do you fix it without salting? (3) City='NULL' has 60% of all pings (GPS loss). How do you handle this skew? (4) The job reads the same 5TB raw data for 4 different aggregations. How do you avoid reading it 4 times?"
    },
    {
        "Week": 7,
        "Day": "Sunday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Rest",
        "SpecificTopic": "Rest & Review",
        "ActionItem_Deliverable": "Review Spark UI tabs (Stages/Tasks)",
        "LeetCodeProblem": null,
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Recursive Parsing</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Active Rest \u2014 How Memory Consolidates</div>\n<div class=\"rich\">\n<h4>The Neuroscience of Spaced Repetition</h4>\n<p>Rest days are not wasted days. During rest, your brain replays what it learned \u2014 a process called <strong>memory consolidation</strong>. Research consistently shows that <em>attempting to recall from memory</em> is 3\u00d7 more effective at building durable knowledge than re-reading notes. The act of struggling to remember \u2014 even failing \u2014 strengthens the neural pathway more than passive review.</p>\n<p>Think of it like muscle recovery: a weightlifter who trains 7 days straight without rest gets weaker, not stronger. The growth happens during recovery. Your brain is the same.</p>\n<h4>Today's Protocol (4 hours, structured)</h4>\n<ol>\n  <li><strong>Active Recall (60 min):</strong> Close everything. On a blank page, write every concept, formula, and pattern from this week from memory. Then check.</li>\n  <li><strong>Error Review (60 min):</strong> Find the hardest problem you struggled with. Attempt it again completely from scratch \u2014 no notes.</li>\n  <li><strong>Teach It (45 min):</strong> Open the AI chatbot and explain Gaps & Islands to it as if it's a junior engineer. If you stumble, that's your gap.</li>\n  <li><strong>Next Week Preview (15 min):</strong> Skim next Monday's topic at headline level only \u2014 priming improves retention when you study it deeply.</li>\n</ol>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Active Recall Quiz \u2014 No Notes Allowed</div>\n<div class=\"rich\">\n<h4>Answer These From Memory. Then Check.</h4>\n<pre>Window Functions:\n\u25a1 What is the difference between ROW_NUMBER, RANK, and DENSE_RANK?\n  (Write the output for a table with two tied rows)\n\u25a1 Write the syntax for a 7-day rolling average from memory\n\u25a1 What does PARTITION BY do differently from GROUP BY?\n\u25a1 Write the complete Gaps & Islands CTE from memory\n\u25a1 What is the difference between ROWS BETWEEN and RANGE BETWEEN?\n\u25a1 LAG(col, 1, 0) vs LAG(col, 1) \u2014 what is different?\n\u25a1 In what SQL execution order step do window functions run?\n\u25a1 Why can't you write WHERE rank &lt;= 3 in the same query as ROW_NUMBER?\n\nScore yourself:\n8/8 = \ud83c\udfc6 Mastery level \u2014 you're ready for interviews on this\n5-7/8 = \ud83d\udfe1 Good \u2014 review the 1-2 concepts you missed\n&lt; 5/8 = \ud83d\udd34 Go back to the specific day's content, re-read Level 1-2</pre>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Spaced Repetition Schedule</div>\n<div class=\"rich\">\n<h4>When to Review This Week's Material</h4>\n<table>\n<tr><th>Review this content</th><th>When</th><th>How</th></tr>\n<tr><td>This week</td><td>Today</td><td>Active recall quiz above</td></tr>\n<tr><td>This week</td><td>In 3 days</td><td>Re-solve the hard problem from Day 4</td></tr>\n<tr><td>This week</td><td>In 7 days</td><td>LeetCode: solve problems 185, 180, 196</td></tr>\n<tr><td>This week</td><td>In 30 days</td><td>Explain all 4 concepts to someone else</td></tr>\n</table>\n<h4>The Feynman Technique</h4>\n<p>After studying any concept, close your notes and explain it as if you're teaching a newcomer. Use only simple words. Avoid jargon. If you stumble, that's exactly where your understanding breaks down \u2014 return to just that part. This technique built Richard Feynman's legendary ability to explain complex physics simply.</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Interview Readiness Self-Assessment</div>\n<div class=\"rich\">\n<pre>Rate yourself 1\u20135 on each topic (honest self-assessment):\n  1 = \"I need to re-read from scratch\"\n  3 = \"I understand but need to look at notes\"\n  5 = \"I can code this under pressure from memory\"\n\nROW_NUMBER / RANK / DENSE_RANK:        ___/5\nRolling windows (ROWS BETWEEN):        ___/5\nLAG/LEAD (adjacent row comparison):    ___/5\nGaps & Islands (date streaks):         ___/5\nSession detection (timestamp version): ___/5\n\nAction:\n  Score &lt;3: revisit that day's Level 1 and Level 2 content tomorrow\n  Score 3:  do one LeetCode problem on that specific topic\n  Score 5:  you're interview-ready \u2705 on this topic</pre>\n<h4>FAANG Interview Reality Check</h4>\n<p>At meta, Amazon, and Google data engineering interviews, you'll be asked to write SQL live on a shared screen without autocomplete or documentation. The most common reason candidates fail: they know the concept but freeze on the syntax under pressure. The fix: write every pattern from memory at least 5 times this week. Muscle memory is more reliable than conscious recall under stress.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Memory consolidation happens during rest \u2014 the brain replays and stabilizes learned patterns during downtime.",
            "Active recall (testing yourself) builds 3x stronger memory than re-reading notes.",
            "Spaced repetition: review at day 1, day 3, day 7, day 30 for maximum retention.",
            "The Feynman Technique: if you can't explain it simply, you don't fully understand it yet.",
            "Error analysis: the problems you got WRONG are your highest-value review items \u2014 spend the most time there."
        ],
        "Tasks": [
            "<strong>Active Recall:</strong> Write the Gaps & Islands CTE from memory on paper. Check it against Day 4.",
            "<strong>Re-attempt:</strong> Pick the hardest problem from this week and solve it in 20 minutes from scratch.",
            "<strong>Teach it:</strong> Explain RANK vs DENSE_RANK to the AI chatbot as if it's a junior. Get it right without notes.",
            "<strong>Preview:</strong> Skim next week's Monday topic for 10 minutes \u2014 just headline concepts."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Review Spark UI tabs (Stages/Tasks)",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Review Spark UI tabs (Stages/Tasks) \u2014 trace through 2-3 edge cases before writing any code.",
                "Use the AI chatbot to quiz yourself \u2014 ask it 'Give me a SQL window function interview question.'",
                "Write SQL from memory in a plain text editor \u2014 no autocomplete. This builds true recall.",
                "The hardest concept this week: spend 30 minutes writing a full explanation in your own words."
            ]
        },
        "HardProblem": "Connect-the-dots: Write 2-3 paragraphs explaining how ROW_NUMBER, rolling averages, LAG, and Gaps & Islands could ALL be used in a single production pipeline for a ride-sharing company. What problem does each solve? How do they complement each other? What would break if you removed any one of them?"
    },
    {
        "Week": 8,
        "Day": "Monday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Data Modeling",
        "SpecificTopic": "SCD Type 2",
        "ActionItem_Deliverable": "Write MERGE logic for History Tracking on whiteboard",
        "LeetCodeProblem": "<strong>LC 731 \u2013 My Calendar II</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Data Formats</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 The Problem and Three Strategies</div><div class=\"rich\"><h4>The Business Problem That Makes SCDs Hard</h4><p>You've built a star schema. dim_customer has customer_id, name, city, loyalty_tier. Your boss asks: \"Show revenue from gold-tier customers last year \u2014 but I want the tier they had <em>AT THE TIME OF PURCHASE</em>, not what tier they have today.\"</p><p>This reveals the fundamental problem: <strong>the real world changes over time</strong>. Customers move cities. Products change categories. If you simply UPDATE a dimension row, you permanently lose the history. Yesterday's record no longer exists. Historical questions become unanswerable.</p><p>This is the <strong>Slowly Changing Dimension (SCD)</strong> problem. Three strategies dominate:</p><ul><li><strong>SCD Type 1:</strong> Overwrite. No history kept. Simple but lossy.</li><li><strong>SCD Type 2:</strong> Add a new row per change. Full history. The gold standard.</li><li><strong>SCD Type 3:</strong> Add a \"previous value\" column. One level of history.</li></ul><p>\u270d\ufe0f <strong>In 95% of FAANG interviews and DW jobs, SCD means Type 2. Know this one deeply.</strong></p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Type 1, Type 2, Type 3 \u2014 Full Examples</div><div class=\"rich\"><h4>SCD Type 1: Overwrite \u2014 Simple but History-Destroying</h4><pre>-- Alice moves from Boston to Dallas:\nUPDATE dim_customer SET city = 'Dallas' WHERE customer_id = 42;\n-- \u26a0\ufe0f  Boston is GONE. All historical orders now falsely show Dallas.\n-- Use Type 1 ONLY for data corrections (typos) \u2014 never for real changes.</pre><h4>SCD Type 2: New Row Per Change \u2014 Full History Preserved</h4><pre>CREATE TABLE dim_customer (\n  customer_key   INT PRIMARY KEY,  -- surrogate: changes with each version\n  customer_id    INT,              -- natural key: stays the same always\n  name           VARCHAR(100),\n  city           VARCHAR(100),\n  loyalty_tier   VARCHAR(20),\n  effective_start DATE NOT NULL,\n  effective_end   DATE,            -- NULL = currently active\n  is_current      BOOLEAN DEFAULT TRUE\n);\n\n-- Alice starts in Boston, Silver tier:\nINSERT INTO dim_customer VALUES (1001,42,'Alice','Boston','Silver','2023-01-01',NULL,TRUE);\n\n-- Alice moves to Dallas on March 15, 2024:\n-- Step 1: Close old row\nUPDATE dim_customer SET effective_end='2024-03-15', is_current=FALSE\nWHERE customer_id=42 AND is_current=TRUE;\n\n-- Step 2: Insert new row (NEW surrogate key!)\nINSERT INTO dim_customer VALUES (1087,42,'Alice','Dallas','Silver','2024-03-15',NULL,TRUE);\n\n-- Alice now has TWO rows:\n-- key=1001: Boston, Silver, 2023-01-01 \u2192 2024-03-15 (historical)\n-- key=1087: Dallas, Silver, 2024-03-15 \u2192 NULL (current)\n\n-- fact_sales rows from 2023 \u2192 customer_key=1001 (Boston)\n-- fact_sales rows from Apr 2024 \u2192 customer_key=1087 (Dallas)\n-- Revenue by city is now historically accurate \u2705</pre><h4>SCD Type 3: Previous Value Column \u2014 One Level Only</h4><pre>ALTER TABLE dim_customer ADD COLUMN prev_city VARCHAR(100);\nUPDATE dim_customer SET prev_city=city, city='Dallas' WHERE customer_id=42;\n-- city=Dallas, prev_city=Boston\n-- \u26a0\ufe0f  If Alice moves again to Austin: prev_city=Dallas, Boston is gone.\n-- Use for: planned reorgs where you need both old+new for 6 months only.</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Query Patterns and ETL Pipeline</div><div class=\"rich\"><h4>Three Essential SCD Type 2 Query Patterns</h4><p><strong>Pattern 1: Current state</strong></p><pre>SELECT customer_id, name, city, loyalty_tier\nFROM dim_customer WHERE is_current = TRUE;</pre><p><strong>Pattern 2: Historical accuracy \u2014 tier at time of purchase</strong></p><pre>-- Revenue by tier AT time of sale (not current tier)\nSELECT c.loyalty_tier, SUM(f.total_revenue)\nFROM fact_sales f\nJOIN dim_customer c ON f.customer_key = c.customer_key  -- direct FK!\nJOIN dim_date d ON f.date_key = d.date_key\nWHERE d.year = 2023\nGROUP BY c.loyalty_tier;\n-- Works automatically: fact FK already points to the right version</pre><p><strong>Pattern 3: Point-in-time</strong> \u2014 what tier was customer 42 on Feb 1, 2024?</p><pre>SELECT customer_id, name, city, loyalty_tier\nFROM dim_customer\nWHERE customer_id = 42\n  AND effective_start <= '2024-02-01'\n  AND (effective_end > '2024-02-01' OR effective_end IS NULL);</pre><h4>The SCD ETL Pipeline \u2014 How Nightly Loads Work</h4><ol><li>Extract: pull changed records from OLTP (via CDC \u2014 Change Data Capture)</li><li>Compare: for each changed record, compare to current dim row. Any tracked attribute changed?</li><li>Expire: UPDATE old row \u2014 set effective_end = today, is_current = FALSE</li><li>Insert: INSERT new row with new surrogate key and new attribute values</li><li>No-change: skip records with unchanged tracked attributes entirely</li></ol></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Bi-temporal + Late-Arriving Facts</div><div class=\"rich\"><h4>Bi-temporal Modeling \u2014 Two Timelines</h4><p>SCD Type 2 tracks ONE timeline: when did the attribute change in our database? Bi-temporal modeling tracks TWO: <strong>valid time</strong> (when true in reality) vs <strong>transaction time</strong> (when our system recorded it). A customer might move cities Jan 1 but we don't record it until Jan 15 \u2014 these differ.</p><pre>CREATE TABLE dim_customer_bitemporal (\n  customer_key    INT PRIMARY KEY,\n  customer_id     INT,\n  city            VARCHAR(100),\n  -- Axis 1: when true in the real world\n  valid_start     DATE,\n  valid_end       DATE,\n  -- Axis 2: when our system recorded this\n  recorded_start  TIMESTAMP,\n  recorded_end    TIMESTAMP\n);\n-- Enables: \"What did our SYSTEM BELIEVE on Jan 10 about Jan 1's state?\"\n-- Critical for: auditing, compliance, retroactive corrections</pre><h4>Late-Arriving Facts \u2014 The Night Shift Problem</h4><p>A mobile app logs an event offline, connects to WiFi 3 days later. The event arrives Day+3 but the dimension may have changed in those 3 days. Load the fact by joining to the dimension version active on the ORIGINAL event date.</p><pre>INSERT INTO fact_sales (customer_key, ...)\nSELECT c.customer_key, ...\nFROM source_events e\nJOIN dim_customer c\n  ON c.customer_id = e.customer_id\n  AND e.event_date BETWEEN c.effective_start AND COALESCE(c.effective_end, '9999-12-31');</pre></div></div></div>",
        "KeyConcepts": [
            "SCD = Slowly Changing Dimension. Strategy for handling dimension attribute changes over time.",
            "SCD Type 1: overwrite. Simple, destroys history. Use only for data corrections (typos).",
            "SCD Type 2: new row per change. Full history preserved. The industry standard.",
            "SCD Type 2 columns: surrogate key (changes), natural key (constant), effective_start, effective_end, is_current.",
            "Fact table FK to surrogate key automatically delivers historical accuracy in all joins.",
            "SCD Type 3: previous_value column. One level of history. Use for planned reorgs, not permanent tracking.",
            "Point-in-time query: WHERE natural_key=X AND start<=date AND (end>date OR end IS NULL).",
            "Bi-temporal: valid time (real world) + transaction time (recorded). For auditing and late corrections.",
            "Late-arriving facts: join to dimension version active on the original event date, not current."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Create dim_customer with SCD Type 2 columns. Insert one customer (Boston, Silver tier).",
            "<strong>Step 2:</strong> Simulate a tier upgrade (Silver \u2192 Gold). Write the UPDATE + INSERT correctly.",
            "<strong>Step 3:</strong> Write a point-in-time query: 'What tier was customer 42 on March 1, 2024?' \u2014 dates span before and after the change.",
            "<strong>Step 4:</strong> A late-arriving fact arrives with event_date=2023-11-15. Write the INSERT that joins to the correct historical dimension version."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Write MERGE logic for History Tracking on whiteboard",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Write MERGE logic for History Tracking on whiteboard \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: 'Which SCD type?' \u2014 almost always Type 2. Explain why Type 1 loses history.",
                "Surrogate key changes with each new version row. Natural/business key stays the same across all versions.",
                "Current records: WHERE is_current=TRUE or WHERE effective_end IS NULL. Both work.",
                "MERGE statement (UPSERT) handles the SCD Type 2 ETL expire+insert in a single SQL command."
            ]
        },
        "HardProblem": "Boss Problem (Spotify): 50M users, average 4 tier changes over 3 years. (1) Design dim_user with SCD Type 2. (2) A user can change country too \u2014 when both tier AND country change on the same day, do you create 1 or 2 new rows? Why? (3) Write a MERGE statement that handles insert/update(expire+insert)/no-change in one SQL command. (4) If 200K users change tier every day, how does this affect fact table FK updates?"
    },
    {
        "Week": 8,
        "Day": "Tuesday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Data Modeling",
        "SpecificTopic": "Fact Tables",
        "ActionItem_Deliverable": "Design Fact table for User Clicks (Granularity check)",
        "LeetCodeProblem": "<strong>LC 1094 \u2013 Car Pooling</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Joins Deep Dive</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 The History \u2014 Why This Exists</div><div class=\"rich\"><h4>Why Dimensional Modeling Exists \u2014 The History</h4><p>In the 1960s databases were built for transactions: record an order, update inventory, log a payment. Every table was tightly normalized. Queries were simple lookups. IBM's relational model was perfect for this.</p><p>Then businesses wanted <em>analytics</em>. Not \"what is this customer's balance?\" but \"how did western-region revenue compare to last year, by product category and month?\" These questions required joining 8-12 tables, aggregating millions of rows, slicing across multiple dimensions simultaneously. Normalized OLTP schemas were terrible at this \u2014 joins were slow, SQL was unreadable, answers took hours.</p><p><strong>Ralph Kimball's insight (1990s):</strong> every business question has the same structure \u2014 \"How much [measure] by [dimension] by [dimension]?\" He designed the <strong>Star Schema</strong>: one central fact table of numbers, surrounded by dimension tables of context. The shape is a star. It mirrors how humans think about data.</p><pre>           DIM_PRODUCT             DIM_DATE\n           (category, brand)       (year, month, quarter)\n                    \\\\              /\n       DIM_STORE \u2500\u2500 FACT_SALES \u2500\u2500 DIM_CUSTOMER\n       (city,region)  (revenue,   (segment, age)\n                       units,\n                       discount)\n</pre><p>\u270d\ufe0f <strong>Core rule:</strong> Fact tables contain MEASUREMENTS (numbers you aggregate: revenue, clicks, quantity). Dimension tables contain CONTEXT (who, what, where, when). Quick test: \"Can I SUM this column?\" \u2014 yes \u2192 fact/measure, no \u2192 dimension/attribute.</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Building a Star Schema</div><div class=\"rich\"><h4>Building a Star Schema \u2014 Step by Step</h4><p>The fact table contains one row per business event \u2014 one sale, one click, one payment. It has foreign keys to every dimension (who/what/where/when) and numeric measures.</p><pre>CREATE TABLE fact_sales (\n  sale_id      BIGINT PRIMARY KEY,\n  date_key     INT REFERENCES dim_date(date_key),       -- WHEN\n  product_key  INT REFERENCES dim_product(product_key), -- WHAT\n  store_key    INT REFERENCES dim_store(store_key),      -- WHERE\n  customer_key INT REFERENCES dim_customer(customer_key),-- WHO\n  quantity_sold INT,\n  unit_price    DECIMAL(10,2),\n  total_revenue DECIMAL(10,2)   -- additive: SUM across all dims\n);</pre><h4>The Date Dimension \u2014 Why It's Special</h4><p>The date dimension is pre-populated for 10 years (3,650 rows). It stores year, quarter, month, week, day, holiday/weekend flags so analysts can <code>GROUP BY d.quarter</code> without ever calling EXTRACT(). It is the most important and most universally present dimension in any data warehouse.</p><pre>CREATE TABLE dim_date (\n  date_key    INT PRIMARY KEY,   -- e.g. 20240115\n  full_date   DATE,\n  year INT, quarter INT, month INT, month_name VARCHAR(20),\n  is_holiday BOOLEAN, is_weekend BOOLEAN\n);</pre><p><strong>Surrogate keys:</strong> We use simple integer PKs in dimension tables, not natural business keys. If an upstream product ID format changes, only the ETL mapping changes \u2014 not the entire fact table.</p></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Star vs Snowflake + Grain</div><div class=\"rich\"><h4>Star vs Snowflake Schema \u2014 When to Normalize Dimensions</h4><p><strong>Star schema:</strong> dimension tables are denormalized \u2014 category, subcategory, and brand all live in one flat product dimension. Some string repetition, but JOIN queries are simple (one join per dimension).</p><p><strong>Snowflake schema:</strong> dimension tables are normalized \u2014 dim_product \u2192 dim_subcategory \u2192 dim_category, each linked by FK. Less redundancy, but every analytical query needs 3 joins to get from a sale to its category.</p><table><tr><th>Aspect</th><td>Star Schema</td><td>Snowflake Schema</td></tr><tr><th>Storage</th><td>More (repeated strings)</td><td>Less (normalized)</td></tr><tr><th>Query complexity</th><td>1 join per dimension</td><td>3\u20134 joins per dimension chain</td></tr><tr><th>Query speed</th><td>Faster (fewer joins)</td><td>Slower on large data sets</td></tr><tr><th>FAANG preference</th><td>\u2705 Almost universal</td><td>\u274c Rare \u2014 only for very large dims</td></tr></table><h4>Grain \u2014 The Most Critical Design Decision</h4><p>A fact table's <strong>grain</strong> is the precise definition of what one row represents. Getting grain wrong is the most expensive modeling mistake \u2014 nearly impossible to fix without rebuilding.</p><pre>Grains for sales data:\n  \u2705 \"One row per line item in a sales transaction\" (finest grain)\n  \u2705 \"One row per sales receipt\"\n  \u2705 \"One row per day per store total\"\n  \u274c \"One row per transaction... sometimes per day\" \u2014 MIXED grain!\n\nMixed grain: SUM(revenue) double-counts. Every analyst gets different numbers.\nTrust in the data warehouse collapses.</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 FAANG Interview Framework</div><div class=\"rich\"><h4>The Interview Framework: Designing a Data Warehouse From Scratch</h4><p>FAANG interview: \"Design a data warehouse for an e-commerce company.\" The interviewer scores: (1) grain identified first, (2) facts vs dimensions correct, (3) SCD strategy named, (4) query pattern driven design.</p><pre>Step 1 \u2014 Ask clarifying questions:\n  \"What questions will analysts need to answer?\"\n  \"What is the finest level of detail needed? Line item or order?\"\n  \"How often does product/customer data change?\"\n  \"Expected data volume \u2014 rows per day?\"\n\nStep 2 \u2014 State the grain explicitly:\n  \"One row per order line item\"\n\nStep 3 \u2014 Identify measures:\n  quantity, unit_price, discount, line_total, margin\n\nStep 4 \u2014 Identify dimensions:\n  dim_date (when), dim_customer (who), dim_product (what),\n  dim_store (where), dim_promotion (why discounted)\n\nStep 5 \u2014 Handle attribute changes:\n  \"Products change prices, customers move \u2014 SCD Type 2 on both\"\n\nStep 6 \u2014 Plan aggregations:\n  \"Daily dashboard sums pre-aggregated in agg_daily_sales\n   so we don't scan the 10B-row fact table on every request\"</pre></div></div></div>",
        "KeyConcepts": [
            "Star schema = fact table (measures) surrounded by dimension tables (context). Optimized for analytical queries.",
            "Fact table: events (sales, clicks). Contains FKs to dimensions + numeric measures to aggregate.",
            "Additive measures: SUM across all dims (revenue). Semi-additive: only across some. Non-additive: never SUM (ratios).",
            "Surrogate keys: simple integer PKs. Insulate DW from upstream ID format changes.",
            "Grain: precise definition of one fact row. Must be declared, consistent, never mixed.",
            "Star vs Snowflake: star = denormalized = simpler queries. Snowflake = normalized = more joins.",
            "Date dimension: pre-built, one row per day, 10 years. Year/quarter/month/holiday flags for fast GROUP BY.",
            "Design sequence: grain \u2192 measures \u2192 dimensions \u2192 SCD strategy \u2192 aggregation plan."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Draw on paper: a star schema for a food delivery app. Define the grain. Name all fact measures and all dimensions.",
            "<strong>Step 2:</strong> Write DDL for fact_orders and dim_restaurant. Include surrogate keys and all measures.",
            "<strong>Step 3:</strong> Write a query joining fact_sales \u2192 dim_date \u2192 dim_product to get monthly revenue by category.",
            "<strong>Step 4:</strong> Redesign as a snowflake \u2014 add dim_subcategory and dim_category as separate tables. How many more JOINs does the same query require?"
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Design Fact table for User Clicks (Granularity check)",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Design Fact table for User Clicks (Granularity check) \u2014 trace through 2-3 edge cases before writing any code.",
                "'Can I SUM this column?' \u2192 fact/measure. 'Is this descriptive context?' \u2192 dimension attribute.",
                "Always declare grain explicitly before DDL. Mixed-grain fact tables are the #1 data warehouse bug.",
                "Surrogate keys protect against upstream ID changes. Never use natural source keys as fact table FKs.",
                "Date dimension: analysts GROUP BY d.quarter without EXTRACT() \u2014 huge usability win."
            ]
        },
        "HardProblem": "Boss Problem (Amazon): Design a DW for marketplace analytics. Sellers list products, customers place orders with multiple line items, items can be returned. Design fact + dimension tables for: (1) revenue by seller, product category, date, (2) return rate analysis, (3) seller performance ranking. State grain of each fact table. Handle: products changing categories over time, international orders with currency conversion."
    },
    {
        "Week": 8,
        "Day": "Wednesday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Data Modeling",
        "SpecificTopic": "Dimension Tables",
        "ActionItem_Deliverable": "Normalize table to 3NF then Denormalize to Star",
        "LeetCodeProblem": "<strong>LC 252 \u2013 Meeting Rooms</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Row vs Columnar</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 The History \u2014 Why This Exists</div><div class=\"rich\"><h4>Why Dimensional Modeling Exists \u2014 The History</h4><p>In the 1960s databases were built for transactions: record an order, update inventory, log a payment. Every table was tightly normalized. Queries were simple lookups. IBM's relational model was perfect for this.</p><p>Then businesses wanted <em>analytics</em>. Not \"what is this customer's balance?\" but \"how did western-region revenue compare to last year, by product category and month?\" These questions required joining 8-12 tables, aggregating millions of rows, slicing across multiple dimensions simultaneously. Normalized OLTP schemas were terrible at this \u2014 joins were slow, SQL was unreadable, answers took hours.</p><p><strong>Ralph Kimball's insight (1990s):</strong> every business question has the same structure \u2014 \"How much [measure] by [dimension] by [dimension]?\" He designed the <strong>Star Schema</strong>: one central fact table of numbers, surrounded by dimension tables of context. The shape is a star. It mirrors how humans think about data.</p><pre>           DIM_PRODUCT             DIM_DATE\n           (category, brand)       (year, month, quarter)\n                    \\\\              /\n       DIM_STORE \u2500\u2500 FACT_SALES \u2500\u2500 DIM_CUSTOMER\n       (city,region)  (revenue,   (segment, age)\n                       units,\n                       discount)\n</pre><p>\u270d\ufe0f <strong>Core rule:</strong> Fact tables contain MEASUREMENTS (numbers you aggregate: revenue, clicks, quantity). Dimension tables contain CONTEXT (who, what, where, when). Quick test: \"Can I SUM this column?\" \u2014 yes \u2192 fact/measure, no \u2192 dimension/attribute.</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Building a Star Schema</div><div class=\"rich\"><h4>Building a Star Schema \u2014 Step by Step</h4><p>The fact table contains one row per business event \u2014 one sale, one click, one payment. It has foreign keys to every dimension (who/what/where/when) and numeric measures.</p><pre>CREATE TABLE fact_sales (\n  sale_id      BIGINT PRIMARY KEY,\n  date_key     INT REFERENCES dim_date(date_key),       -- WHEN\n  product_key  INT REFERENCES dim_product(product_key), -- WHAT\n  store_key    INT REFERENCES dim_store(store_key),      -- WHERE\n  customer_key INT REFERENCES dim_customer(customer_key),-- WHO\n  quantity_sold INT,\n  unit_price    DECIMAL(10,2),\n  total_revenue DECIMAL(10,2)   -- additive: SUM across all dims\n);</pre><h4>The Date Dimension \u2014 Why It's Special</h4><p>The date dimension is pre-populated for 10 years (3,650 rows). It stores year, quarter, month, week, day, holiday/weekend flags so analysts can <code>GROUP BY d.quarter</code> without ever calling EXTRACT(). It is the most important and most universally present dimension in any data warehouse.</p><pre>CREATE TABLE dim_date (\n  date_key    INT PRIMARY KEY,   -- e.g. 20240115\n  full_date   DATE,\n  year INT, quarter INT, month INT, month_name VARCHAR(20),\n  is_holiday BOOLEAN, is_weekend BOOLEAN\n);</pre><p><strong>Surrogate keys:</strong> We use simple integer PKs in dimension tables, not natural business keys. If an upstream product ID format changes, only the ETL mapping changes \u2014 not the entire fact table.</p></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Star vs Snowflake + Grain</div><div class=\"rich\"><h4>Star vs Snowflake Schema \u2014 When to Normalize Dimensions</h4><p><strong>Star schema:</strong> dimension tables are denormalized \u2014 category, subcategory, and brand all live in one flat product dimension. Some string repetition, but JOIN queries are simple (one join per dimension).</p><p><strong>Snowflake schema:</strong> dimension tables are normalized \u2014 dim_product \u2192 dim_subcategory \u2192 dim_category, each linked by FK. Less redundancy, but every analytical query needs 3 joins to get from a sale to its category.</p><table><tr><th>Aspect</th><td>Star Schema</td><td>Snowflake Schema</td></tr><tr><th>Storage</th><td>More (repeated strings)</td><td>Less (normalized)</td></tr><tr><th>Query complexity</th><td>1 join per dimension</td><td>3\u20134 joins per dimension chain</td></tr><tr><th>Query speed</th><td>Faster (fewer joins)</td><td>Slower on large data sets</td></tr><tr><th>FAANG preference</th><td>\u2705 Almost universal</td><td>\u274c Rare \u2014 only for very large dims</td></tr></table><h4>Grain \u2014 The Most Critical Design Decision</h4><p>A fact table's <strong>grain</strong> is the precise definition of what one row represents. Getting grain wrong is the most expensive modeling mistake \u2014 nearly impossible to fix without rebuilding.</p><pre>Grains for sales data:\n  \u2705 \"One row per line item in a sales transaction\" (finest grain)\n  \u2705 \"One row per sales receipt\"\n  \u2705 \"One row per day per store total\"\n  \u274c \"One row per transaction... sometimes per day\" \u2014 MIXED grain!\n\nMixed grain: SUM(revenue) double-counts. Every analyst gets different numbers.\nTrust in the data warehouse collapses.</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 FAANG Interview Framework</div><div class=\"rich\"><h4>The Interview Framework: Designing a Data Warehouse From Scratch</h4><p>FAANG interview: \"Design a data warehouse for an e-commerce company.\" The interviewer scores: (1) grain identified first, (2) facts vs dimensions correct, (3) SCD strategy named, (4) query pattern driven design.</p><pre>Step 1 \u2014 Ask clarifying questions:\n  \"What questions will analysts need to answer?\"\n  \"What is the finest level of detail needed? Line item or order?\"\n  \"How often does product/customer data change?\"\n  \"Expected data volume \u2014 rows per day?\"\n\nStep 2 \u2014 State the grain explicitly:\n  \"One row per order line item\"\n\nStep 3 \u2014 Identify measures:\n  quantity, unit_price, discount, line_total, margin\n\nStep 4 \u2014 Identify dimensions:\n  dim_date (when), dim_customer (who), dim_product (what),\n  dim_store (where), dim_promotion (why discounted)\n\nStep 5 \u2014 Handle attribute changes:\n  \"Products change prices, customers move \u2014 SCD Type 2 on both\"\n\nStep 6 \u2014 Plan aggregations:\n  \"Daily dashboard sums pre-aggregated in agg_daily_sales\n   so we don't scan the 10B-row fact table on every request\"</pre></div></div></div>",
        "KeyConcepts": [
            "Star schema = fact table (measures) surrounded by dimension tables (context). Optimized for analytical queries.",
            "Fact table: events (sales, clicks). Contains FKs to dimensions + numeric measures to aggregate.",
            "Additive measures: SUM across all dims (revenue). Semi-additive: only across some. Non-additive: never SUM (ratios).",
            "Surrogate keys: simple integer PKs. Insulate DW from upstream ID format changes.",
            "Grain: precise definition of one fact row. Must be declared, consistent, never mixed.",
            "Star vs Snowflake: star = denormalized = simpler queries. Snowflake = normalized = more joins.",
            "Date dimension: pre-built, one row per day, 10 years. Year/quarter/month/holiday flags for fast GROUP BY.",
            "Design sequence: grain \u2192 measures \u2192 dimensions \u2192 SCD strategy \u2192 aggregation plan."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Draw on paper: a star schema for a food delivery app. Define the grain. Name all fact measures and all dimensions.",
            "<strong>Step 2:</strong> Write DDL for fact_orders and dim_restaurant. Include surrogate keys and all measures.",
            "<strong>Step 3:</strong> Write a query joining fact_sales \u2192 dim_date \u2192 dim_product to get monthly revenue by category.",
            "<strong>Step 4:</strong> Redesign as a snowflake \u2014 add dim_subcategory and dim_category as separate tables. How many more JOINs does the same query require?"
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Normalize table to 3NF then Denormalize to Star",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Normalize table to 3NF then Denormalize to Star \u2014 trace through 2-3 edge cases before writing any code.",
                "'Can I SUM this column?' \u2192 fact/measure. 'Is this descriptive context?' \u2192 dimension attribute.",
                "Always declare grain explicitly before DDL. Mixed-grain fact tables are the #1 data warehouse bug.",
                "Surrogate keys protect against upstream ID changes. Never use natural source keys as fact table FKs.",
                "Date dimension: analysts GROUP BY d.quarter without EXTRACT() \u2014 huge usability win."
            ]
        },
        "HardProblem": "Boss Problem (Amazon): Design a DW for marketplace analytics. Sellers list products, customers place orders with multiple line items, items can be returned. Design fact + dimension tables for: (1) revenue by seller, product category, date, (2) return rate analysis, (3) seller performance ranking. State grain of each fact table. Handle: products changing categories over time, international orders with currency conversion."
    },
    {
        "Week": 8,
        "Day": "Thursday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Data Modeling",
        "SpecificTopic": "Handling Skew",
        "ActionItem_Deliverable": "Explain Salting technique for hot keys",
        "LeetCodeProblem": "<strong>LC 253 \u2013 Meeting Rooms II</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Partitions vs Tasks</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 What Spark Is \u2014 Driver/Executor + Lazy Eval</div><div class=\"rich\"><h4>What Spark Actually Is \u2014 And Why It Exists</h4><p>Apache Spark was created in 2009 at UC Berkeley's AMPLab as a faster replacement for Hadoop MapReduce. Hadoop MapReduce wrote intermediate results to disk after every step. A 10-step pipeline wrote and read disk 10 times \u2014 massive I/O overhead. Spark keeps intermediate results <strong>in memory</strong>, passing data between stages without disk. For iterative algorithms (machine learning) and multi-step pipelines, this is 10\u2013100x faster.</p><h4>How Spark Executes Code \u2014 The Driver/Executor Model</h4><pre>Spark Cluster Architecture:\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              DRIVER (your Python code)          \u2502\n\u2502  - Runs on the master node                      \u2502\n\u2502  - Builds the execution plan                    \u2502\n\u2502  - Sends tasks to executors                     \u2502\n\u2502  - Collects results                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502 (sends serialized tasks)\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u25bc              \u25bc              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Executor \u2502  \u2502Executor \u2502  \u2502Executor \u2502\n\u2502 - Node 1\u2502  \u2502 - Node 2\u2502  \u2502 - Node 3\u2502\n\u2502 - tasks \u2502  \u2502 - tasks \u2502  \u2502 - tasks \u2502\n\u2502 - cache \u2502  \u2502 - cache \u2502  \u2502 - cache \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n# DataFrame operations run on executors in parallel\n# Driver only sees final results (unless you call .collect() \u2014 dangerous!)</pre><h4>Lazy Evaluation \u2014 Nothing Runs Until You Ask</h4><p>Spark is lazy: DataFrame transformations (filter, select, join, groupBy) do NOT execute immediately. They build a <strong>query plan</strong>. Only <strong>actions</strong> (collect(), count(), write(), show()) trigger actual execution.</p><pre>df = spark.read.parquet(\"s3://events/\")   # no I/O yet\ndf2 = df.filter(df.year == 2024)           # no I/O yet \u2014 builds plan\ndf3 = df2.groupBy(\"country\").count()       # no I/O yet \u2014 extends plan\n\ndf3.show()  # \u2190 ACTION: NOW Spark executes the entire plan\n\n# Why lazy? Spark's optimizer (Catalyst) can rearrange operations:\n# - Pushes filters before joins (read less data)\n# - Eliminates unused columns early\n# - Merges multiple transformations into single scan\n# This often makes your unoptimized code run faster automatically.</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 The Shuffle + Broadcast Joins</div><div class=\"rich\"><h4>The Shuffle \u2014 The Most Expensive Spark Operation</h4><p>A <strong>shuffle</strong> occurs when data needs to be redistributed across executors based on a key \u2014 for example, during a <code>groupBy</code>, <code>join</code>, or <code>repartition</code>. Shuffle is expensive because: (1) executors must write their partition to disk, (2) data is sent over the network, (3) receiving executors read from disk and memory. In a job with 100 executors processing 1TB, a shuffle can move terabytes over the network.</p><pre># Operations that trigger a shuffle:\ndf.groupBy(\"country\").count()            # \u2190 SHUFFLE: all rows for same country\n                                         #   must land on same executor\ndf.join(other, \"user_id\")               # \u2190 SHUFFLE: matching user_ids must\n                                         #   be co-located\ndf.repartition(200, \"country\")          # \u2190 SHUFFLE: explicit redistribution\ndf.distinct()                            # \u2190 SHUFFLE: deduplicate requires global view\n\n# Operations that do NOT shuffle (cheap \"narrow\" transformations):\ndf.filter(df.year == 2024)              # stays on same partition\ndf.select(\"user_id\", \"amount\")          # stays on same partition\ndf.withColumn(\"total\", df.a + df.b)    # stays on same partition\ndf.coalesce(10)                         # merge partitions, no redistribution\n\n# Reduce shuffles by:\n# 1. Partition data by join key BEFORE the join (pre-partition)\n# 2. Broadcast small tables (eliminates join shuffle entirely)\n# 3. Aggregate before joining (reduce data volume before shuffle)</pre><h4>Broadcast Join \u2014 Eliminating the Join Shuffle</h4><pre># Problem: joining a 100TB fact table with a 50MB dimension\n# Default join \u2192 shuffle 100TB over network (catastrophically slow)\n\n# Solution: broadcast the small table to ALL executors\n# Each executor gets a full copy \u2014 no network movement for the large table\n\nfrom pyspark.sql.functions import broadcast\n\nresult = large_df.join(\n    broadcast(small_dim_df),  # \u2190 small table copied to all executors\n    on=\"product_id\"\n)  # No shuffle of large_df! Each executor joins locally.\n\n# Spark auto-broadcasts tables < spark.sql.autoBroadcastJoinThreshold (default 10MB)\n# Increase for larger dimensions if they fit in executor memory:\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"100m\")</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Data Skew + Partitions vs Tasks</div><div class=\"rich\"><h4>Data Skew \u2014 When One Executor Does 90% of the Work</h4><p><strong>Data skew</strong> is the #1 production Spark performance problem. It occurs when one partition key value has far more rows than others. Example: a NULL user_id for all guest orders means the \"NULL\" partition gets 80% of all rows. One executor processing 8TB while others process 100GB \u2014 the job is only as fast as the slowest executor.</p><pre># Detect skew: check partition sizes\ndf.groupBy(spark_partition_id()).count().show()  # see partition row counts\n# If one partition has 100M rows and others have 1M \u2192 skew\n\n# Fix 1: Salt the key (add random suffix to distribute skewed keys)\nfrom pyspark.sql import functions as F\n\n# Add salt to the large table\nnum_salt_buckets = 10\nsalted_large = large_df.withColumn(\n    \"salted_key\",\n    F.concat(F.col(\"user_id\"), F.lit(\"_\"), (F.rand() * num_salt_buckets).cast(\"int\"))\n)\n\n# Explode the small table to match all salted keys\nsalt_values = spark.range(num_salt_buckets).withColumnRenamed(\"id\", \"salt\")\nsalted_small = small_df.crossJoin(salt_values).withColumn(\n    \"salted_key\",\n    F.concat(F.col(\"user_id\"), F.lit(\"_\"), F.col(\"salt\"))\n)\n\n# Now join on salted_key \u2014 skew is evenly distributed!\nresult = salted_large.join(salted_small, \"salted_key\")\n\n# Fix 2: Filter skewed keys and process separately\nnormal = df.filter(F.col(\"user_id\").isNotNull())\nnulls  = df.filter(F.col(\"user_id\").isNull())\n# Process each differently, then union</pre><h4>Partitions vs Tasks \u2014 Understanding Parallelism</h4><p>In Spark, a <strong>partition</strong> is a chunk of data. A <strong>task</strong> processes one partition. The number of parallel tasks = number of executor cores. If you have 100 cores and 1,000 partitions \u2192 100 tasks run simultaneously, 10 rounds of 100 tasks = 10 rounds total.</p><pre># Rule of thumb: 2-4 partitions per CPU core\n# 100 cores \u2192 200-400 partitions\n\n# Too few partitions: executors are idle (not enough work to distribute)\n# Too many partitions: per-partition overhead dominates (too many small tasks)\n\ndf.rdd.getNumPartitions()  # check current partition count\ndf.repartition(400)  # redistribute to 400 partitions (shuffle)\ndf.coalesce(10)      # reduce to 10 partitions (no shuffle, but may be uneven)\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"400\")  # for groupBy/join output</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Diagnosing a Slow Spark Job</div><div class=\"rich\"><h4>FAANG Interview: Diagnosing a Slow Spark Job</h4><p>Interview question: \"Your Spark job processes 10TB of data but takes 6 hours. Walk me through diagnosing it.\"</p><pre>Step 1 \u2014 Open the Spark UI (port 4040)\n  \u2192 DAG tab: which stage takes the most time?\n  \u2192 Stage details: which task is the slowest? (skew indicator)\n  \u2192 Storage tab: how much data is cached? Is spill occurring?\n\nStep 2 \u2014 Identify the type of slowdown:\n\n  A. One stage takes 90% of job time:\n     \u2192 Check if it contains a shuffle (groupBy, join)\n     \u2192 If so: can you reduce data before the shuffle? Broadcast the smaller side?\n\n  B. One task in a stage takes 10x longer than others:\n     \u2192 Data skew. Check: what is the key distribution?\n     \u2192 Fix: salt the key, or handle skewed keys separately\n\n  C. Spill to disk (red in Spark UI):\n     \u2192 Executor ran out of memory, wrote shuffle data to disk\n     \u2192 Fix: increase executor memory, or increase spark.sql.shuffle.partitions\n       (more partitions = smaller per-partition size = less memory per partition)\n\n  D. Many stages, each fast individually but slow overall:\n     \u2192 Wide dependency chain \u2014 hard to parallelize\n     \u2192 Fix: checkpoint intermediate results, cache frequently reused DataFrames\n\ndf.cache()  # cache in memory \u2014 avoid recomputing in subsequent actions\ndf.persist(StorageLevel.DISK_ONLY)  # persist to disk if too large for memory</pre><p>\u270d\ufe0f <strong>The Spark UI is your most important debugging tool.</strong> FAANG interviewers expect you to know how to read it and what each metric means.</p></div></div></div>",
        "KeyConcepts": [
            "Spark = distributed in-memory computing. Driver builds plan, executors execute tasks on partitions.",
            "Lazy evaluation: transformations build a plan. Actions (count/show/write) trigger execution.",
            "Catalyst optimizer: rearranges your query plan for efficiency \u2014 predicate pushdown, column pruning.",
            "Shuffle: data redistribution across network. Most expensive operation. Triggered by groupBy, join, distinct.",
            "Narrow transformation: each partition maps to exactly one output partition. No shuffle. Fast.",
            "Broadcast join: copies small table to all executors. Eliminates join shuffle for large+small joins.",
            "Data skew: one partition key has disproportionate rows. Fix: salting (randomize key) or separate processing.",
            "Partitions = data chunks. Tasks = work units (1 per partition). Target 2-4 partitions per core.",
            "spark.sql.shuffle.partitions default = 200. Tune to 2-4x your number of cores."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Read 10M rows. Chain: filter \u2192 groupBy \u2192 count. Use .explain(True) to show the physical plan. Identify which operations cause shuffles.",
            "<strong>Step 2:</strong> Join a 1B-row fact table with a 5MB lookup table. Compare: regular join vs broadcast join. Measure time difference with spark.time().",
            "<strong>Step 3:</strong> Simulate data skew: create a dataset where 80% of rows have user_id=NULL. Run a groupBy. Observe one task taking 10x longer. Apply the salt fix.",
            "<strong>Step 4:</strong> Set spark.sql.shuffle.partitions to 10, 200, 1000 on the same groupBy query. Observe partition count and execution time changes."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Explain Salting technique for hot keys",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Explain Salting technique for hot keys \u2014 trace through 2-3 edge cases before writing any code.",
                "Seeing one stage taking 90% of job time? Check for shuffle. Can you broadcast the smaller side?",
                "One task running 10x longer than others? Data skew. Check key value distribution with groupBy + count.",
                "Spill to disk in Spark UI? Executor out of memory. Increase executor memory or spark.sql.shuffle.partitions.",
                "Cache DataFrames reused multiple times: df.cache(). Unpersist when done: df.unpersist()."
            ]
        },
        "HardProblem": "Boss Problem (Uber): Your daily Spark job aggregates 500B GPS ping events (5TB) to compute driver earnings per city. It runs in 14 hours but needs to run in 2 hours. Steps: (1) You find 3 shuffle stages dominate. How do you investigate each one? (2) One join between pings (5TB) and a driver_metadata table (2GB) causes a 3TB shuffle. How do you fix it without salting? (3) City='NULL' has 60% of all pings (GPS loss). How do you handle this skew? (4) The job reads the same 5TB raw data for 4 different aggregations. How do you avoid reading it 4 times?"
    },
    {
        "Week": 8,
        "Day": "Friday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Data Modeling",
        "SpecificTopic": "Surrogate Keys",
        "ActionItem_Deliverable": "List Pros/Cons of UUID vs Integer Sequence",
        "LeetCodeProblem": "<strong>LC 57 \u2013 Insert Interval</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>SCD Types (1, 2, 3)</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 The History \u2014 Why This Exists</div><div class=\"rich\"><h4>Why Dimensional Modeling Exists \u2014 The History</h4><p>In the 1960s databases were built for transactions: record an order, update inventory, log a payment. Every table was tightly normalized. Queries were simple lookups. IBM's relational model was perfect for this.</p><p>Then businesses wanted <em>analytics</em>. Not \"what is this customer's balance?\" but \"how did western-region revenue compare to last year, by product category and month?\" These questions required joining 8-12 tables, aggregating millions of rows, slicing across multiple dimensions simultaneously. Normalized OLTP schemas were terrible at this \u2014 joins were slow, SQL was unreadable, answers took hours.</p><p><strong>Ralph Kimball's insight (1990s):</strong> every business question has the same structure \u2014 \"How much [measure] by [dimension] by [dimension]?\" He designed the <strong>Star Schema</strong>: one central fact table of numbers, surrounded by dimension tables of context. The shape is a star. It mirrors how humans think about data.</p><pre>           DIM_PRODUCT             DIM_DATE\n           (category, brand)       (year, month, quarter)\n                    \\\\              /\n       DIM_STORE \u2500\u2500 FACT_SALES \u2500\u2500 DIM_CUSTOMER\n       (city,region)  (revenue,   (segment, age)\n                       units,\n                       discount)\n</pre><p>\u270d\ufe0f <strong>Core rule:</strong> Fact tables contain MEASUREMENTS (numbers you aggregate: revenue, clicks, quantity). Dimension tables contain CONTEXT (who, what, where, when). Quick test: \"Can I SUM this column?\" \u2014 yes \u2192 fact/measure, no \u2192 dimension/attribute.</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Building a Star Schema</div><div class=\"rich\"><h4>Building a Star Schema \u2014 Step by Step</h4><p>The fact table contains one row per business event \u2014 one sale, one click, one payment. It has foreign keys to every dimension (who/what/where/when) and numeric measures.</p><pre>CREATE TABLE fact_sales (\n  sale_id      BIGINT PRIMARY KEY,\n  date_key     INT REFERENCES dim_date(date_key),       -- WHEN\n  product_key  INT REFERENCES dim_product(product_key), -- WHAT\n  store_key    INT REFERENCES dim_store(store_key),      -- WHERE\n  customer_key INT REFERENCES dim_customer(customer_key),-- WHO\n  quantity_sold INT,\n  unit_price    DECIMAL(10,2),\n  total_revenue DECIMAL(10,2)   -- additive: SUM across all dims\n);</pre><h4>The Date Dimension \u2014 Why It's Special</h4><p>The date dimension is pre-populated for 10 years (3,650 rows). It stores year, quarter, month, week, day, holiday/weekend flags so analysts can <code>GROUP BY d.quarter</code> without ever calling EXTRACT(). It is the most important and most universally present dimension in any data warehouse.</p><pre>CREATE TABLE dim_date (\n  date_key    INT PRIMARY KEY,   -- e.g. 20240115\n  full_date   DATE,\n  year INT, quarter INT, month INT, month_name VARCHAR(20),\n  is_holiday BOOLEAN, is_weekend BOOLEAN\n);</pre><p><strong>Surrogate keys:</strong> We use simple integer PKs in dimension tables, not natural business keys. If an upstream product ID format changes, only the ETL mapping changes \u2014 not the entire fact table.</p></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Star vs Snowflake + Grain</div><div class=\"rich\"><h4>Star vs Snowflake Schema \u2014 When to Normalize Dimensions</h4><p><strong>Star schema:</strong> dimension tables are denormalized \u2014 category, subcategory, and brand all live in one flat product dimension. Some string repetition, but JOIN queries are simple (one join per dimension).</p><p><strong>Snowflake schema:</strong> dimension tables are normalized \u2014 dim_product \u2192 dim_subcategory \u2192 dim_category, each linked by FK. Less redundancy, but every analytical query needs 3 joins to get from a sale to its category.</p><table><tr><th>Aspect</th><td>Star Schema</td><td>Snowflake Schema</td></tr><tr><th>Storage</th><td>More (repeated strings)</td><td>Less (normalized)</td></tr><tr><th>Query complexity</th><td>1 join per dimension</td><td>3\u20134 joins per dimension chain</td></tr><tr><th>Query speed</th><td>Faster (fewer joins)</td><td>Slower on large data sets</td></tr><tr><th>FAANG preference</th><td>\u2705 Almost universal</td><td>\u274c Rare \u2014 only for very large dims</td></tr></table><h4>Grain \u2014 The Most Critical Design Decision</h4><p>A fact table's <strong>grain</strong> is the precise definition of what one row represents. Getting grain wrong is the most expensive modeling mistake \u2014 nearly impossible to fix without rebuilding.</p><pre>Grains for sales data:\n  \u2705 \"One row per line item in a sales transaction\" (finest grain)\n  \u2705 \"One row per sales receipt\"\n  \u2705 \"One row per day per store total\"\n  \u274c \"One row per transaction... sometimes per day\" \u2014 MIXED grain!\n\nMixed grain: SUM(revenue) double-counts. Every analyst gets different numbers.\nTrust in the data warehouse collapses.</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 FAANG Interview Framework</div><div class=\"rich\"><h4>The Interview Framework: Designing a Data Warehouse From Scratch</h4><p>FAANG interview: \"Design a data warehouse for an e-commerce company.\" The interviewer scores: (1) grain identified first, (2) facts vs dimensions correct, (3) SCD strategy named, (4) query pattern driven design.</p><pre>Step 1 \u2014 Ask clarifying questions:\n  \"What questions will analysts need to answer?\"\n  \"What is the finest level of detail needed? Line item or order?\"\n  \"How often does product/customer data change?\"\n  \"Expected data volume \u2014 rows per day?\"\n\nStep 2 \u2014 State the grain explicitly:\n  \"One row per order line item\"\n\nStep 3 \u2014 Identify measures:\n  quantity, unit_price, discount, line_total, margin\n\nStep 4 \u2014 Identify dimensions:\n  dim_date (when), dim_customer (who), dim_product (what),\n  dim_store (where), dim_promotion (why discounted)\n\nStep 5 \u2014 Handle attribute changes:\n  \"Products change prices, customers move \u2014 SCD Type 2 on both\"\n\nStep 6 \u2014 Plan aggregations:\n  \"Daily dashboard sums pre-aggregated in agg_daily_sales\n   so we don't scan the 10B-row fact table on every request\"</pre></div></div></div>",
        "KeyConcepts": [
            "Star schema = fact table (measures) surrounded by dimension tables (context). Optimized for analytical queries.",
            "Fact table: events (sales, clicks). Contains FKs to dimensions + numeric measures to aggregate.",
            "Additive measures: SUM across all dims (revenue). Semi-additive: only across some. Non-additive: never SUM (ratios).",
            "Surrogate keys: simple integer PKs. Insulate DW from upstream ID format changes.",
            "Grain: precise definition of one fact row. Must be declared, consistent, never mixed.",
            "Star vs Snowflake: star = denormalized = simpler queries. Snowflake = normalized = more joins.",
            "Date dimension: pre-built, one row per day, 10 years. Year/quarter/month/holiday flags for fast GROUP BY.",
            "Design sequence: grain \u2192 measures \u2192 dimensions \u2192 SCD strategy \u2192 aggregation plan."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Draw on paper: a star schema for a food delivery app. Define the grain. Name all fact measures and all dimensions.",
            "<strong>Step 2:</strong> Write DDL for fact_orders and dim_restaurant. Include surrogate keys and all measures.",
            "<strong>Step 3:</strong> Write a query joining fact_sales \u2192 dim_date \u2192 dim_product to get monthly revenue by category.",
            "<strong>Step 4:</strong> Redesign as a snowflake \u2014 add dim_subcategory and dim_category as separate tables. How many more JOINs does the same query require?"
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> List Pros/Cons of UUID vs Integer Sequence",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> List Pros/Cons of UUID vs Integer Sequence \u2014 trace through 2-3 edge cases before writing any code.",
                "'Can I SUM this column?' \u2192 fact/measure. 'Is this descriptive context?' \u2192 dimension attribute.",
                "Always declare grain explicitly before DDL. Mixed-grain fact tables are the #1 data warehouse bug.",
                "Surrogate keys protect against upstream ID changes. Never use natural source keys as fact table FKs.",
                "Date dimension: analysts GROUP BY d.quarter without EXTRACT() \u2014 huge usability win."
            ]
        },
        "HardProblem": "Boss Problem (Amazon): Design a DW for marketplace analytics. Sellers list products, customers place orders with multiple line items, items can be returned. Design fact + dimension tables for: (1) revenue by seller, product category, date, (2) return rate analysis, (3) seller performance ranking. State grain of each fact table. Handle: products changing categories over time, international orders with currency conversion."
    },
    {
        "Week": 8,
        "Day": "Saturday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Data Modeling + Cloud",
        "SpecificTopic": "Mock Design + Cloud DE",
        "ActionItem_Deliverable": "Design Amazon Order History Schema + study AWS Glue/EMR & GCP Dataflow",
        "LeetCodeProblem": "<strong>LC 435 \u2013 Non-overlapping Intervals</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": "NEW: Cloud DE services added",
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Broadcasting</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 What DAGs Are + Airflow Core Concepts</div><div class=\"rich\"><h4>What is a DAG and Why Orchestration Matters</h4><p>A data pipeline consists of multiple steps: extract from source, validate, transform, load, test, notify. These steps have dependencies \u2014 step 3 cannot start until steps 1 and 2 complete. Some steps can run in parallel (downloading from multiple sources simultaneously). Some steps must run in sequence (load only after validation passes).</p><p>A <strong>Directed Acyclic Graph (DAG)</strong> is the formal way to represent these dependencies: nodes are tasks, directed edges represent \"must complete before.\" Acyclic means there are no circular dependencies (which would cause deadlock). Apache Airflow is the dominant DAG orchestration platform at FAANG companies.</p><pre># Example DAG: daily customer revenue pipeline\n\n     [extract_orders]   [extract_customers]   \u2190 parallel execution\n            \\                 /\n          [validate_data]            \u2190 waits for both\n                  |\n     [compute_revenue]              \u2190 runs after validation\n         /          \\\\\n[load_to_dw]  [send_slack_report]   \u2190 parallel execution\n         \\\\\n       [run_dbt_tests]              \u2190 runs after load</pre><h4>Airflow Core Concepts</h4><table><tr><th>Concept</th><td>What It Does</td></tr><tr><th>DAG</th><td>A Python file defining the workflow graph</td></tr><tr><th>Task</th><td>A single unit of work (one node in the graph)</td></tr><tr><th>Operator</th><td>Type of task: PythonOperator, BashOperator, SparkSubmitOperator, etc.</td></tr><tr><th>Sensor</th><td>Special operator that waits for a condition (file arrives, API responds)</td></tr><tr><th>Schedule</th><td>Cron expression or timedelta defining when DAG runs</td></tr><tr><th>DAG Run</th><td>One execution instance of a DAG for a specific logical date</td></tr><tr><th>Task Instance</th><td>One execution of one task within a DAG Run</td></tr><tr><th>XCom</th><td>Mechanism for tasks to pass small values to downstream tasks</td></tr></table></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Operators, Sensors, Real DAG Code</div><div class=\"rich\"><h4>Operators and Sensors \u2014 The Building Blocks</h4><p>An <strong>Operator</strong> defines what a task does. Airflow provides built-in operators for common actions, and you can write custom operators for anything else.</p><pre>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nfrom airflow.sensors.filesystem import FileSensor\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    \"owner\": \"data-team\",\n    \"retries\": 3,\n    \"retry_delay\": timedelta(minutes=5),\n    \"email_on_failure\": True,\n}\n\nwith DAG(\n    dag_id=\"daily_revenue_pipeline\",\n    schedule_interval=\"0 3 * * *\",    # 3am UTC daily\n    start_date=datetime(2024, 1, 1),\n    catchup=False,                    # don't backfill old runs\n    default_args=default_args,\n) as dag:\n\n    wait_for_source = FileSensor(\n        task_id=\"wait_for_source_file\",\n        filepath=\"/data/source/orders_{{ ds }}.csv\",\n        poke_interval=60,   # check every 60 seconds\n        timeout=3600,       # fail if not found within 1 hour\n    )\n\n    run_spark = SparkSubmitOperator(\n        task_id=\"compute_revenue\",\n        application=\"s3://jobs/revenue_job.py\",\n        conf={\"spark.executor.memory\": \"8g\"},\n    )\n\n    def validate(**context):\n        # access the execution date via context\n        ds = context[\"ds\"]  # \"2024-01-15\"\n        count = check_row_count(ds)\n        if count < 1000:\n            raise ValueError(f\"Only {count} rows \u2014 expected > 1000\")\n\n    validate_task = PythonOperator(\n        task_id=\"validate_output\",\n        python_callable=validate,\n    )\n\n    # Dependency chain\n    wait_for_source >> run_spark >> validate_task</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 XComs + Backfilling Pitfalls</div><div class=\"rich\"><h4>XComs \u2014 Passing Data Between Tasks</h4><p>XCom (cross-communication) allows tasks to pass small values to downstream tasks. <strong>Critical rule:</strong> XComs are stored in the Airflow metadata database. They are for small values only: row counts, file paths, status strings, timestamps. Never XCom a DataFrame or large file \u2014 use S3/GCS paths instead.</p><pre># Task 1: pushes a value via XCom (auto-push with return)\ndef extract(**context):\n    rows = run_extraction()\n    return len(rows)  # auto-pushed to XCom key \"return_value\"\n\n# Task 2: pulls the value from upstream task\ndef validate(**context):\n    row_count = context[\"ti\"].xcom_pull(\n        task_ids=\"extract_task\",  # which task\n        key=\"return_value\"        # which XCom key\n    )\n    if row_count < 1000:\n        raise ValueError(f\"Too few rows: {row_count}\")\n\n# Better pattern for large data: pass S3 path, not the data itself\ndef extract(**context):\n    ds = context[\"ds\"]\n    path = f\"s3://staging/orders/{ds}/data.parquet\"\n    write_data_to_s3(path)\n    return path  # XCom the path, not the data</pre><h4>Backfilling \u2014 Running Historical Dates</h4><p><strong>Backfilling</strong> means running a DAG for historical dates \u2014 for example, you added a new column to your pipeline and need to reprocess the last 90 days of data. Airflow can trigger DAG runs for any historical date via the CLI or UI.</p><pre># Backfill via CLI: run every day from Jan 1 to Jan 31\nairflow dags backfill \\\n  --dag-id daily_revenue_pipeline \\\n  --start-date 2024-01-01 \\\n  --end-date 2024-01-31\n\n# \u26a0\ufe0f  Backfill pitfalls:\n# 1. catchup=True (default) auto-backfills missed runs on restart\n#    \u2192 set catchup=False to prevent surprise backfills\n# 2. Backfills run concurrently (all 31 days at once by default)\n#    \u2192 set max_active_runs=1 to run one day at a time\n# 3. Tasks may not be idempotent \u2192 backfill causes duplicate data\n#    \u2192 always make your pipeline idempotent!</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Dynamic DAGs + Design Best Practices</div><div class=\"rich\"><h4>Dynamic DAGs \u2014 Generating at Scale</h4><p>When you need dozens or hundreds of similar pipelines (one DAG per client, per table, per region), manually writing each DAG is impractical. <strong>Dynamic DAGs</strong> generate the DAG structure programmatically from a config or database.</p><pre># Pattern: generate one DAG per client from a config file\n# clients.yaml: [{name: \"acme\", tables: [\"orders\",\"users\"]}, ...]\n\nimport yaml\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\nwith open(\"/dags/config/clients.yaml\") as f:\n    clients = yaml.safe_load(f)[\"clients\"]\n\nfor client in clients:\n    dag_id = f\"etl_{client['name']}\"\n\n    with DAG(\n        dag_id=dag_id,\n        schedule_interval=\"0 4 * * *\",\n        ...\n    ) as dag:\n        for table in client[\"tables\"]:\n            task = PythonOperator(\n                task_id=f\"process_{table}\",\n                python_callable=process_table,\n                op_args=[client[\"name\"], table],\n            )\n        globals()[dag_id] = dag  # register DAG globally for Airflow scanner\n\n# Result: one DAG per client, all generated from config\n# Adding a new client = add one line to clients.yaml</pre><h4>FAANG Interview: DAG Design Best Practices</h4><ul><li><strong>Idempotent tasks:</strong> running the same task twice for the same date should produce the same result. No duplicate inserts. Use INSERT ... ON CONFLICT or DELETE then INSERT.</li><li><strong>Atomic writes:</strong> write to a staging location, atomically move to final location. Never overwrite production data partially.</li><li><strong>Alerting:</strong> every production DAG should have email_on_failure=True and PagerDuty/Slack integration for SLA misses.</li><li><strong>SLA monitoring:</strong> if a DAG that normally finishes at 4am does not complete by 6am, trigger an alert \u2014 the downstream team needs to know.</li><li><strong>Prefer sensors over long-running tasks:</strong> a FileSensor that checks every 60 seconds uses minimal resources. A sleep loop in a Python task holds an executor slot.</li></ul></div></div></div>",
        "KeyConcepts": [
            "DAG = Directed Acyclic Graph. Nodes are tasks, edges are dependencies. Acyclic = no circular dependencies.",
            "Airflow: Python-defined DAGs. schedule_interval (cron), start_date, catchup, max_active_runs.",
            "Operator types: PythonOperator, BashOperator, SparkSubmitOperator, SqlOperator, S3ToRedshiftOperator.",
            "Sensor: waits for a condition. FileSensor, HttpSensor, S3KeySensor. Uses poke_interval to check periodically.",
            "XCom: small values passed between tasks (paths, counts, flags). Never XCom large data \u2014 use S3 path instead.",
            "catchup=False: prevents Airflow from backfilling all missed runs on restart. Almost always set this.",
            "Idempotent tasks: running twice for the same date = same result. Essential for safe retries and backfills.",
            "Dynamic DAGs: generate multiple DAGs from a config (loop in Python file). Use globals() to register."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write an Airflow DAG with 3 tasks: extract (PythonOperator), transform (SparkSubmitOperator), load (PythonOperator). Set up correct dependencies.",
            "<strong>Step 2:</strong> Add a FileSensor before extract that waits for a source file to appear in S3. Set poke_interval=60 and timeout=3600.",
            "<strong>Step 3:</strong> Make the extract task push a row count to XCom. Have the validate task pull it and raise ValueError if count < 1000.",
            "<strong>Step 4:</strong> Write a dynamic DAG generator that creates one DAG per client from a JSON config file with 5 client entries. Verify all 5 DAGs appear in Airflow UI."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Design Amazon Order History Schema + study AWS Glue/EMR & GCP Dataflow",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Design Amazon Order History Schema + study AWS Glue/EMR & GCP Dataflow \u2014 trace through 2-3 edge cases before writing any code.",
                "catchup=True (default) can auto-run hundreds of missed historical DAG runs \u2014 almost always set catchup=False.",
                "XCom best practice: pass S3 paths, not data. The data lives in S3, the path lives in XCom.",
                "Sensor vs sleep in task: sensor releases executor slot while waiting. Sleep holds the slot \u2014 avoid it.",
                "max_active_runs=1: prevents concurrent DAG runs. Important for pipelines that write to the same partition."
            ]
        },
        "HardProblem": "Boss Problem (Spotify): You manage a daily pipeline: 200 Airflow DAGs, each processing data for one country. DAGs run at different times (by time zone) and share downstream dependencies (a global aggregation DAG that waits for ALL 200 country DAGs to finish). (1) Design the dependency structure. (2) One country DAG fails \u2014 should the global aggregation wait or proceed with 199/200 countries? (3) A new country is added \u2014 how do you add it without rewriting code? (4) The global aggregation consistently misses its 6am SLA by 15 minutes \u2014 diagnose and fix."
    },
    {
        "Week": 8,
        "Day": "Sunday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Rest",
        "SpecificTopic": "Rest & Review",
        "ActionItem_Deliverable": "Review Kimball Dimensional Modeling",
        "LeetCodeProblem": null,
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Generators</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Active Rest \u2014 How Memory Consolidates</div>\n<div class=\"rich\">\n<h4>The Neuroscience of Spaced Repetition</h4>\n<p>Rest days are not wasted days. During rest, your brain replays what it learned \u2014 a process called <strong>memory consolidation</strong>. Research consistently shows that <em>attempting to recall from memory</em> is 3\u00d7 more effective at building durable knowledge than re-reading notes. The act of struggling to remember \u2014 even failing \u2014 strengthens the neural pathway more than passive review.</p>\n<p>Think of it like muscle recovery: a weightlifter who trains 7 days straight without rest gets weaker, not stronger. The growth happens during recovery. Your brain is the same.</p>\n<h4>Today's Protocol (4 hours, structured)</h4>\n<ol>\n  <li><strong>Active Recall (60 min):</strong> Close everything. On a blank page, write every concept, formula, and pattern from this week from memory. Then check.</li>\n  <li><strong>Error Review (60 min):</strong> Find the hardest problem you struggled with. Attempt it again completely from scratch \u2014 no notes.</li>\n  <li><strong>Teach It (45 min):</strong> Open the AI chatbot and explain Gaps & Islands to it as if it's a junior engineer. If you stumble, that's your gap.</li>\n  <li><strong>Next Week Preview (15 min):</strong> Skim next Monday's topic at headline level only \u2014 priming improves retention when you study it deeply.</li>\n</ol>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Active Recall Quiz \u2014 No Notes Allowed</div>\n<div class=\"rich\">\n<h4>Answer These From Memory. Then Check.</h4>\n<pre>Window Functions:\n\u25a1 What is the difference between ROW_NUMBER, RANK, and DENSE_RANK?\n  (Write the output for a table with two tied rows)\n\u25a1 Write the syntax for a 7-day rolling average from memory\n\u25a1 What does PARTITION BY do differently from GROUP BY?\n\u25a1 Write the complete Gaps & Islands CTE from memory\n\u25a1 What is the difference between ROWS BETWEEN and RANGE BETWEEN?\n\u25a1 LAG(col, 1, 0) vs LAG(col, 1) \u2014 what is different?\n\u25a1 In what SQL execution order step do window functions run?\n\u25a1 Why can't you write WHERE rank &lt;= 3 in the same query as ROW_NUMBER?\n\nScore yourself:\n8/8 = \ud83c\udfc6 Mastery level \u2014 you're ready for interviews on this\n5-7/8 = \ud83d\udfe1 Good \u2014 review the 1-2 concepts you missed\n&lt; 5/8 = \ud83d\udd34 Go back to the specific day's content, re-read Level 1-2</pre>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Spaced Repetition Schedule</div>\n<div class=\"rich\">\n<h4>When to Review This Week's Material</h4>\n<table>\n<tr><th>Review this content</th><th>When</th><th>How</th></tr>\n<tr><td>This week</td><td>Today</td><td>Active recall quiz above</td></tr>\n<tr><td>This week</td><td>In 3 days</td><td>Re-solve the hard problem from Day 4</td></tr>\n<tr><td>This week</td><td>In 7 days</td><td>LeetCode: solve problems 185, 180, 196</td></tr>\n<tr><td>This week</td><td>In 30 days</td><td>Explain all 4 concepts to someone else</td></tr>\n</table>\n<h4>The Feynman Technique</h4>\n<p>After studying any concept, close your notes and explain it as if you're teaching a newcomer. Use only simple words. Avoid jargon. If you stumble, that's exactly where your understanding breaks down \u2014 return to just that part. This technique built Richard Feynman's legendary ability to explain complex physics simply.</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Interview Readiness Self-Assessment</div>\n<div class=\"rich\">\n<pre>Rate yourself 1\u20135 on each topic (honest self-assessment):\n  1 = \"I need to re-read from scratch\"\n  3 = \"I understand but need to look at notes\"\n  5 = \"I can code this under pressure from memory\"\n\nROW_NUMBER / RANK / DENSE_RANK:        ___/5\nRolling windows (ROWS BETWEEN):        ___/5\nLAG/LEAD (adjacent row comparison):    ___/5\nGaps & Islands (date streaks):         ___/5\nSession detection (timestamp version): ___/5\n\nAction:\n  Score &lt;3: revisit that day's Level 1 and Level 2 content tomorrow\n  Score 3:  do one LeetCode problem on that specific topic\n  Score 5:  you're interview-ready \u2705 on this topic</pre>\n<h4>FAANG Interview Reality Check</h4>\n<p>At meta, Amazon, and Google data engineering interviews, you'll be asked to write SQL live on a shared screen without autocomplete or documentation. The most common reason candidates fail: they know the concept but freeze on the syntax under pressure. The fix: write every pattern from memory at least 5 times this week. Muscle memory is more reliable than conscious recall under stress.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Memory consolidation happens during rest \u2014 the brain replays and stabilizes learned patterns during downtime.",
            "Active recall (testing yourself) builds 3x stronger memory than re-reading notes.",
            "Spaced repetition: review at day 1, day 3, day 7, day 30 for maximum retention.",
            "The Feynman Technique: if you can't explain it simply, you don't fully understand it yet.",
            "Error analysis: the problems you got WRONG are your highest-value review items \u2014 spend the most time there."
        ],
        "Tasks": [
            "<strong>Active Recall:</strong> Write the Gaps & Islands CTE from memory on paper. Check it against Day 4.",
            "<strong>Re-attempt:</strong> Pick the hardest problem from this week and solve it in 20 minutes from scratch.",
            "<strong>Teach it:</strong> Explain RANK vs DENSE_RANK to the AI chatbot as if it's a junior. Get it right without notes.",
            "<strong>Preview:</strong> Skim next week's Monday topic for 10 minutes \u2014 just headline concepts."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Review Kimball Dimensional Modeling",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Review Kimball Dimensional Modeling \u2014 trace through 2-3 edge cases before writing any code.",
                "Use the AI chatbot to quiz yourself \u2014 ask it 'Give me a SQL window function interview question.'",
                "Write SQL from memory in a plain text editor \u2014 no autocomplete. This builds true recall.",
                "The hardest concept this week: spend 30 minutes writing a full explanation in your own words."
            ]
        },
        "HardProblem": "Connect-the-dots: Write 2-3 paragraphs explaining how ROW_NUMBER, rolling averages, LAG, and Gaps & Islands could ALL be used in a single production pipeline for a ride-sharing company. What problem does each solve? How do they complement each other? What would break if you removed any one of them?"
    },
    {
        "Week": 9,
        "Day": "Monday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Quality & Contracts",
        "SpecificTopic": "Data Quality",
        "ActionItem_Deliverable": "Write 3 SQL tests (Nulls/Volume/Distribution)",
        "LeetCodeProblem": "<strong>LC 1667 \u2013 Fix Names in a Table</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Mock Design Round</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Data Quality Dimensions + Where to Check</div><div class=\"rich\"><h4>Data Quality \u2014 The Silent Business Risk</h4><p>Data quality failures are responsible for an estimated $3.1 trillion in losses annually in the US alone (IBM, 2016). At FAANG scale, bad data can cause: incorrect recommendations (Netflix), miscounted ad impressions (Google billing error), wrong product availability (Amazon), or payments to the wrong accounts (banking). Data quality is not a nice-to-have \u2014 it is a business-critical function.</p><h4>The Five Dimensions of Data Quality</h4><table><tr><th>Dimension</th><td>Definition</td><td>Example check</td></tr><tr><th>Completeness</th><td>Are required values present?</td><td>WHERE user_id IS NULL \u2192 count should be 0</td></tr><tr><th>Uniqueness</th><td>No duplicate records?</td><td>COUNT(*) vs COUNT(DISTINCT pk) \u2192 must be equal</td></tr><tr><th>Validity</th><td>Values within expected range/format?</td><td>age BETWEEN 0 AND 120, email LIKE '%@%'</td></tr><tr><th>Consistency</th><td>Same fact represented the same way?</td><td>revenue = SUM(line_items) for every order</td></tr><tr><th>Timeliness</th><td>Data arrives when expected?</td><td>MAX(event_time) > NOW() - INTERVAL 1 HOUR</td></tr></table><h4>Data Quality Checks \u2014 Where to Put Them</h4><pre>Three places to add quality checks:\n\n1. INGESTION: validate data as it arrives from source\n   \u2192 Reject/quarantine rows that fail schema/range checks\n   \u2192 Stop bad data from entering the warehouse\n\n2. TRANSFORMATION: check intermediate results after each step\n   \u2192 Assert: if I join A to B, I should get X rows (\u00b1 5%)\n   \u2192 Check: did revenue change by >50% overnight? (anomaly)\n\n3. OUTPUT: check the final data before publishing to consumers\n   \u2192 Ensure tables are not empty\n   \u2192 Verify foreign key integrity\n   \u2192 Check business rules (all orders have a payment)</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 dbt + Schema Evolution</div><div class=\"rich\"><h4>dbt \u2014 Data Build Tool</h4><p>dbt (data build tool) is the standard tool for SQL-based transformations in modern data warehouses. It lets you write SQL SELECT queries (models), run them as CREATE TABLE / CREATE VIEW, test them automatically, document them, and track lineage.</p><pre># dbt model: models/revenue/daily_revenue.sql\n-- This SQL becomes a table in your data warehouse\n{{ config(materialized='table') }}\n\nSELECT\n    order_date,\n    SUM(revenue)     AS total_revenue,\n    COUNT(order_id)  AS order_count\nFROM {{ ref('stg_orders') }}   -- reference another dbt model\nWHERE order_status = 'COMPLETED'\nGROUP BY order_date\n\n# dbt test: models/revenue/schema.yml\n# Tests run automatically after each model build\nmodels:\n  - name: daily_revenue\n    columns:\n      - name: order_date\n        tests:\n          - not_null\n          - unique\n      - name: total_revenue\n        tests:\n          - not_null\n          - dbt_utils.expression_is_true:\n              expression: \"> 0\"  # revenue must be positive</pre><h4>Schema Evolution \u2014 Handling Schema Changes Safely</h4><pre># Adding a column: backwards compatible \u2014 safe\nALTER TABLE orders ADD COLUMN promo_code VARCHAR(50);\n-- Existing readers ignore the new column (Parquet, Avro do this automatically)\n\n# Removing a column: BREAKING CHANGE\n-- Readers that expect the column will fail!\n-- Strategy: deprecate first (document), migrate readers, then drop\n\n# Changing a column type: almost always BREAKING\n-- VARCHAR(50) \u2192 VARCHAR(200): safe (wider)\n-- INT \u2192 BIGINT: safe in Parquet (widening)\n-- INT \u2192 VARCHAR: breaking (downstream code fails)\n\n# Schema enforcement in Parquet (PySpark):\ndf.write.option(\"mergeSchema\", \"true\").parquet(\"s3://data/orders/\")\n# mergeSchema=True: allows new columns to be added to existing Parquet files\n# Missing columns in old files appear as NULL in combined reads</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Idempotency + Unit Testing</div><div class=\"rich\"><h4>Idempotency \u2014 The Most Important Pipeline Property</h4><p><strong>Idempotency:</strong> running a pipeline multiple times for the same date produces the same result. This is essential for: safe retries after failures, safe backfills, and debugging in production.</p><pre># \u274c NON-IDEMPOTENT: running twice creates duplicate rows\nINSERT INTO daily_revenue\nSELECT order_date, SUM(revenue) FROM orders GROUP BY order_date;\n-- Run twice for 2024-01-15 \u2192 two rows for 2024-01-15!\n\n# \u2705 IDEMPOTENT pattern 1: DELETE THEN INSERT (delete-and-replace)\nDELETE FROM daily_revenue WHERE order_date = '2024-01-15';\nINSERT INTO daily_revenue\nSELECT order_date, SUM(revenue) FROM orders WHERE order_date='2024-01-15' GROUP BY 1;\n\n# \u2705 IDEMPOTENT pattern 2: INSERT ... ON CONFLICT UPDATE\nINSERT INTO daily_revenue(order_date, total_revenue)\nSELECT order_date, SUM(revenue) FROM orders GROUP BY order_date\nON CONFLICT (order_date) DO UPDATE SET total_revenue=EXCLUDED.total_revenue;\n\n# \u2705 IDEMPOTENT pattern 3: overwrite the partition\ndf.write\\\n  .partitionBy(\"order_date\")\\\n  .mode(\"overwrite\")\\\n  .parquet(\"s3://revenue/\")\n# Rewrites only the affected partition \u2014 other partitions untouched</pre><h4>Unit Testing Data Pipelines</h4><pre>import pytest\nfrom pyspark.sql import SparkSession\n\ndef test_revenue_computation():\n    spark = SparkSession.builder.master(\"local[2]\").getOrCreate()\n\n    # Create small test dataframe\n    input_data = [\n        (\"2024-01-15\", \"COMPLETED\", 100.0),\n        (\"2024-01-15\", \"COMPLETED\", 50.0),\n        (\"2024-01-15\", \"CANCELLED\", 200.0),   # should be excluded\n    ]\n    orders_df = spark.createDataFrame(input_data, [\"order_date\",\"status\",\"revenue\"])\n\n    # Run the transformation\n    result = compute_daily_revenue(orders_df)\n\n    # Assert expected output\n    rows = result.collect()\n    assert len(rows) == 1, \"Expected one result row\"\n    assert rows[0].total_revenue == 150.0, \"Cancelled orders should be excluded\"\n    assert rows[0].order_date == \"2024-01-15\"</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Governance + Handling Bad Data in Production</div><div class=\"rich\"><h4>Data Governance \u2014 Lineage, Catalog, Access Control</h4><p>Data governance answers: what data do we have, what is it for, who should access it, where did it come from? At FAANG scale, this is essential because hundreds of engineers use the same data warehouse, and a new engineer must be able to find trustworthy data without asking.</p><ul><li><strong>Data Catalog</strong>: searchable index of all tables with descriptions, owners, update frequency. Tools: AWS Glue, Amundsen, DataHub.</li><li><strong>Data Lineage</strong>: visualizes the transformation chain: \"This dashboard metric comes from table X, which is built from source table Y, which ingests from API Z.\" Enables impact analysis: \"If API Z changes, what breaks?\"</li><li><strong>Column-level lineage</strong>: even more granular \u2014 traces which source columns contributed to each output column.</li><li><strong>PII governance</strong>: tables containing personal data (name, email, SSN) require special access controls, encryption, and retention policies.</li><li><strong>Row-level security</strong>: different rows visible to different users (e.g., each sales rep sees only their region's data).</li></ul><h4>FAANG Interview: \"How do you handle bad data in production?\"</h4><pre>Framework answer:\n\n1. DETECT: automated quality checks at every pipeline stage\n   - dbt tests on output models\n   - Great Expectations or custom SQL assertions\n   - Anomaly detection (revenue dropped 80%? \u2192 alert)\n\n2. ALERT: PagerDuty or Slack notification immediately\n   - Include: which table, which check failed, expected vs actual\n\n3. QUARANTINE: do not propagate bad data downstream\n   - Move failing records to quarantine table\n   - Good records continue to production\n\n4. INVESTIGATE: what is the root cause?\n   - Source system change? (schema evolution)\n   - Missing upstream data? (late arrival)\n   - Pipeline bug? (code change)\n\n5. FIX + BACKFILL: fix the pipeline, reprocess affected dates</pre></div></div></div>",
        "KeyConcepts": [
            "5 quality dimensions: Completeness, Uniqueness, Validity, Consistency, Timeliness.",
            "Add quality checks at: ingestion (reject bad data), transformation (assert intermediate results), output (verify before publish).",
            "dbt: write SQL SELECT \u2192 runs as table/view. Built-in tests: not_null, unique, accepted_values, relationships.",
            "Schema evolution: adding columns = safe. Removing/renaming columns = breaking. Changing types = usually breaking.",
            "Idempotency: pipeline can run multiple times for same date \u2192 same result. Critical for safe retries + backfills.",
            "Idempotent patterns: DELETE+INSERT, INSERT ON CONFLICT UPDATE, write.mode('overwrite').partitionBy().",
            "Unit testing: create small input DataFrame, run transformation, assert expected output. Use pytest + local Spark.",
            "Data governance: catalog (what exists), lineage (where it comes from), access control (who sees what)."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write 5 SQL quality checks for an orders table: NULL check, uniqueness, range validation (amount > 0), foreign key integrity, and timeliness (max order_date within last 2 hours).",
            "<strong>Step 2:</strong> Write a dbt model and schema.yml with at least 3 tests (not_null, unique, expression_is_true). Run dbt test.",
            "<strong>Step 3:</strong> Make a non-idempotent INSERT idempotent using all 3 patterns: DELETE+INSERT, ON CONFLICT, and Spark overwrite partition. Verify by running twice.",
            "<strong>Step 4:</strong> Write a pytest unit test for a Spark transformation that computes daily revenue. Include edge cases: empty input, all-NULL revenue, negative amounts."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Write 3 SQL tests (Nulls/Volume/Distribution)",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Write 3 SQL tests (Nulls/Volume/Distribution) \u2014 trace through 2-3 edge cases before writing any code.",
                "dbt test types: not_null, unique (built-in), accepted_values, relationships (built-in), custom SQL assertions.",
                "Non-idempotent pipelines are the #1 cause of duplicate data bugs at scale. Always design for idempotency.",
                "Great Expectations: Python library for defining and running data validation rules (similar to dbt tests but in Python).",
                "Anomaly detection alert: if today's row count < 80% or > 120% of last 7-day average \u2192 alert. Simple and powerful."
            ]
        },
        "HardProblem": "Boss Problem (Airbnb): You are the data quality lead. Your nightly ETL ingests from 20 source databases across 5 countries. (1) Design a quality check framework: what checks run at each stage, who is alerted when they fail, and how do you prevent bad data from reaching BI reports? (2) A source system in Germany changes the 'price' column from EUR decimal to STRING '\u20acX.XX' without warning. How does your system detect this? What happens to the pipeline? How do you fix it? (3) A data engineer accidentally writes non-idempotent code that ran 3 times \u2014 you now have 3x the rows for 2024-01-15 only. How do you fix production data?"
    },
    {
        "Week": 9,
        "Day": "Tuesday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Quality & Contracts",
        "SpecificTopic": "Idempotency",
        "ActionItem_Deliverable": "Explain how to make a pipeline safe to re-run",
        "LeetCodeProblem": "<strong>LC 1527 \u2013 Patients With Condition</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Delta Lake / Iceberg</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Data Quality Dimensions + Where to Check</div><div class=\"rich\"><h4>Data Quality \u2014 The Silent Business Risk</h4><p>Data quality failures are responsible for an estimated $3.1 trillion in losses annually in the US alone (IBM, 2016). At FAANG scale, bad data can cause: incorrect recommendations (Netflix), miscounted ad impressions (Google billing error), wrong product availability (Amazon), or payments to the wrong accounts (banking). Data quality is not a nice-to-have \u2014 it is a business-critical function.</p><h4>The Five Dimensions of Data Quality</h4><table><tr><th>Dimension</th><td>Definition</td><td>Example check</td></tr><tr><th>Completeness</th><td>Are required values present?</td><td>WHERE user_id IS NULL \u2192 count should be 0</td></tr><tr><th>Uniqueness</th><td>No duplicate records?</td><td>COUNT(*) vs COUNT(DISTINCT pk) \u2192 must be equal</td></tr><tr><th>Validity</th><td>Values within expected range/format?</td><td>age BETWEEN 0 AND 120, email LIKE '%@%'</td></tr><tr><th>Consistency</th><td>Same fact represented the same way?</td><td>revenue = SUM(line_items) for every order</td></tr><tr><th>Timeliness</th><td>Data arrives when expected?</td><td>MAX(event_time) > NOW() - INTERVAL 1 HOUR</td></tr></table><h4>Data Quality Checks \u2014 Where to Put Them</h4><pre>Three places to add quality checks:\n\n1. INGESTION: validate data as it arrives from source\n   \u2192 Reject/quarantine rows that fail schema/range checks\n   \u2192 Stop bad data from entering the warehouse\n\n2. TRANSFORMATION: check intermediate results after each step\n   \u2192 Assert: if I join A to B, I should get X rows (\u00b1 5%)\n   \u2192 Check: did revenue change by >50% overnight? (anomaly)\n\n3. OUTPUT: check the final data before publishing to consumers\n   \u2192 Ensure tables are not empty\n   \u2192 Verify foreign key integrity\n   \u2192 Check business rules (all orders have a payment)</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 dbt + Schema Evolution</div><div class=\"rich\"><h4>dbt \u2014 Data Build Tool</h4><p>dbt (data build tool) is the standard tool for SQL-based transformations in modern data warehouses. It lets you write SQL SELECT queries (models), run them as CREATE TABLE / CREATE VIEW, test them automatically, document them, and track lineage.</p><pre># dbt model: models/revenue/daily_revenue.sql\n-- This SQL becomes a table in your data warehouse\n{{ config(materialized='table') }}\n\nSELECT\n    order_date,\n    SUM(revenue)     AS total_revenue,\n    COUNT(order_id)  AS order_count\nFROM {{ ref('stg_orders') }}   -- reference another dbt model\nWHERE order_status = 'COMPLETED'\nGROUP BY order_date\n\n# dbt test: models/revenue/schema.yml\n# Tests run automatically after each model build\nmodels:\n  - name: daily_revenue\n    columns:\n      - name: order_date\n        tests:\n          - not_null\n          - unique\n      - name: total_revenue\n        tests:\n          - not_null\n          - dbt_utils.expression_is_true:\n              expression: \"> 0\"  # revenue must be positive</pre><h4>Schema Evolution \u2014 Handling Schema Changes Safely</h4><pre># Adding a column: backwards compatible \u2014 safe\nALTER TABLE orders ADD COLUMN promo_code VARCHAR(50);\n-- Existing readers ignore the new column (Parquet, Avro do this automatically)\n\n# Removing a column: BREAKING CHANGE\n-- Readers that expect the column will fail!\n-- Strategy: deprecate first (document), migrate readers, then drop\n\n# Changing a column type: almost always BREAKING\n-- VARCHAR(50) \u2192 VARCHAR(200): safe (wider)\n-- INT \u2192 BIGINT: safe in Parquet (widening)\n-- INT \u2192 VARCHAR: breaking (downstream code fails)\n\n# Schema enforcement in Parquet (PySpark):\ndf.write.option(\"mergeSchema\", \"true\").parquet(\"s3://data/orders/\")\n# mergeSchema=True: allows new columns to be added to existing Parquet files\n# Missing columns in old files appear as NULL in combined reads</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Idempotency + Unit Testing</div><div class=\"rich\"><h4>Idempotency \u2014 The Most Important Pipeline Property</h4><p><strong>Idempotency:</strong> running a pipeline multiple times for the same date produces the same result. This is essential for: safe retries after failures, safe backfills, and debugging in production.</p><pre># \u274c NON-IDEMPOTENT: running twice creates duplicate rows\nINSERT INTO daily_revenue\nSELECT order_date, SUM(revenue) FROM orders GROUP BY order_date;\n-- Run twice for 2024-01-15 \u2192 two rows for 2024-01-15!\n\n# \u2705 IDEMPOTENT pattern 1: DELETE THEN INSERT (delete-and-replace)\nDELETE FROM daily_revenue WHERE order_date = '2024-01-15';\nINSERT INTO daily_revenue\nSELECT order_date, SUM(revenue) FROM orders WHERE order_date='2024-01-15' GROUP BY 1;\n\n# \u2705 IDEMPOTENT pattern 2: INSERT ... ON CONFLICT UPDATE\nINSERT INTO daily_revenue(order_date, total_revenue)\nSELECT order_date, SUM(revenue) FROM orders GROUP BY order_date\nON CONFLICT (order_date) DO UPDATE SET total_revenue=EXCLUDED.total_revenue;\n\n# \u2705 IDEMPOTENT pattern 3: overwrite the partition\ndf.write\\\n  .partitionBy(\"order_date\")\\\n  .mode(\"overwrite\")\\\n  .parquet(\"s3://revenue/\")\n# Rewrites only the affected partition \u2014 other partitions untouched</pre><h4>Unit Testing Data Pipelines</h4><pre>import pytest\nfrom pyspark.sql import SparkSession\n\ndef test_revenue_computation():\n    spark = SparkSession.builder.master(\"local[2]\").getOrCreate()\n\n    # Create small test dataframe\n    input_data = [\n        (\"2024-01-15\", \"COMPLETED\", 100.0),\n        (\"2024-01-15\", \"COMPLETED\", 50.0),\n        (\"2024-01-15\", \"CANCELLED\", 200.0),   # should be excluded\n    ]\n    orders_df = spark.createDataFrame(input_data, [\"order_date\",\"status\",\"revenue\"])\n\n    # Run the transformation\n    result = compute_daily_revenue(orders_df)\n\n    # Assert expected output\n    rows = result.collect()\n    assert len(rows) == 1, \"Expected one result row\"\n    assert rows[0].total_revenue == 150.0, \"Cancelled orders should be excluded\"\n    assert rows[0].order_date == \"2024-01-15\"</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Governance + Handling Bad Data in Production</div><div class=\"rich\"><h4>Data Governance \u2014 Lineage, Catalog, Access Control</h4><p>Data governance answers: what data do we have, what is it for, who should access it, where did it come from? At FAANG scale, this is essential because hundreds of engineers use the same data warehouse, and a new engineer must be able to find trustworthy data without asking.</p><ul><li><strong>Data Catalog</strong>: searchable index of all tables with descriptions, owners, update frequency. Tools: AWS Glue, Amundsen, DataHub.</li><li><strong>Data Lineage</strong>: visualizes the transformation chain: \"This dashboard metric comes from table X, which is built from source table Y, which ingests from API Z.\" Enables impact analysis: \"If API Z changes, what breaks?\"</li><li><strong>Column-level lineage</strong>: even more granular \u2014 traces which source columns contributed to each output column.</li><li><strong>PII governance</strong>: tables containing personal data (name, email, SSN) require special access controls, encryption, and retention policies.</li><li><strong>Row-level security</strong>: different rows visible to different users (e.g., each sales rep sees only their region's data).</li></ul><h4>FAANG Interview: \"How do you handle bad data in production?\"</h4><pre>Framework answer:\n\n1. DETECT: automated quality checks at every pipeline stage\n   - dbt tests on output models\n   - Great Expectations or custom SQL assertions\n   - Anomaly detection (revenue dropped 80%? \u2192 alert)\n\n2. ALERT: PagerDuty or Slack notification immediately\n   - Include: which table, which check failed, expected vs actual\n\n3. QUARANTINE: do not propagate bad data downstream\n   - Move failing records to quarantine table\n   - Good records continue to production\n\n4. INVESTIGATE: what is the root cause?\n   - Source system change? (schema evolution)\n   - Missing upstream data? (late arrival)\n   - Pipeline bug? (code change)\n\n5. FIX + BACKFILL: fix the pipeline, reprocess affected dates</pre></div></div></div>",
        "KeyConcepts": [
            "5 quality dimensions: Completeness, Uniqueness, Validity, Consistency, Timeliness.",
            "Add quality checks at: ingestion (reject bad data), transformation (assert intermediate results), output (verify before publish).",
            "dbt: write SQL SELECT \u2192 runs as table/view. Built-in tests: not_null, unique, accepted_values, relationships.",
            "Schema evolution: adding columns = safe. Removing/renaming columns = breaking. Changing types = usually breaking.",
            "Idempotency: pipeline can run multiple times for same date \u2192 same result. Critical for safe retries + backfills.",
            "Idempotent patterns: DELETE+INSERT, INSERT ON CONFLICT UPDATE, write.mode('overwrite').partitionBy().",
            "Unit testing: create small input DataFrame, run transformation, assert expected output. Use pytest + local Spark.",
            "Data governance: catalog (what exists), lineage (where it comes from), access control (who sees what)."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write 5 SQL quality checks for an orders table: NULL check, uniqueness, range validation (amount > 0), foreign key integrity, and timeliness (max order_date within last 2 hours).",
            "<strong>Step 2:</strong> Write a dbt model and schema.yml with at least 3 tests (not_null, unique, expression_is_true). Run dbt test.",
            "<strong>Step 3:</strong> Make a non-idempotent INSERT idempotent using all 3 patterns: DELETE+INSERT, ON CONFLICT, and Spark overwrite partition. Verify by running twice.",
            "<strong>Step 4:</strong> Write a pytest unit test for a Spark transformation that computes daily revenue. Include edge cases: empty input, all-NULL revenue, negative amounts."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Explain how to make a pipeline safe to re-run",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Explain how to make a pipeline safe to re-run \u2014 trace through 2-3 edge cases before writing any code.",
                "dbt test types: not_null, unique (built-in), accepted_values, relationships (built-in), custom SQL assertions.",
                "Non-idempotent pipelines are the #1 cause of duplicate data bugs at scale. Always design for idempotency.",
                "Great Expectations: Python library for defining and running data validation rules (similar to dbt tests but in Python).",
                "Anomaly detection alert: if today's row count < 80% or > 120% of last 7-day average \u2192 alert. Simple and powerful."
            ]
        },
        "HardProblem": "Boss Problem (Airbnb): You are the data quality lead. Your nightly ETL ingests from 20 source databases across 5 countries. (1) Design a quality check framework: what checks run at each stage, who is alerted when they fail, and how do you prevent bad data from reaching BI reports? (2) A source system in Germany changes the 'price' column from EUR decimal to STRING '\u20acX.XX' without warning. How does your system detect this? What happens to the pipeline? How do you fix it? (3) A data engineer accidentally writes non-idempotent code that ran 3 times \u2014 you now have 3x the rows for 2024-01-15 only. How do you fix production data?"
    },
    {
        "Week": 9,
        "Day": "Wednesday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Quality & Contracts",
        "SpecificTopic": "Schema Evolution",
        "ActionItem_Deliverable": "Explain Backward Compatibility in Avro",
        "LeetCodeProblem": "<strong>LC 1484 \u2013 Group Sold Products by Date</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>S3 Partitioning</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Data Quality Dimensions + Where to Check</div><div class=\"rich\"><h4>Data Quality \u2014 The Silent Business Risk</h4><p>Data quality failures are responsible for an estimated $3.1 trillion in losses annually in the US alone (IBM, 2016). At FAANG scale, bad data can cause: incorrect recommendations (Netflix), miscounted ad impressions (Google billing error), wrong product availability (Amazon), or payments to the wrong accounts (banking). Data quality is not a nice-to-have \u2014 it is a business-critical function.</p><h4>The Five Dimensions of Data Quality</h4><table><tr><th>Dimension</th><td>Definition</td><td>Example check</td></tr><tr><th>Completeness</th><td>Are required values present?</td><td>WHERE user_id IS NULL \u2192 count should be 0</td></tr><tr><th>Uniqueness</th><td>No duplicate records?</td><td>COUNT(*) vs COUNT(DISTINCT pk) \u2192 must be equal</td></tr><tr><th>Validity</th><td>Values within expected range/format?</td><td>age BETWEEN 0 AND 120, email LIKE '%@%'</td></tr><tr><th>Consistency</th><td>Same fact represented the same way?</td><td>revenue = SUM(line_items) for every order</td></tr><tr><th>Timeliness</th><td>Data arrives when expected?</td><td>MAX(event_time) > NOW() - INTERVAL 1 HOUR</td></tr></table><h4>Data Quality Checks \u2014 Where to Put Them</h4><pre>Three places to add quality checks:\n\n1. INGESTION: validate data as it arrives from source\n   \u2192 Reject/quarantine rows that fail schema/range checks\n   \u2192 Stop bad data from entering the warehouse\n\n2. TRANSFORMATION: check intermediate results after each step\n   \u2192 Assert: if I join A to B, I should get X rows (\u00b1 5%)\n   \u2192 Check: did revenue change by >50% overnight? (anomaly)\n\n3. OUTPUT: check the final data before publishing to consumers\n   \u2192 Ensure tables are not empty\n   \u2192 Verify foreign key integrity\n   \u2192 Check business rules (all orders have a payment)</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 dbt + Schema Evolution</div><div class=\"rich\"><h4>dbt \u2014 Data Build Tool</h4><p>dbt (data build tool) is the standard tool for SQL-based transformations in modern data warehouses. It lets you write SQL SELECT queries (models), run them as CREATE TABLE / CREATE VIEW, test them automatically, document them, and track lineage.</p><pre># dbt model: models/revenue/daily_revenue.sql\n-- This SQL becomes a table in your data warehouse\n{{ config(materialized='table') }}\n\nSELECT\n    order_date,\n    SUM(revenue)     AS total_revenue,\n    COUNT(order_id)  AS order_count\nFROM {{ ref('stg_orders') }}   -- reference another dbt model\nWHERE order_status = 'COMPLETED'\nGROUP BY order_date\n\n# dbt test: models/revenue/schema.yml\n# Tests run automatically after each model build\nmodels:\n  - name: daily_revenue\n    columns:\n      - name: order_date\n        tests:\n          - not_null\n          - unique\n      - name: total_revenue\n        tests:\n          - not_null\n          - dbt_utils.expression_is_true:\n              expression: \"> 0\"  # revenue must be positive</pre><h4>Schema Evolution \u2014 Handling Schema Changes Safely</h4><pre># Adding a column: backwards compatible \u2014 safe\nALTER TABLE orders ADD COLUMN promo_code VARCHAR(50);\n-- Existing readers ignore the new column (Parquet, Avro do this automatically)\n\n# Removing a column: BREAKING CHANGE\n-- Readers that expect the column will fail!\n-- Strategy: deprecate first (document), migrate readers, then drop\n\n# Changing a column type: almost always BREAKING\n-- VARCHAR(50) \u2192 VARCHAR(200): safe (wider)\n-- INT \u2192 BIGINT: safe in Parquet (widening)\n-- INT \u2192 VARCHAR: breaking (downstream code fails)\n\n# Schema enforcement in Parquet (PySpark):\ndf.write.option(\"mergeSchema\", \"true\").parquet(\"s3://data/orders/\")\n# mergeSchema=True: allows new columns to be added to existing Parquet files\n# Missing columns in old files appear as NULL in combined reads</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Idempotency + Unit Testing</div><div class=\"rich\"><h4>Idempotency \u2014 The Most Important Pipeline Property</h4><p><strong>Idempotency:</strong> running a pipeline multiple times for the same date produces the same result. This is essential for: safe retries after failures, safe backfills, and debugging in production.</p><pre># \u274c NON-IDEMPOTENT: running twice creates duplicate rows\nINSERT INTO daily_revenue\nSELECT order_date, SUM(revenue) FROM orders GROUP BY order_date;\n-- Run twice for 2024-01-15 \u2192 two rows for 2024-01-15!\n\n# \u2705 IDEMPOTENT pattern 1: DELETE THEN INSERT (delete-and-replace)\nDELETE FROM daily_revenue WHERE order_date = '2024-01-15';\nINSERT INTO daily_revenue\nSELECT order_date, SUM(revenue) FROM orders WHERE order_date='2024-01-15' GROUP BY 1;\n\n# \u2705 IDEMPOTENT pattern 2: INSERT ... ON CONFLICT UPDATE\nINSERT INTO daily_revenue(order_date, total_revenue)\nSELECT order_date, SUM(revenue) FROM orders GROUP BY order_date\nON CONFLICT (order_date) DO UPDATE SET total_revenue=EXCLUDED.total_revenue;\n\n# \u2705 IDEMPOTENT pattern 3: overwrite the partition\ndf.write\\\n  .partitionBy(\"order_date\")\\\n  .mode(\"overwrite\")\\\n  .parquet(\"s3://revenue/\")\n# Rewrites only the affected partition \u2014 other partitions untouched</pre><h4>Unit Testing Data Pipelines</h4><pre>import pytest\nfrom pyspark.sql import SparkSession\n\ndef test_revenue_computation():\n    spark = SparkSession.builder.master(\"local[2]\").getOrCreate()\n\n    # Create small test dataframe\n    input_data = [\n        (\"2024-01-15\", \"COMPLETED\", 100.0),\n        (\"2024-01-15\", \"COMPLETED\", 50.0),\n        (\"2024-01-15\", \"CANCELLED\", 200.0),   # should be excluded\n    ]\n    orders_df = spark.createDataFrame(input_data, [\"order_date\",\"status\",\"revenue\"])\n\n    # Run the transformation\n    result = compute_daily_revenue(orders_df)\n\n    # Assert expected output\n    rows = result.collect()\n    assert len(rows) == 1, \"Expected one result row\"\n    assert rows[0].total_revenue == 150.0, \"Cancelled orders should be excluded\"\n    assert rows[0].order_date == \"2024-01-15\"</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Governance + Handling Bad Data in Production</div><div class=\"rich\"><h4>Data Governance \u2014 Lineage, Catalog, Access Control</h4><p>Data governance answers: what data do we have, what is it for, who should access it, where did it come from? At FAANG scale, this is essential because hundreds of engineers use the same data warehouse, and a new engineer must be able to find trustworthy data without asking.</p><ul><li><strong>Data Catalog</strong>: searchable index of all tables with descriptions, owners, update frequency. Tools: AWS Glue, Amundsen, DataHub.</li><li><strong>Data Lineage</strong>: visualizes the transformation chain: \"This dashboard metric comes from table X, which is built from source table Y, which ingests from API Z.\" Enables impact analysis: \"If API Z changes, what breaks?\"</li><li><strong>Column-level lineage</strong>: even more granular \u2014 traces which source columns contributed to each output column.</li><li><strong>PII governance</strong>: tables containing personal data (name, email, SSN) require special access controls, encryption, and retention policies.</li><li><strong>Row-level security</strong>: different rows visible to different users (e.g., each sales rep sees only their region's data).</li></ul><h4>FAANG Interview: \"How do you handle bad data in production?\"</h4><pre>Framework answer:\n\n1. DETECT: automated quality checks at every pipeline stage\n   - dbt tests on output models\n   - Great Expectations or custom SQL assertions\n   - Anomaly detection (revenue dropped 80%? \u2192 alert)\n\n2. ALERT: PagerDuty or Slack notification immediately\n   - Include: which table, which check failed, expected vs actual\n\n3. QUARANTINE: do not propagate bad data downstream\n   - Move failing records to quarantine table\n   - Good records continue to production\n\n4. INVESTIGATE: what is the root cause?\n   - Source system change? (schema evolution)\n   - Missing upstream data? (late arrival)\n   - Pipeline bug? (code change)\n\n5. FIX + BACKFILL: fix the pipeline, reprocess affected dates</pre></div></div></div>",
        "KeyConcepts": [
            "5 quality dimensions: Completeness, Uniqueness, Validity, Consistency, Timeliness.",
            "Add quality checks at: ingestion (reject bad data), transformation (assert intermediate results), output (verify before publish).",
            "dbt: write SQL SELECT \u2192 runs as table/view. Built-in tests: not_null, unique, accepted_values, relationships.",
            "Schema evolution: adding columns = safe. Removing/renaming columns = breaking. Changing types = usually breaking.",
            "Idempotency: pipeline can run multiple times for same date \u2192 same result. Critical for safe retries + backfills.",
            "Idempotent patterns: DELETE+INSERT, INSERT ON CONFLICT UPDATE, write.mode('overwrite').partitionBy().",
            "Unit testing: create small input DataFrame, run transformation, assert expected output. Use pytest + local Spark.",
            "Data governance: catalog (what exists), lineage (where it comes from), access control (who sees what)."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write 5 SQL quality checks for an orders table: NULL check, uniqueness, range validation (amount > 0), foreign key integrity, and timeliness (max order_date within last 2 hours).",
            "<strong>Step 2:</strong> Write a dbt model and schema.yml with at least 3 tests (not_null, unique, expression_is_true). Run dbt test.",
            "<strong>Step 3:</strong> Make a non-idempotent INSERT idempotent using all 3 patterns: DELETE+INSERT, ON CONFLICT, and Spark overwrite partition. Verify by running twice.",
            "<strong>Step 4:</strong> Write a pytest unit test for a Spark transformation that computes daily revenue. Include edge cases: empty input, all-NULL revenue, negative amounts."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Explain Backward Compatibility in Avro",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Explain Backward Compatibility in Avro \u2014 trace through 2-3 edge cases before writing any code.",
                "dbt test types: not_null, unique (built-in), accepted_values, relationships (built-in), custom SQL assertions.",
                "Non-idempotent pipelines are the #1 cause of duplicate data bugs at scale. Always design for idempotency.",
                "Great Expectations: Python library for defining and running data validation rules (similar to dbt tests but in Python).",
                "Anomaly detection alert: if today's row count < 80% or > 120% of last 7-day average \u2192 alert. Simple and powerful."
            ]
        },
        "HardProblem": "Boss Problem (Airbnb): You are the data quality lead. Your nightly ETL ingests from 20 source databases across 5 countries. (1) Design a quality check framework: what checks run at each stage, who is alerted when they fail, and how do you prevent bad data from reaching BI reports? (2) A source system in Germany changes the 'price' column from EUR decimal to STRING '\u20acX.XX' without warning. How does your system detect this? What happens to the pipeline? How do you fix it? (3) A data engineer accidentally writes non-idempotent code that ran 3 times \u2014 you now have 3x the rows for 2024-01-15 only. How do you fix production data?"
    },
    {
        "Week": 9,
        "Day": "Thursday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Quality & Contracts",
        "SpecificTopic": "Unit Testing",
        "ActionItem_Deliverable": "Write a pytest unit test for PySpark function",
        "LeetCodeProblem": "<strong>LC 1693 \u2013 Daily Leads and Partners</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Surrogate Keys</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Data Quality Dimensions + Where to Check</div><div class=\"rich\"><h4>Data Quality \u2014 The Silent Business Risk</h4><p>Data quality failures are responsible for an estimated $3.1 trillion in losses annually in the US alone (IBM, 2016). At FAANG scale, bad data can cause: incorrect recommendations (Netflix), miscounted ad impressions (Google billing error), wrong product availability (Amazon), or payments to the wrong accounts (banking). Data quality is not a nice-to-have \u2014 it is a business-critical function.</p><h4>The Five Dimensions of Data Quality</h4><table><tr><th>Dimension</th><td>Definition</td><td>Example check</td></tr><tr><th>Completeness</th><td>Are required values present?</td><td>WHERE user_id IS NULL \u2192 count should be 0</td></tr><tr><th>Uniqueness</th><td>No duplicate records?</td><td>COUNT(*) vs COUNT(DISTINCT pk) \u2192 must be equal</td></tr><tr><th>Validity</th><td>Values within expected range/format?</td><td>age BETWEEN 0 AND 120, email LIKE '%@%'</td></tr><tr><th>Consistency</th><td>Same fact represented the same way?</td><td>revenue = SUM(line_items) for every order</td></tr><tr><th>Timeliness</th><td>Data arrives when expected?</td><td>MAX(event_time) > NOW() - INTERVAL 1 HOUR</td></tr></table><h4>Data Quality Checks \u2014 Where to Put Them</h4><pre>Three places to add quality checks:\n\n1. INGESTION: validate data as it arrives from source\n   \u2192 Reject/quarantine rows that fail schema/range checks\n   \u2192 Stop bad data from entering the warehouse\n\n2. TRANSFORMATION: check intermediate results after each step\n   \u2192 Assert: if I join A to B, I should get X rows (\u00b1 5%)\n   \u2192 Check: did revenue change by >50% overnight? (anomaly)\n\n3. OUTPUT: check the final data before publishing to consumers\n   \u2192 Ensure tables are not empty\n   \u2192 Verify foreign key integrity\n   \u2192 Check business rules (all orders have a payment)</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 dbt + Schema Evolution</div><div class=\"rich\"><h4>dbt \u2014 Data Build Tool</h4><p>dbt (data build tool) is the standard tool for SQL-based transformations in modern data warehouses. It lets you write SQL SELECT queries (models), run them as CREATE TABLE / CREATE VIEW, test them automatically, document them, and track lineage.</p><pre># dbt model: models/revenue/daily_revenue.sql\n-- This SQL becomes a table in your data warehouse\n{{ config(materialized='table') }}\n\nSELECT\n    order_date,\n    SUM(revenue)     AS total_revenue,\n    COUNT(order_id)  AS order_count\nFROM {{ ref('stg_orders') }}   -- reference another dbt model\nWHERE order_status = 'COMPLETED'\nGROUP BY order_date\n\n# dbt test: models/revenue/schema.yml\n# Tests run automatically after each model build\nmodels:\n  - name: daily_revenue\n    columns:\n      - name: order_date\n        tests:\n          - not_null\n          - unique\n      - name: total_revenue\n        tests:\n          - not_null\n          - dbt_utils.expression_is_true:\n              expression: \"> 0\"  # revenue must be positive</pre><h4>Schema Evolution \u2014 Handling Schema Changes Safely</h4><pre># Adding a column: backwards compatible \u2014 safe\nALTER TABLE orders ADD COLUMN promo_code VARCHAR(50);\n-- Existing readers ignore the new column (Parquet, Avro do this automatically)\n\n# Removing a column: BREAKING CHANGE\n-- Readers that expect the column will fail!\n-- Strategy: deprecate first (document), migrate readers, then drop\n\n# Changing a column type: almost always BREAKING\n-- VARCHAR(50) \u2192 VARCHAR(200): safe (wider)\n-- INT \u2192 BIGINT: safe in Parquet (widening)\n-- INT \u2192 VARCHAR: breaking (downstream code fails)\n\n# Schema enforcement in Parquet (PySpark):\ndf.write.option(\"mergeSchema\", \"true\").parquet(\"s3://data/orders/\")\n# mergeSchema=True: allows new columns to be added to existing Parquet files\n# Missing columns in old files appear as NULL in combined reads</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Idempotency + Unit Testing</div><div class=\"rich\"><h4>Idempotency \u2014 The Most Important Pipeline Property</h4><p><strong>Idempotency:</strong> running a pipeline multiple times for the same date produces the same result. This is essential for: safe retries after failures, safe backfills, and debugging in production.</p><pre># \u274c NON-IDEMPOTENT: running twice creates duplicate rows\nINSERT INTO daily_revenue\nSELECT order_date, SUM(revenue) FROM orders GROUP BY order_date;\n-- Run twice for 2024-01-15 \u2192 two rows for 2024-01-15!\n\n# \u2705 IDEMPOTENT pattern 1: DELETE THEN INSERT (delete-and-replace)\nDELETE FROM daily_revenue WHERE order_date = '2024-01-15';\nINSERT INTO daily_revenue\nSELECT order_date, SUM(revenue) FROM orders WHERE order_date='2024-01-15' GROUP BY 1;\n\n# \u2705 IDEMPOTENT pattern 2: INSERT ... ON CONFLICT UPDATE\nINSERT INTO daily_revenue(order_date, total_revenue)\nSELECT order_date, SUM(revenue) FROM orders GROUP BY order_date\nON CONFLICT (order_date) DO UPDATE SET total_revenue=EXCLUDED.total_revenue;\n\n# \u2705 IDEMPOTENT pattern 3: overwrite the partition\ndf.write\\\n  .partitionBy(\"order_date\")\\\n  .mode(\"overwrite\")\\\n  .parquet(\"s3://revenue/\")\n# Rewrites only the affected partition \u2014 other partitions untouched</pre><h4>Unit Testing Data Pipelines</h4><pre>import pytest\nfrom pyspark.sql import SparkSession\n\ndef test_revenue_computation():\n    spark = SparkSession.builder.master(\"local[2]\").getOrCreate()\n\n    # Create small test dataframe\n    input_data = [\n        (\"2024-01-15\", \"COMPLETED\", 100.0),\n        (\"2024-01-15\", \"COMPLETED\", 50.0),\n        (\"2024-01-15\", \"CANCELLED\", 200.0),   # should be excluded\n    ]\n    orders_df = spark.createDataFrame(input_data, [\"order_date\",\"status\",\"revenue\"])\n\n    # Run the transformation\n    result = compute_daily_revenue(orders_df)\n\n    # Assert expected output\n    rows = result.collect()\n    assert len(rows) == 1, \"Expected one result row\"\n    assert rows[0].total_revenue == 150.0, \"Cancelled orders should be excluded\"\n    assert rows[0].order_date == \"2024-01-15\"</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Governance + Handling Bad Data in Production</div><div class=\"rich\"><h4>Data Governance \u2014 Lineage, Catalog, Access Control</h4><p>Data governance answers: what data do we have, what is it for, who should access it, where did it come from? At FAANG scale, this is essential because hundreds of engineers use the same data warehouse, and a new engineer must be able to find trustworthy data without asking.</p><ul><li><strong>Data Catalog</strong>: searchable index of all tables with descriptions, owners, update frequency. Tools: AWS Glue, Amundsen, DataHub.</li><li><strong>Data Lineage</strong>: visualizes the transformation chain: \"This dashboard metric comes from table X, which is built from source table Y, which ingests from API Z.\" Enables impact analysis: \"If API Z changes, what breaks?\"</li><li><strong>Column-level lineage</strong>: even more granular \u2014 traces which source columns contributed to each output column.</li><li><strong>PII governance</strong>: tables containing personal data (name, email, SSN) require special access controls, encryption, and retention policies.</li><li><strong>Row-level security</strong>: different rows visible to different users (e.g., each sales rep sees only their region's data).</li></ul><h4>FAANG Interview: \"How do you handle bad data in production?\"</h4><pre>Framework answer:\n\n1. DETECT: automated quality checks at every pipeline stage\n   - dbt tests on output models\n   - Great Expectations or custom SQL assertions\n   - Anomaly detection (revenue dropped 80%? \u2192 alert)\n\n2. ALERT: PagerDuty or Slack notification immediately\n   - Include: which table, which check failed, expected vs actual\n\n3. QUARANTINE: do not propagate bad data downstream\n   - Move failing records to quarantine table\n   - Good records continue to production\n\n4. INVESTIGATE: what is the root cause?\n   - Source system change? (schema evolution)\n   - Missing upstream data? (late arrival)\n   - Pipeline bug? (code change)\n\n5. FIX + BACKFILL: fix the pipeline, reprocess affected dates</pre></div></div></div>",
        "KeyConcepts": [
            "5 quality dimensions: Completeness, Uniqueness, Validity, Consistency, Timeliness.",
            "Add quality checks at: ingestion (reject bad data), transformation (assert intermediate results), output (verify before publish).",
            "dbt: write SQL SELECT \u2192 runs as table/view. Built-in tests: not_null, unique, accepted_values, relationships.",
            "Schema evolution: adding columns = safe. Removing/renaming columns = breaking. Changing types = usually breaking.",
            "Idempotency: pipeline can run multiple times for same date \u2192 same result. Critical for safe retries + backfills.",
            "Idempotent patterns: DELETE+INSERT, INSERT ON CONFLICT UPDATE, write.mode('overwrite').partitionBy().",
            "Unit testing: create small input DataFrame, run transformation, assert expected output. Use pytest + local Spark.",
            "Data governance: catalog (what exists), lineage (where it comes from), access control (who sees what)."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write 5 SQL quality checks for an orders table: NULL check, uniqueness, range validation (amount > 0), foreign key integrity, and timeliness (max order_date within last 2 hours).",
            "<strong>Step 2:</strong> Write a dbt model and schema.yml with at least 3 tests (not_null, unique, expression_is_true). Run dbt test.",
            "<strong>Step 3:</strong> Make a non-idempotent INSERT idempotent using all 3 patterns: DELETE+INSERT, ON CONFLICT, and Spark overwrite partition. Verify by running twice.",
            "<strong>Step 4:</strong> Write a pytest unit test for a Spark transformation that computes daily revenue. Include edge cases: empty input, all-NULL revenue, negative amounts."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Write a pytest unit test for PySpark function",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Write a pytest unit test for PySpark function \u2014 trace through 2-3 edge cases before writing any code.",
                "dbt test types: not_null, unique (built-in), accepted_values, relationships (built-in), custom SQL assertions.",
                "Non-idempotent pipelines are the #1 cause of duplicate data bugs at scale. Always design for idempotency.",
                "Great Expectations: Python library for defining and running data validation rules (similar to dbt tests but in Python).",
                "Anomaly detection alert: if today's row count < 80% or > 120% of last 7-day average \u2192 alert. Simple and powerful."
            ]
        },
        "HardProblem": "Boss Problem (Airbnb): You are the data quality lead. Your nightly ETL ingests from 20 source databases across 5 countries. (1) Design a quality check framework: what checks run at each stage, who is alerted when they fail, and how do you prevent bad data from reaching BI reports? (2) A source system in Germany changes the 'price' column from EUR decimal to STRING '\u20acX.XX' without warning. How does your system detect this? What happens to the pipeline? How do you fix it? (3) A data engineer accidentally writes non-idempotent code that ran 3 times \u2014 you now have 3x the rows for 2024-01-15 only. How do you fix production data?"
    },
    {
        "Week": 9,
        "Day": "Friday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Quality & Contracts",
        "SpecificTopic": "Governance",
        "ActionItem_Deliverable": "Design strategy to hash/mask PII (Email/SSN)",
        "LeetCodeProblem": "<strong>LC 1303 \u2013 Find the Team Size</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Normalization</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Data Quality Dimensions + Where to Check</div><div class=\"rich\"><h4>Data Quality \u2014 The Silent Business Risk</h4><p>Data quality failures are responsible for an estimated $3.1 trillion in losses annually in the US alone (IBM, 2016). At FAANG scale, bad data can cause: incorrect recommendations (Netflix), miscounted ad impressions (Google billing error), wrong product availability (Amazon), or payments to the wrong accounts (banking). Data quality is not a nice-to-have \u2014 it is a business-critical function.</p><h4>The Five Dimensions of Data Quality</h4><table><tr><th>Dimension</th><td>Definition</td><td>Example check</td></tr><tr><th>Completeness</th><td>Are required values present?</td><td>WHERE user_id IS NULL \u2192 count should be 0</td></tr><tr><th>Uniqueness</th><td>No duplicate records?</td><td>COUNT(*) vs COUNT(DISTINCT pk) \u2192 must be equal</td></tr><tr><th>Validity</th><td>Values within expected range/format?</td><td>age BETWEEN 0 AND 120, email LIKE '%@%'</td></tr><tr><th>Consistency</th><td>Same fact represented the same way?</td><td>revenue = SUM(line_items) for every order</td></tr><tr><th>Timeliness</th><td>Data arrives when expected?</td><td>MAX(event_time) > NOW() - INTERVAL 1 HOUR</td></tr></table><h4>Data Quality Checks \u2014 Where to Put Them</h4><pre>Three places to add quality checks:\n\n1. INGESTION: validate data as it arrives from source\n   \u2192 Reject/quarantine rows that fail schema/range checks\n   \u2192 Stop bad data from entering the warehouse\n\n2. TRANSFORMATION: check intermediate results after each step\n   \u2192 Assert: if I join A to B, I should get X rows (\u00b1 5%)\n   \u2192 Check: did revenue change by >50% overnight? (anomaly)\n\n3. OUTPUT: check the final data before publishing to consumers\n   \u2192 Ensure tables are not empty\n   \u2192 Verify foreign key integrity\n   \u2192 Check business rules (all orders have a payment)</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 dbt + Schema Evolution</div><div class=\"rich\"><h4>dbt \u2014 Data Build Tool</h4><p>dbt (data build tool) is the standard tool for SQL-based transformations in modern data warehouses. It lets you write SQL SELECT queries (models), run them as CREATE TABLE / CREATE VIEW, test them automatically, document them, and track lineage.</p><pre># dbt model: models/revenue/daily_revenue.sql\n-- This SQL becomes a table in your data warehouse\n{{ config(materialized='table') }}\n\nSELECT\n    order_date,\n    SUM(revenue)     AS total_revenue,\n    COUNT(order_id)  AS order_count\nFROM {{ ref('stg_orders') }}   -- reference another dbt model\nWHERE order_status = 'COMPLETED'\nGROUP BY order_date\n\n# dbt test: models/revenue/schema.yml\n# Tests run automatically after each model build\nmodels:\n  - name: daily_revenue\n    columns:\n      - name: order_date\n        tests:\n          - not_null\n          - unique\n      - name: total_revenue\n        tests:\n          - not_null\n          - dbt_utils.expression_is_true:\n              expression: \"> 0\"  # revenue must be positive</pre><h4>Schema Evolution \u2014 Handling Schema Changes Safely</h4><pre># Adding a column: backwards compatible \u2014 safe\nALTER TABLE orders ADD COLUMN promo_code VARCHAR(50);\n-- Existing readers ignore the new column (Parquet, Avro do this automatically)\n\n# Removing a column: BREAKING CHANGE\n-- Readers that expect the column will fail!\n-- Strategy: deprecate first (document), migrate readers, then drop\n\n# Changing a column type: almost always BREAKING\n-- VARCHAR(50) \u2192 VARCHAR(200): safe (wider)\n-- INT \u2192 BIGINT: safe in Parquet (widening)\n-- INT \u2192 VARCHAR: breaking (downstream code fails)\n\n# Schema enforcement in Parquet (PySpark):\ndf.write.option(\"mergeSchema\", \"true\").parquet(\"s3://data/orders/\")\n# mergeSchema=True: allows new columns to be added to existing Parquet files\n# Missing columns in old files appear as NULL in combined reads</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Idempotency + Unit Testing</div><div class=\"rich\"><h4>Idempotency \u2014 The Most Important Pipeline Property</h4><p><strong>Idempotency:</strong> running a pipeline multiple times for the same date produces the same result. This is essential for: safe retries after failures, safe backfills, and debugging in production.</p><pre># \u274c NON-IDEMPOTENT: running twice creates duplicate rows\nINSERT INTO daily_revenue\nSELECT order_date, SUM(revenue) FROM orders GROUP BY order_date;\n-- Run twice for 2024-01-15 \u2192 two rows for 2024-01-15!\n\n# \u2705 IDEMPOTENT pattern 1: DELETE THEN INSERT (delete-and-replace)\nDELETE FROM daily_revenue WHERE order_date = '2024-01-15';\nINSERT INTO daily_revenue\nSELECT order_date, SUM(revenue) FROM orders WHERE order_date='2024-01-15' GROUP BY 1;\n\n# \u2705 IDEMPOTENT pattern 2: INSERT ... ON CONFLICT UPDATE\nINSERT INTO daily_revenue(order_date, total_revenue)\nSELECT order_date, SUM(revenue) FROM orders GROUP BY order_date\nON CONFLICT (order_date) DO UPDATE SET total_revenue=EXCLUDED.total_revenue;\n\n# \u2705 IDEMPOTENT pattern 3: overwrite the partition\ndf.write\\\n  .partitionBy(\"order_date\")\\\n  .mode(\"overwrite\")\\\n  .parquet(\"s3://revenue/\")\n# Rewrites only the affected partition \u2014 other partitions untouched</pre><h4>Unit Testing Data Pipelines</h4><pre>import pytest\nfrom pyspark.sql import SparkSession\n\ndef test_revenue_computation():\n    spark = SparkSession.builder.master(\"local[2]\").getOrCreate()\n\n    # Create small test dataframe\n    input_data = [\n        (\"2024-01-15\", \"COMPLETED\", 100.0),\n        (\"2024-01-15\", \"COMPLETED\", 50.0),\n        (\"2024-01-15\", \"CANCELLED\", 200.0),   # should be excluded\n    ]\n    orders_df = spark.createDataFrame(input_data, [\"order_date\",\"status\",\"revenue\"])\n\n    # Run the transformation\n    result = compute_daily_revenue(orders_df)\n\n    # Assert expected output\n    rows = result.collect()\n    assert len(rows) == 1, \"Expected one result row\"\n    assert rows[0].total_revenue == 150.0, \"Cancelled orders should be excluded\"\n    assert rows[0].order_date == \"2024-01-15\"</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Governance + Handling Bad Data in Production</div><div class=\"rich\"><h4>Data Governance \u2014 Lineage, Catalog, Access Control</h4><p>Data governance answers: what data do we have, what is it for, who should access it, where did it come from? At FAANG scale, this is essential because hundreds of engineers use the same data warehouse, and a new engineer must be able to find trustworthy data without asking.</p><ul><li><strong>Data Catalog</strong>: searchable index of all tables with descriptions, owners, update frequency. Tools: AWS Glue, Amundsen, DataHub.</li><li><strong>Data Lineage</strong>: visualizes the transformation chain: \"This dashboard metric comes from table X, which is built from source table Y, which ingests from API Z.\" Enables impact analysis: \"If API Z changes, what breaks?\"</li><li><strong>Column-level lineage</strong>: even more granular \u2014 traces which source columns contributed to each output column.</li><li><strong>PII governance</strong>: tables containing personal data (name, email, SSN) require special access controls, encryption, and retention policies.</li><li><strong>Row-level security</strong>: different rows visible to different users (e.g., each sales rep sees only their region's data).</li></ul><h4>FAANG Interview: \"How do you handle bad data in production?\"</h4><pre>Framework answer:\n\n1. DETECT: automated quality checks at every pipeline stage\n   - dbt tests on output models\n   - Great Expectations or custom SQL assertions\n   - Anomaly detection (revenue dropped 80%? \u2192 alert)\n\n2. ALERT: PagerDuty or Slack notification immediately\n   - Include: which table, which check failed, expected vs actual\n\n3. QUARANTINE: do not propagate bad data downstream\n   - Move failing records to quarantine table\n   - Good records continue to production\n\n4. INVESTIGATE: what is the root cause?\n   - Source system change? (schema evolution)\n   - Missing upstream data? (late arrival)\n   - Pipeline bug? (code change)\n\n5. FIX + BACKFILL: fix the pipeline, reprocess affected dates</pre></div></div></div>",
        "KeyConcepts": [
            "5 quality dimensions: Completeness, Uniqueness, Validity, Consistency, Timeliness.",
            "Add quality checks at: ingestion (reject bad data), transformation (assert intermediate results), output (verify before publish).",
            "dbt: write SQL SELECT \u2192 runs as table/view. Built-in tests: not_null, unique, accepted_values, relationships.",
            "Schema evolution: adding columns = safe. Removing/renaming columns = breaking. Changing types = usually breaking.",
            "Idempotency: pipeline can run multiple times for same date \u2192 same result. Critical for safe retries + backfills.",
            "Idempotent patterns: DELETE+INSERT, INSERT ON CONFLICT UPDATE, write.mode('overwrite').partitionBy().",
            "Unit testing: create small input DataFrame, run transformation, assert expected output. Use pytest + local Spark.",
            "Data governance: catalog (what exists), lineage (where it comes from), access control (who sees what)."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write 5 SQL quality checks for an orders table: NULL check, uniqueness, range validation (amount > 0), foreign key integrity, and timeliness (max order_date within last 2 hours).",
            "<strong>Step 2:</strong> Write a dbt model and schema.yml with at least 3 tests (not_null, unique, expression_is_true). Run dbt test.",
            "<strong>Step 3:</strong> Make a non-idempotent INSERT idempotent using all 3 patterns: DELETE+INSERT, ON CONFLICT, and Spark overwrite partition. Verify by running twice.",
            "<strong>Step 4:</strong> Write a pytest unit test for a Spark transformation that computes daily revenue. Include edge cases: empty input, all-NULL revenue, negative amounts."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Design strategy to hash/mask PII (Email/SSN)",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Design strategy to hash/mask PII (Email/SSN) \u2014 trace through 2-3 edge cases before writing any code.",
                "dbt test types: not_null, unique (built-in), accepted_values, relationships (built-in), custom SQL assertions.",
                "Non-idempotent pipelines are the #1 cause of duplicate data bugs at scale. Always design for idempotency.",
                "Great Expectations: Python library for defining and running data validation rules (similar to dbt tests but in Python).",
                "Anomaly detection alert: if today's row count < 80% or > 120% of last 7-day average \u2192 alert. Simple and powerful."
            ]
        },
        "HardProblem": "Boss Problem (Airbnb): You are the data quality lead. Your nightly ETL ingests from 20 source databases across 5 countries. (1) Design a quality check framework: what checks run at each stage, who is alerted when they fail, and how do you prevent bad data from reaching BI reports? (2) A source system in Germany changes the 'price' column from EUR decimal to STRING '\u20acX.XX' without warning. How does your system detect this? What happens to the pipeline? How do you fix it? (3) A data engineer accidentally writes non-idempotent code that ran 3 times \u2014 you now have 3x the rows for 2024-01-15 only. How do you fix production data?"
    },
    {
        "Week": 9,
        "Day": "Saturday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Quality & Contracts + dbt",
        "SpecificTopic": "dbt Intro + Mock #2",
        "ActionItem_Deliverable": "Install dbt-core, run first model + Full Mock Interview Loop #2",
        "LeetCodeProblem": "<strong>LC 1045 \u2013 Customers Who Bought All Products</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": "NEW: dbt intro + Mock #2 added",
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Data Formats</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Data Quality Dimensions + Where to Check</div><div class=\"rich\"><h4>Data Quality \u2014 The Silent Business Risk</h4><p>Data quality failures are responsible for an estimated $3.1 trillion in losses annually in the US alone (IBM, 2016). At FAANG scale, bad data can cause: incorrect recommendations (Netflix), miscounted ad impressions (Google billing error), wrong product availability (Amazon), or payments to the wrong accounts (banking). Data quality is not a nice-to-have \u2014 it is a business-critical function.</p><h4>The Five Dimensions of Data Quality</h4><table><tr><th>Dimension</th><td>Definition</td><td>Example check</td></tr><tr><th>Completeness</th><td>Are required values present?</td><td>WHERE user_id IS NULL \u2192 count should be 0</td></tr><tr><th>Uniqueness</th><td>No duplicate records?</td><td>COUNT(*) vs COUNT(DISTINCT pk) \u2192 must be equal</td></tr><tr><th>Validity</th><td>Values within expected range/format?</td><td>age BETWEEN 0 AND 120, email LIKE '%@%'</td></tr><tr><th>Consistency</th><td>Same fact represented the same way?</td><td>revenue = SUM(line_items) for every order</td></tr><tr><th>Timeliness</th><td>Data arrives when expected?</td><td>MAX(event_time) > NOW() - INTERVAL 1 HOUR</td></tr></table><h4>Data Quality Checks \u2014 Where to Put Them</h4><pre>Three places to add quality checks:\n\n1. INGESTION: validate data as it arrives from source\n   \u2192 Reject/quarantine rows that fail schema/range checks\n   \u2192 Stop bad data from entering the warehouse\n\n2. TRANSFORMATION: check intermediate results after each step\n   \u2192 Assert: if I join A to B, I should get X rows (\u00b1 5%)\n   \u2192 Check: did revenue change by >50% overnight? (anomaly)\n\n3. OUTPUT: check the final data before publishing to consumers\n   \u2192 Ensure tables are not empty\n   \u2192 Verify foreign key integrity\n   \u2192 Check business rules (all orders have a payment)</pre></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 dbt + Schema Evolution</div><div class=\"rich\"><h4>dbt \u2014 Data Build Tool</h4><p>dbt (data build tool) is the standard tool for SQL-based transformations in modern data warehouses. It lets you write SQL SELECT queries (models), run them as CREATE TABLE / CREATE VIEW, test them automatically, document them, and track lineage.</p><pre># dbt model: models/revenue/daily_revenue.sql\n-- This SQL becomes a table in your data warehouse\n{{ config(materialized='table') }}\n\nSELECT\n    order_date,\n    SUM(revenue)     AS total_revenue,\n    COUNT(order_id)  AS order_count\nFROM {{ ref('stg_orders') }}   -- reference another dbt model\nWHERE order_status = 'COMPLETED'\nGROUP BY order_date\n\n# dbt test: models/revenue/schema.yml\n# Tests run automatically after each model build\nmodels:\n  - name: daily_revenue\n    columns:\n      - name: order_date\n        tests:\n          - not_null\n          - unique\n      - name: total_revenue\n        tests:\n          - not_null\n          - dbt_utils.expression_is_true:\n              expression: \"> 0\"  # revenue must be positive</pre><h4>Schema Evolution \u2014 Handling Schema Changes Safely</h4><pre># Adding a column: backwards compatible \u2014 safe\nALTER TABLE orders ADD COLUMN promo_code VARCHAR(50);\n-- Existing readers ignore the new column (Parquet, Avro do this automatically)\n\n# Removing a column: BREAKING CHANGE\n-- Readers that expect the column will fail!\n-- Strategy: deprecate first (document), migrate readers, then drop\n\n# Changing a column type: almost always BREAKING\n-- VARCHAR(50) \u2192 VARCHAR(200): safe (wider)\n-- INT \u2192 BIGINT: safe in Parquet (widening)\n-- INT \u2192 VARCHAR: breaking (downstream code fails)\n\n# Schema enforcement in Parquet (PySpark):\ndf.write.option(\"mergeSchema\", \"true\").parquet(\"s3://data/orders/\")\n# mergeSchema=True: allows new columns to be added to existing Parquet files\n# Missing columns in old files appear as NULL in combined reads</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Idempotency + Unit Testing</div><div class=\"rich\"><h4>Idempotency \u2014 The Most Important Pipeline Property</h4><p><strong>Idempotency:</strong> running a pipeline multiple times for the same date produces the same result. This is essential for: safe retries after failures, safe backfills, and debugging in production.</p><pre># \u274c NON-IDEMPOTENT: running twice creates duplicate rows\nINSERT INTO daily_revenue\nSELECT order_date, SUM(revenue) FROM orders GROUP BY order_date;\n-- Run twice for 2024-01-15 \u2192 two rows for 2024-01-15!\n\n# \u2705 IDEMPOTENT pattern 1: DELETE THEN INSERT (delete-and-replace)\nDELETE FROM daily_revenue WHERE order_date = '2024-01-15';\nINSERT INTO daily_revenue\nSELECT order_date, SUM(revenue) FROM orders WHERE order_date='2024-01-15' GROUP BY 1;\n\n# \u2705 IDEMPOTENT pattern 2: INSERT ... ON CONFLICT UPDATE\nINSERT INTO daily_revenue(order_date, total_revenue)\nSELECT order_date, SUM(revenue) FROM orders GROUP BY order_date\nON CONFLICT (order_date) DO UPDATE SET total_revenue=EXCLUDED.total_revenue;\n\n# \u2705 IDEMPOTENT pattern 3: overwrite the partition\ndf.write\\\n  .partitionBy(\"order_date\")\\\n  .mode(\"overwrite\")\\\n  .parquet(\"s3://revenue/\")\n# Rewrites only the affected partition \u2014 other partitions untouched</pre><h4>Unit Testing Data Pipelines</h4><pre>import pytest\nfrom pyspark.sql import SparkSession\n\ndef test_revenue_computation():\n    spark = SparkSession.builder.master(\"local[2]\").getOrCreate()\n\n    # Create small test dataframe\n    input_data = [\n        (\"2024-01-15\", \"COMPLETED\", 100.0),\n        (\"2024-01-15\", \"COMPLETED\", 50.0),\n        (\"2024-01-15\", \"CANCELLED\", 200.0),   # should be excluded\n    ]\n    orders_df = spark.createDataFrame(input_data, [\"order_date\",\"status\",\"revenue\"])\n\n    # Run the transformation\n    result = compute_daily_revenue(orders_df)\n\n    # Assert expected output\n    rows = result.collect()\n    assert len(rows) == 1, \"Expected one result row\"\n    assert rows[0].total_revenue == 150.0, \"Cancelled orders should be excluded\"\n    assert rows[0].order_date == \"2024-01-15\"</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Governance + Handling Bad Data in Production</div><div class=\"rich\"><h4>Data Governance \u2014 Lineage, Catalog, Access Control</h4><p>Data governance answers: what data do we have, what is it for, who should access it, where did it come from? At FAANG scale, this is essential because hundreds of engineers use the same data warehouse, and a new engineer must be able to find trustworthy data without asking.</p><ul><li><strong>Data Catalog</strong>: searchable index of all tables with descriptions, owners, update frequency. Tools: AWS Glue, Amundsen, DataHub.</li><li><strong>Data Lineage</strong>: visualizes the transformation chain: \"This dashboard metric comes from table X, which is built from source table Y, which ingests from API Z.\" Enables impact analysis: \"If API Z changes, what breaks?\"</li><li><strong>Column-level lineage</strong>: even more granular \u2014 traces which source columns contributed to each output column.</li><li><strong>PII governance</strong>: tables containing personal data (name, email, SSN) require special access controls, encryption, and retention policies.</li><li><strong>Row-level security</strong>: different rows visible to different users (e.g., each sales rep sees only their region's data).</li></ul><h4>FAANG Interview: \"How do you handle bad data in production?\"</h4><pre>Framework answer:\n\n1. DETECT: automated quality checks at every pipeline stage\n   - dbt tests on output models\n   - Great Expectations or custom SQL assertions\n   - Anomaly detection (revenue dropped 80%? \u2192 alert)\n\n2. ALERT: PagerDuty or Slack notification immediately\n   - Include: which table, which check failed, expected vs actual\n\n3. QUARANTINE: do not propagate bad data downstream\n   - Move failing records to quarantine table\n   - Good records continue to production\n\n4. INVESTIGATE: what is the root cause?\n   - Source system change? (schema evolution)\n   - Missing upstream data? (late arrival)\n   - Pipeline bug? (code change)\n\n5. FIX + BACKFILL: fix the pipeline, reprocess affected dates</pre></div></div></div>",
        "KeyConcepts": [
            "5 quality dimensions: Completeness, Uniqueness, Validity, Consistency, Timeliness.",
            "Add quality checks at: ingestion (reject bad data), transformation (assert intermediate results), output (verify before publish).",
            "dbt: write SQL SELECT \u2192 runs as table/view. Built-in tests: not_null, unique, accepted_values, relationships.",
            "Schema evolution: adding columns = safe. Removing/renaming columns = breaking. Changing types = usually breaking.",
            "Idempotency: pipeline can run multiple times for same date \u2192 same result. Critical for safe retries + backfills.",
            "Idempotent patterns: DELETE+INSERT, INSERT ON CONFLICT UPDATE, write.mode('overwrite').partitionBy().",
            "Unit testing: create small input DataFrame, run transformation, assert expected output. Use pytest + local Spark.",
            "Data governance: catalog (what exists), lineage (where it comes from), access control (who sees what)."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write 5 SQL quality checks for an orders table: NULL check, uniqueness, range validation (amount > 0), foreign key integrity, and timeliness (max order_date within last 2 hours).",
            "<strong>Step 2:</strong> Write a dbt model and schema.yml with at least 3 tests (not_null, unique, expression_is_true). Run dbt test.",
            "<strong>Step 3:</strong> Make a non-idempotent INSERT idempotent using all 3 patterns: DELETE+INSERT, ON CONFLICT, and Spark overwrite partition. Verify by running twice.",
            "<strong>Step 4:</strong> Write a pytest unit test for a Spark transformation that computes daily revenue. Include edge cases: empty input, all-NULL revenue, negative amounts."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Install dbt-core, run first model + Full Mock Interview Loop #2",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Install dbt-core, run first model + Full Mock Interview Loop #2 \u2014 trace through 2-3 edge cases before writing any code.",
                "dbt test types: not_null, unique (built-in), accepted_values, relationships (built-in), custom SQL assertions.",
                "Non-idempotent pipelines are the #1 cause of duplicate data bugs at scale. Always design for idempotency.",
                "Great Expectations: Python library for defining and running data validation rules (similar to dbt tests but in Python).",
                "Anomaly detection alert: if today's row count < 80% or > 120% of last 7-day average \u2192 alert. Simple and powerful."
            ]
        },
        "HardProblem": "Boss Problem (Airbnb): You are the data quality lead. Your nightly ETL ingests from 20 source databases across 5 countries. (1) Design a quality check framework: what checks run at each stage, who is alerted when they fail, and how do you prevent bad data from reaching BI reports? (2) A source system in Germany changes the 'price' column from EUR decimal to STRING '\u20acX.XX' without warning. How does your system detect this? What happens to the pipeline? How do you fix it? (3) A data engineer accidentally writes non-idempotent code that ran 3 times \u2014 you now have 3x the rows for 2024-01-15 only. How do you fix production data?"
    },
    {
        "Week": 9,
        "Day": "Sunday",
        "Phase": "2. Platform (Internals)",
        "Theme": "Rest",
        "SpecificTopic": "Rest & Review",
        "ActionItem_Deliverable": "Re-read notes from Weeks 5-7",
        "LeetCodeProblem": null,
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Indexing Strategies</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Active Rest \u2014 How Memory Consolidates</div>\n<div class=\"rich\">\n<h4>The Neuroscience of Spaced Repetition</h4>\n<p>Rest days are not wasted days. During rest, your brain replays what it learned \u2014 a process called <strong>memory consolidation</strong>. Research consistently shows that <em>attempting to recall from memory</em> is 3\u00d7 more effective at building durable knowledge than re-reading notes. The act of struggling to remember \u2014 even failing \u2014 strengthens the neural pathway more than passive review.</p>\n<p>Think of it like muscle recovery: a weightlifter who trains 7 days straight without rest gets weaker, not stronger. The growth happens during recovery. Your brain is the same.</p>\n<h4>Today's Protocol (4 hours, structured)</h4>\n<ol>\n  <li><strong>Active Recall (60 min):</strong> Close everything. On a blank page, write every concept, formula, and pattern from this week from memory. Then check.</li>\n  <li><strong>Error Review (60 min):</strong> Find the hardest problem you struggled with. Attempt it again completely from scratch \u2014 no notes.</li>\n  <li><strong>Teach It (45 min):</strong> Open the AI chatbot and explain Gaps & Islands to it as if it's a junior engineer. If you stumble, that's your gap.</li>\n  <li><strong>Next Week Preview (15 min):</strong> Skim next Monday's topic at headline level only \u2014 priming improves retention when you study it deeply.</li>\n</ol>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Active Recall Quiz \u2014 No Notes Allowed</div>\n<div class=\"rich\">\n<h4>Answer These From Memory. Then Check.</h4>\n<pre>Window Functions:\n\u25a1 What is the difference between ROW_NUMBER, RANK, and DENSE_RANK?\n  (Write the output for a table with two tied rows)\n\u25a1 Write the syntax for a 7-day rolling average from memory\n\u25a1 What does PARTITION BY do differently from GROUP BY?\n\u25a1 Write the complete Gaps & Islands CTE from memory\n\u25a1 What is the difference between ROWS BETWEEN and RANGE BETWEEN?\n\u25a1 LAG(col, 1, 0) vs LAG(col, 1) \u2014 what is different?\n\u25a1 In what SQL execution order step do window functions run?\n\u25a1 Why can't you write WHERE rank &lt;= 3 in the same query as ROW_NUMBER?\n\nScore yourself:\n8/8 = \ud83c\udfc6 Mastery level \u2014 you're ready for interviews on this\n5-7/8 = \ud83d\udfe1 Good \u2014 review the 1-2 concepts you missed\n&lt; 5/8 = \ud83d\udd34 Go back to the specific day's content, re-read Level 1-2</pre>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Spaced Repetition Schedule</div>\n<div class=\"rich\">\n<h4>When to Review This Week's Material</h4>\n<table>\n<tr><th>Review this content</th><th>When</th><th>How</th></tr>\n<tr><td>This week</td><td>Today</td><td>Active recall quiz above</td></tr>\n<tr><td>This week</td><td>In 3 days</td><td>Re-solve the hard problem from Day 4</td></tr>\n<tr><td>This week</td><td>In 7 days</td><td>LeetCode: solve problems 185, 180, 196</td></tr>\n<tr><td>This week</td><td>In 30 days</td><td>Explain all 4 concepts to someone else</td></tr>\n</table>\n<h4>The Feynman Technique</h4>\n<p>After studying any concept, close your notes and explain it as if you're teaching a newcomer. Use only simple words. Avoid jargon. If you stumble, that's exactly where your understanding breaks down \u2014 return to just that part. This technique built Richard Feynman's legendary ability to explain complex physics simply.</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Interview Readiness Self-Assessment</div>\n<div class=\"rich\">\n<pre>Rate yourself 1\u20135 on each topic (honest self-assessment):\n  1 = \"I need to re-read from scratch\"\n  3 = \"I understand but need to look at notes\"\n  5 = \"I can code this under pressure from memory\"\n\nROW_NUMBER / RANK / DENSE_RANK:        ___/5\nRolling windows (ROWS BETWEEN):        ___/5\nLAG/LEAD (adjacent row comparison):    ___/5\nGaps & Islands (date streaks):         ___/5\nSession detection (timestamp version): ___/5\n\nAction:\n  Score &lt;3: revisit that day's Level 1 and Level 2 content tomorrow\n  Score 3:  do one LeetCode problem on that specific topic\n  Score 5:  you're interview-ready \u2705 on this topic</pre>\n<h4>FAANG Interview Reality Check</h4>\n<p>At meta, Amazon, and Google data engineering interviews, you'll be asked to write SQL live on a shared screen without autocomplete or documentation. The most common reason candidates fail: they know the concept but freeze on the syntax under pressure. The fix: write every pattern from memory at least 5 times this week. Muscle memory is more reliable than conscious recall under stress.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Memory consolidation happens during rest \u2014 the brain replays and stabilizes learned patterns during downtime.",
            "Active recall (testing yourself) builds 3x stronger memory than re-reading notes.",
            "Spaced repetition: review at day 1, day 3, day 7, day 30 for maximum retention.",
            "The Feynman Technique: if you can't explain it simply, you don't fully understand it yet.",
            "Error analysis: the problems you got WRONG are your highest-value review items \u2014 spend the most time there."
        ],
        "Tasks": [
            "<strong>Active Recall:</strong> Write the Gaps & Islands CTE from memory on paper. Check it against Day 4.",
            "<strong>Re-attempt:</strong> Pick the hardest problem from this week and solve it in 20 minutes from scratch.",
            "<strong>Teach it:</strong> Explain RANK vs DENSE_RANK to the AI chatbot as if it's a junior. Get it right without notes.",
            "<strong>Preview:</strong> Skim next week's Monday topic for 10 minutes \u2014 just headline concepts."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Re-read notes from Weeks 5-7",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Re-read notes from Weeks 5-7 \u2014 trace through 2-3 edge cases before writing any code.",
                "Use the AI chatbot to quiz yourself \u2014 ask it 'Give me a SQL window function interview question.'",
                "Write SQL from memory in a plain text editor \u2014 no autocomplete. This builds true recall.",
                "The hardest concept this week: spend 30 minutes writing a full explanation in your own words."
            ]
        },
        "HardProblem": "Connect-the-dots: Write 2-3 paragraphs explaining how ROW_NUMBER, rolling averages, LAG, and Gaps & Islands could ALL be used in a single production pipeline for a ride-sharing company. What problem does each solve? How do they complement each other? What would break if you removed any one of them?"
    },
    {
        "Week": 10,
        "Day": "Monday",
        "Phase": "3. Architect (System)",
        "Theme": "Orchestration",
        "SpecificTopic": "DAG Architecture",
        "ActionItem_Deliverable": "Explain Scheduler vs Executor vs Webserver",
        "LeetCodeProblem": "<strong>LC 207 \u2013 Course Schedule</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Compression</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 What DAGs Are + Airflow Core Concepts</div><div class=\"rich\"><h4>What is a DAG and Why Orchestration Matters</h4><p>A data pipeline consists of multiple steps: extract from source, validate, transform, load, test, notify. These steps have dependencies \u2014 step 3 cannot start until steps 1 and 2 complete. Some steps can run in parallel (downloading from multiple sources simultaneously). Some steps must run in sequence (load only after validation passes).</p><p>A <strong>Directed Acyclic Graph (DAG)</strong> is the formal way to represent these dependencies: nodes are tasks, directed edges represent \"must complete before.\" Acyclic means there are no circular dependencies (which would cause deadlock). Apache Airflow is the dominant DAG orchestration platform at FAANG companies.</p><pre># Example DAG: daily customer revenue pipeline\n\n     [extract_orders]   [extract_customers]   \u2190 parallel execution\n            \\                 /\n          [validate_data]            \u2190 waits for both\n                  |\n     [compute_revenue]              \u2190 runs after validation\n         /          \\\\\n[load_to_dw]  [send_slack_report]   \u2190 parallel execution\n         \\\\\n       [run_dbt_tests]              \u2190 runs after load</pre><h4>Airflow Core Concepts</h4><table><tr><th>Concept</th><td>What It Does</td></tr><tr><th>DAG</th><td>A Python file defining the workflow graph</td></tr><tr><th>Task</th><td>A single unit of work (one node in the graph)</td></tr><tr><th>Operator</th><td>Type of task: PythonOperator, BashOperator, SparkSubmitOperator, etc.</td></tr><tr><th>Sensor</th><td>Special operator that waits for a condition (file arrives, API responds)</td></tr><tr><th>Schedule</th><td>Cron expression or timedelta defining when DAG runs</td></tr><tr><th>DAG Run</th><td>One execution instance of a DAG for a specific logical date</td></tr><tr><th>Task Instance</th><td>One execution of one task within a DAG Run</td></tr><tr><th>XCom</th><td>Mechanism for tasks to pass small values to downstream tasks</td></tr></table></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Operators, Sensors, Real DAG Code</div><div class=\"rich\"><h4>Operators and Sensors \u2014 The Building Blocks</h4><p>An <strong>Operator</strong> defines what a task does. Airflow provides built-in operators for common actions, and you can write custom operators for anything else.</p><pre>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nfrom airflow.sensors.filesystem import FileSensor\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    \"owner\": \"data-team\",\n    \"retries\": 3,\n    \"retry_delay\": timedelta(minutes=5),\n    \"email_on_failure\": True,\n}\n\nwith DAG(\n    dag_id=\"daily_revenue_pipeline\",\n    schedule_interval=\"0 3 * * *\",    # 3am UTC daily\n    start_date=datetime(2024, 1, 1),\n    catchup=False,                    # don't backfill old runs\n    default_args=default_args,\n) as dag:\n\n    wait_for_source = FileSensor(\n        task_id=\"wait_for_source_file\",\n        filepath=\"/data/source/orders_{{ ds }}.csv\",\n        poke_interval=60,   # check every 60 seconds\n        timeout=3600,       # fail if not found within 1 hour\n    )\n\n    run_spark = SparkSubmitOperator(\n        task_id=\"compute_revenue\",\n        application=\"s3://jobs/revenue_job.py\",\n        conf={\"spark.executor.memory\": \"8g\"},\n    )\n\n    def validate(**context):\n        # access the execution date via context\n        ds = context[\"ds\"]  # \"2024-01-15\"\n        count = check_row_count(ds)\n        if count < 1000:\n            raise ValueError(f\"Only {count} rows \u2014 expected > 1000\")\n\n    validate_task = PythonOperator(\n        task_id=\"validate_output\",\n        python_callable=validate,\n    )\n\n    # Dependency chain\n    wait_for_source >> run_spark >> validate_task</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 XComs + Backfilling Pitfalls</div><div class=\"rich\"><h4>XComs \u2014 Passing Data Between Tasks</h4><p>XCom (cross-communication) allows tasks to pass small values to downstream tasks. <strong>Critical rule:</strong> XComs are stored in the Airflow metadata database. They are for small values only: row counts, file paths, status strings, timestamps. Never XCom a DataFrame or large file \u2014 use S3/GCS paths instead.</p><pre># Task 1: pushes a value via XCom (auto-push with return)\ndef extract(**context):\n    rows = run_extraction()\n    return len(rows)  # auto-pushed to XCom key \"return_value\"\n\n# Task 2: pulls the value from upstream task\ndef validate(**context):\n    row_count = context[\"ti\"].xcom_pull(\n        task_ids=\"extract_task\",  # which task\n        key=\"return_value\"        # which XCom key\n    )\n    if row_count < 1000:\n        raise ValueError(f\"Too few rows: {row_count}\")\n\n# Better pattern for large data: pass S3 path, not the data itself\ndef extract(**context):\n    ds = context[\"ds\"]\n    path = f\"s3://staging/orders/{ds}/data.parquet\"\n    write_data_to_s3(path)\n    return path  # XCom the path, not the data</pre><h4>Backfilling \u2014 Running Historical Dates</h4><p><strong>Backfilling</strong> means running a DAG for historical dates \u2014 for example, you added a new column to your pipeline and need to reprocess the last 90 days of data. Airflow can trigger DAG runs for any historical date via the CLI or UI.</p><pre># Backfill via CLI: run every day from Jan 1 to Jan 31\nairflow dags backfill \\\n  --dag-id daily_revenue_pipeline \\\n  --start-date 2024-01-01 \\\n  --end-date 2024-01-31\n\n# \u26a0\ufe0f  Backfill pitfalls:\n# 1. catchup=True (default) auto-backfills missed runs on restart\n#    \u2192 set catchup=False to prevent surprise backfills\n# 2. Backfills run concurrently (all 31 days at once by default)\n#    \u2192 set max_active_runs=1 to run one day at a time\n# 3. Tasks may not be idempotent \u2192 backfill causes duplicate data\n#    \u2192 always make your pipeline idempotent!</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Dynamic DAGs + Design Best Practices</div><div class=\"rich\"><h4>Dynamic DAGs \u2014 Generating at Scale</h4><p>When you need dozens or hundreds of similar pipelines (one DAG per client, per table, per region), manually writing each DAG is impractical. <strong>Dynamic DAGs</strong> generate the DAG structure programmatically from a config or database.</p><pre># Pattern: generate one DAG per client from a config file\n# clients.yaml: [{name: \"acme\", tables: [\"orders\",\"users\"]}, ...]\n\nimport yaml\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\nwith open(\"/dags/config/clients.yaml\") as f:\n    clients = yaml.safe_load(f)[\"clients\"]\n\nfor client in clients:\n    dag_id = f\"etl_{client['name']}\"\n\n    with DAG(\n        dag_id=dag_id,\n        schedule_interval=\"0 4 * * *\",\n        ...\n    ) as dag:\n        for table in client[\"tables\"]:\n            task = PythonOperator(\n                task_id=f\"process_{table}\",\n                python_callable=process_table,\n                op_args=[client[\"name\"], table],\n            )\n        globals()[dag_id] = dag  # register DAG globally for Airflow scanner\n\n# Result: one DAG per client, all generated from config\n# Adding a new client = add one line to clients.yaml</pre><h4>FAANG Interview: DAG Design Best Practices</h4><ul><li><strong>Idempotent tasks:</strong> running the same task twice for the same date should produce the same result. No duplicate inserts. Use INSERT ... ON CONFLICT or DELETE then INSERT.</li><li><strong>Atomic writes:</strong> write to a staging location, atomically move to final location. Never overwrite production data partially.</li><li><strong>Alerting:</strong> every production DAG should have email_on_failure=True and PagerDuty/Slack integration for SLA misses.</li><li><strong>SLA monitoring:</strong> if a DAG that normally finishes at 4am does not complete by 6am, trigger an alert \u2014 the downstream team needs to know.</li><li><strong>Prefer sensors over long-running tasks:</strong> a FileSensor that checks every 60 seconds uses minimal resources. A sleep loop in a Python task holds an executor slot.</li></ul></div></div></div>",
        "KeyConcepts": [
            "DAG = Directed Acyclic Graph. Nodes are tasks, edges are dependencies. Acyclic = no circular dependencies.",
            "Airflow: Python-defined DAGs. schedule_interval (cron), start_date, catchup, max_active_runs.",
            "Operator types: PythonOperator, BashOperator, SparkSubmitOperator, SqlOperator, S3ToRedshiftOperator.",
            "Sensor: waits for a condition. FileSensor, HttpSensor, S3KeySensor. Uses poke_interval to check periodically.",
            "XCom: small values passed between tasks (paths, counts, flags). Never XCom large data \u2014 use S3 path instead.",
            "catchup=False: prevents Airflow from backfilling all missed runs on restart. Almost always set this.",
            "Idempotent tasks: running twice for the same date = same result. Essential for safe retries and backfills.",
            "Dynamic DAGs: generate multiple DAGs from a config (loop in Python file). Use globals() to register."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write an Airflow DAG with 3 tasks: extract (PythonOperator), transform (SparkSubmitOperator), load (PythonOperator). Set up correct dependencies.",
            "<strong>Step 2:</strong> Add a FileSensor before extract that waits for a source file to appear in S3. Set poke_interval=60 and timeout=3600.",
            "<strong>Step 3:</strong> Make the extract task push a row count to XCom. Have the validate task pull it and raise ValueError if count < 1000.",
            "<strong>Step 4:</strong> Write a dynamic DAG generator that creates one DAG per client from a JSON config file with 5 client entries. Verify all 5 DAGs appear in Airflow UI."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Explain Scheduler vs Executor vs Webserver",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Explain Scheduler vs Executor vs Webserver \u2014 trace through 2-3 edge cases before writing any code.",
                "catchup=True (default) can auto-run hundreds of missed historical DAG runs \u2014 almost always set catchup=False.",
                "XCom best practice: pass S3 paths, not data. The data lives in S3, the path lives in XCom.",
                "Sensor vs sleep in task: sensor releases executor slot while waiting. Sleep holds the slot \u2014 avoid it.",
                "max_active_runs=1: prevents concurrent DAG runs. Important for pipelines that write to the same partition."
            ]
        },
        "HardProblem": "Boss Problem (Spotify): You manage a daily pipeline: 200 Airflow DAGs, each processing data for one country. DAGs run at different times (by time zone) and share downstream dependencies (a global aggregation DAG that waits for ALL 200 country DAGs to finish). (1) Design the dependency structure. (2) One country DAG fails \u2014 should the global aggregation wait or proceed with 199/200 countries? (3) A new country is added \u2014 how do you add it without rewriting code? (4) The global aggregation consistently misses its 6am SLA by 15 minutes \u2014 diagnose and fix."
    },
    {
        "Week": 10,
        "Day": "Tuesday",
        "Phase": "3. Architect (System)",
        "Theme": "Orchestration",
        "SpecificTopic": "Operators & Sensors",
        "ActionItem_Deliverable": "Explain use case for S3Sensor",
        "LeetCodeProblem": "<strong>LC 210 \u2013 Course Schedule II</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Data Formats</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 What DAGs Are + Airflow Core Concepts</div><div class=\"rich\"><h4>What is a DAG and Why Orchestration Matters</h4><p>A data pipeline consists of multiple steps: extract from source, validate, transform, load, test, notify. These steps have dependencies \u2014 step 3 cannot start until steps 1 and 2 complete. Some steps can run in parallel (downloading from multiple sources simultaneously). Some steps must run in sequence (load only after validation passes).</p><p>A <strong>Directed Acyclic Graph (DAG)</strong> is the formal way to represent these dependencies: nodes are tasks, directed edges represent \"must complete before.\" Acyclic means there are no circular dependencies (which would cause deadlock). Apache Airflow is the dominant DAG orchestration platform at FAANG companies.</p><pre># Example DAG: daily customer revenue pipeline\n\n     [extract_orders]   [extract_customers]   \u2190 parallel execution\n            \\                 /\n          [validate_data]            \u2190 waits for both\n                  |\n     [compute_revenue]              \u2190 runs after validation\n         /          \\\\\n[load_to_dw]  [send_slack_report]   \u2190 parallel execution\n         \\\\\n       [run_dbt_tests]              \u2190 runs after load</pre><h4>Airflow Core Concepts</h4><table><tr><th>Concept</th><td>What It Does</td></tr><tr><th>DAG</th><td>A Python file defining the workflow graph</td></tr><tr><th>Task</th><td>A single unit of work (one node in the graph)</td></tr><tr><th>Operator</th><td>Type of task: PythonOperator, BashOperator, SparkSubmitOperator, etc.</td></tr><tr><th>Sensor</th><td>Special operator that waits for a condition (file arrives, API responds)</td></tr><tr><th>Schedule</th><td>Cron expression or timedelta defining when DAG runs</td></tr><tr><th>DAG Run</th><td>One execution instance of a DAG for a specific logical date</td></tr><tr><th>Task Instance</th><td>One execution of one task within a DAG Run</td></tr><tr><th>XCom</th><td>Mechanism for tasks to pass small values to downstream tasks</td></tr></table></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Operators, Sensors, Real DAG Code</div><div class=\"rich\"><h4>Operators and Sensors \u2014 The Building Blocks</h4><p>An <strong>Operator</strong> defines what a task does. Airflow provides built-in operators for common actions, and you can write custom operators for anything else.</p><pre>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nfrom airflow.sensors.filesystem import FileSensor\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    \"owner\": \"data-team\",\n    \"retries\": 3,\n    \"retry_delay\": timedelta(minutes=5),\n    \"email_on_failure\": True,\n}\n\nwith DAG(\n    dag_id=\"daily_revenue_pipeline\",\n    schedule_interval=\"0 3 * * *\",    # 3am UTC daily\n    start_date=datetime(2024, 1, 1),\n    catchup=False,                    # don't backfill old runs\n    default_args=default_args,\n) as dag:\n\n    wait_for_source = FileSensor(\n        task_id=\"wait_for_source_file\",\n        filepath=\"/data/source/orders_{{ ds }}.csv\",\n        poke_interval=60,   # check every 60 seconds\n        timeout=3600,       # fail if not found within 1 hour\n    )\n\n    run_spark = SparkSubmitOperator(\n        task_id=\"compute_revenue\",\n        application=\"s3://jobs/revenue_job.py\",\n        conf={\"spark.executor.memory\": \"8g\"},\n    )\n\n    def validate(**context):\n        # access the execution date via context\n        ds = context[\"ds\"]  # \"2024-01-15\"\n        count = check_row_count(ds)\n        if count < 1000:\n            raise ValueError(f\"Only {count} rows \u2014 expected > 1000\")\n\n    validate_task = PythonOperator(\n        task_id=\"validate_output\",\n        python_callable=validate,\n    )\n\n    # Dependency chain\n    wait_for_source >> run_spark >> validate_task</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 XComs + Backfilling Pitfalls</div><div class=\"rich\"><h4>XComs \u2014 Passing Data Between Tasks</h4><p>XCom (cross-communication) allows tasks to pass small values to downstream tasks. <strong>Critical rule:</strong> XComs are stored in the Airflow metadata database. They are for small values only: row counts, file paths, status strings, timestamps. Never XCom a DataFrame or large file \u2014 use S3/GCS paths instead.</p><pre># Task 1: pushes a value via XCom (auto-push with return)\ndef extract(**context):\n    rows = run_extraction()\n    return len(rows)  # auto-pushed to XCom key \"return_value\"\n\n# Task 2: pulls the value from upstream task\ndef validate(**context):\n    row_count = context[\"ti\"].xcom_pull(\n        task_ids=\"extract_task\",  # which task\n        key=\"return_value\"        # which XCom key\n    )\n    if row_count < 1000:\n        raise ValueError(f\"Too few rows: {row_count}\")\n\n# Better pattern for large data: pass S3 path, not the data itself\ndef extract(**context):\n    ds = context[\"ds\"]\n    path = f\"s3://staging/orders/{ds}/data.parquet\"\n    write_data_to_s3(path)\n    return path  # XCom the path, not the data</pre><h4>Backfilling \u2014 Running Historical Dates</h4><p><strong>Backfilling</strong> means running a DAG for historical dates \u2014 for example, you added a new column to your pipeline and need to reprocess the last 90 days of data. Airflow can trigger DAG runs for any historical date via the CLI or UI.</p><pre># Backfill via CLI: run every day from Jan 1 to Jan 31\nairflow dags backfill \\\n  --dag-id daily_revenue_pipeline \\\n  --start-date 2024-01-01 \\\n  --end-date 2024-01-31\n\n# \u26a0\ufe0f  Backfill pitfalls:\n# 1. catchup=True (default) auto-backfills missed runs on restart\n#    \u2192 set catchup=False to prevent surprise backfills\n# 2. Backfills run concurrently (all 31 days at once by default)\n#    \u2192 set max_active_runs=1 to run one day at a time\n# 3. Tasks may not be idempotent \u2192 backfill causes duplicate data\n#    \u2192 always make your pipeline idempotent!</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Dynamic DAGs + Design Best Practices</div><div class=\"rich\"><h4>Dynamic DAGs \u2014 Generating at Scale</h4><p>When you need dozens or hundreds of similar pipelines (one DAG per client, per table, per region), manually writing each DAG is impractical. <strong>Dynamic DAGs</strong> generate the DAG structure programmatically from a config or database.</p><pre># Pattern: generate one DAG per client from a config file\n# clients.yaml: [{name: \"acme\", tables: [\"orders\",\"users\"]}, ...]\n\nimport yaml\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\nwith open(\"/dags/config/clients.yaml\") as f:\n    clients = yaml.safe_load(f)[\"clients\"]\n\nfor client in clients:\n    dag_id = f\"etl_{client['name']}\"\n\n    with DAG(\n        dag_id=dag_id,\n        schedule_interval=\"0 4 * * *\",\n        ...\n    ) as dag:\n        for table in client[\"tables\"]:\n            task = PythonOperator(\n                task_id=f\"process_{table}\",\n                python_callable=process_table,\n                op_args=[client[\"name\"], table],\n            )\n        globals()[dag_id] = dag  # register DAG globally for Airflow scanner\n\n# Result: one DAG per client, all generated from config\n# Adding a new client = add one line to clients.yaml</pre><h4>FAANG Interview: DAG Design Best Practices</h4><ul><li><strong>Idempotent tasks:</strong> running the same task twice for the same date should produce the same result. No duplicate inserts. Use INSERT ... ON CONFLICT or DELETE then INSERT.</li><li><strong>Atomic writes:</strong> write to a staging location, atomically move to final location. Never overwrite production data partially.</li><li><strong>Alerting:</strong> every production DAG should have email_on_failure=True and PagerDuty/Slack integration for SLA misses.</li><li><strong>SLA monitoring:</strong> if a DAG that normally finishes at 4am does not complete by 6am, trigger an alert \u2014 the downstream team needs to know.</li><li><strong>Prefer sensors over long-running tasks:</strong> a FileSensor that checks every 60 seconds uses minimal resources. A sleep loop in a Python task holds an executor slot.</li></ul></div></div></div>",
        "KeyConcepts": [
            "DAG = Directed Acyclic Graph. Nodes are tasks, edges are dependencies. Acyclic = no circular dependencies.",
            "Airflow: Python-defined DAGs. schedule_interval (cron), start_date, catchup, max_active_runs.",
            "Operator types: PythonOperator, BashOperator, SparkSubmitOperator, SqlOperator, S3ToRedshiftOperator.",
            "Sensor: waits for a condition. FileSensor, HttpSensor, S3KeySensor. Uses poke_interval to check periodically.",
            "XCom: small values passed between tasks (paths, counts, flags). Never XCom large data \u2014 use S3 path instead.",
            "catchup=False: prevents Airflow from backfilling all missed runs on restart. Almost always set this.",
            "Idempotent tasks: running twice for the same date = same result. Essential for safe retries and backfills.",
            "Dynamic DAGs: generate multiple DAGs from a config (loop in Python file). Use globals() to register."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write an Airflow DAG with 3 tasks: extract (PythonOperator), transform (SparkSubmitOperator), load (PythonOperator). Set up correct dependencies.",
            "<strong>Step 2:</strong> Add a FileSensor before extract that waits for a source file to appear in S3. Set poke_interval=60 and timeout=3600.",
            "<strong>Step 3:</strong> Make the extract task push a row count to XCom. Have the validate task pull it and raise ValueError if count < 1000.",
            "<strong>Step 4:</strong> Write a dynamic DAG generator that creates one DAG per client from a JSON config file with 5 client entries. Verify all 5 DAGs appear in Airflow UI."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Explain use case for S3Sensor",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Explain use case for S3Sensor \u2014 trace through 2-3 edge cases before writing any code.",
                "catchup=True (default) can auto-run hundreds of missed historical DAG runs \u2014 almost always set catchup=False.",
                "XCom best practice: pass S3 paths, not data. The data lives in S3, the path lives in XCom.",
                "Sensor vs sleep in task: sensor releases executor slot while waiting. Sleep holds the slot \u2014 avoid it.",
                "max_active_runs=1: prevents concurrent DAG runs. Important for pipelines that write to the same partition."
            ]
        },
        "HardProblem": "Boss Problem (Spotify): You manage a daily pipeline: 200 Airflow DAGs, each processing data for one country. DAGs run at different times (by time zone) and share downstream dependencies (a global aggregation DAG that waits for ALL 200 country DAGs to finish). (1) Design the dependency structure. (2) One country DAG fails \u2014 should the global aggregation wait or proceed with 199/200 countries? (3) A new country is added \u2014 how do you add it without rewriting code? (4) The global aggregation consistently misses its 6am SLA by 15 minutes \u2014 diagnose and fix."
    },
    {
        "Week": 10,
        "Day": "Wednesday",
        "Phase": "3. Architect (System)",
        "Theme": "Orchestration",
        "SpecificTopic": "XComs",
        "ActionItem_Deliverable": "Explain limits of XComs (Data Size)",
        "LeetCodeProblem": "<strong>LC 269 \u2013 Alien Dictionary</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Gaps & Islands (Logic)</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 What DAGs Are + Airflow Core Concepts</div><div class=\"rich\"><h4>What is a DAG and Why Orchestration Matters</h4><p>A data pipeline consists of multiple steps: extract from source, validate, transform, load, test, notify. These steps have dependencies \u2014 step 3 cannot start until steps 1 and 2 complete. Some steps can run in parallel (downloading from multiple sources simultaneously). Some steps must run in sequence (load only after validation passes).</p><p>A <strong>Directed Acyclic Graph (DAG)</strong> is the formal way to represent these dependencies: nodes are tasks, directed edges represent \"must complete before.\" Acyclic means there are no circular dependencies (which would cause deadlock). Apache Airflow is the dominant DAG orchestration platform at FAANG companies.</p><pre># Example DAG: daily customer revenue pipeline\n\n     [extract_orders]   [extract_customers]   \u2190 parallel execution\n            \\                 /\n          [validate_data]            \u2190 waits for both\n                  |\n     [compute_revenue]              \u2190 runs after validation\n         /          \\\\\n[load_to_dw]  [send_slack_report]   \u2190 parallel execution\n         \\\\\n       [run_dbt_tests]              \u2190 runs after load</pre><h4>Airflow Core Concepts</h4><table><tr><th>Concept</th><td>What It Does</td></tr><tr><th>DAG</th><td>A Python file defining the workflow graph</td></tr><tr><th>Task</th><td>A single unit of work (one node in the graph)</td></tr><tr><th>Operator</th><td>Type of task: PythonOperator, BashOperator, SparkSubmitOperator, etc.</td></tr><tr><th>Sensor</th><td>Special operator that waits for a condition (file arrives, API responds)</td></tr><tr><th>Schedule</th><td>Cron expression or timedelta defining when DAG runs</td></tr><tr><th>DAG Run</th><td>One execution instance of a DAG for a specific logical date</td></tr><tr><th>Task Instance</th><td>One execution of one task within a DAG Run</td></tr><tr><th>XCom</th><td>Mechanism for tasks to pass small values to downstream tasks</td></tr></table></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Operators, Sensors, Real DAG Code</div><div class=\"rich\"><h4>Operators and Sensors \u2014 The Building Blocks</h4><p>An <strong>Operator</strong> defines what a task does. Airflow provides built-in operators for common actions, and you can write custom operators for anything else.</p><pre>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nfrom airflow.sensors.filesystem import FileSensor\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    \"owner\": \"data-team\",\n    \"retries\": 3,\n    \"retry_delay\": timedelta(minutes=5),\n    \"email_on_failure\": True,\n}\n\nwith DAG(\n    dag_id=\"daily_revenue_pipeline\",\n    schedule_interval=\"0 3 * * *\",    # 3am UTC daily\n    start_date=datetime(2024, 1, 1),\n    catchup=False,                    # don't backfill old runs\n    default_args=default_args,\n) as dag:\n\n    wait_for_source = FileSensor(\n        task_id=\"wait_for_source_file\",\n        filepath=\"/data/source/orders_{{ ds }}.csv\",\n        poke_interval=60,   # check every 60 seconds\n        timeout=3600,       # fail if not found within 1 hour\n    )\n\n    run_spark = SparkSubmitOperator(\n        task_id=\"compute_revenue\",\n        application=\"s3://jobs/revenue_job.py\",\n        conf={\"spark.executor.memory\": \"8g\"},\n    )\n\n    def validate(**context):\n        # access the execution date via context\n        ds = context[\"ds\"]  # \"2024-01-15\"\n        count = check_row_count(ds)\n        if count < 1000:\n            raise ValueError(f\"Only {count} rows \u2014 expected > 1000\")\n\n    validate_task = PythonOperator(\n        task_id=\"validate_output\",\n        python_callable=validate,\n    )\n\n    # Dependency chain\n    wait_for_source >> run_spark >> validate_task</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 XComs + Backfilling Pitfalls</div><div class=\"rich\"><h4>XComs \u2014 Passing Data Between Tasks</h4><p>XCom (cross-communication) allows tasks to pass small values to downstream tasks. <strong>Critical rule:</strong> XComs are stored in the Airflow metadata database. They are for small values only: row counts, file paths, status strings, timestamps. Never XCom a DataFrame or large file \u2014 use S3/GCS paths instead.</p><pre># Task 1: pushes a value via XCom (auto-push with return)\ndef extract(**context):\n    rows = run_extraction()\n    return len(rows)  # auto-pushed to XCom key \"return_value\"\n\n# Task 2: pulls the value from upstream task\ndef validate(**context):\n    row_count = context[\"ti\"].xcom_pull(\n        task_ids=\"extract_task\",  # which task\n        key=\"return_value\"        # which XCom key\n    )\n    if row_count < 1000:\n        raise ValueError(f\"Too few rows: {row_count}\")\n\n# Better pattern for large data: pass S3 path, not the data itself\ndef extract(**context):\n    ds = context[\"ds\"]\n    path = f\"s3://staging/orders/{ds}/data.parquet\"\n    write_data_to_s3(path)\n    return path  # XCom the path, not the data</pre><h4>Backfilling \u2014 Running Historical Dates</h4><p><strong>Backfilling</strong> means running a DAG for historical dates \u2014 for example, you added a new column to your pipeline and need to reprocess the last 90 days of data. Airflow can trigger DAG runs for any historical date via the CLI or UI.</p><pre># Backfill via CLI: run every day from Jan 1 to Jan 31\nairflow dags backfill \\\n  --dag-id daily_revenue_pipeline \\\n  --start-date 2024-01-01 \\\n  --end-date 2024-01-31\n\n# \u26a0\ufe0f  Backfill pitfalls:\n# 1. catchup=True (default) auto-backfills missed runs on restart\n#    \u2192 set catchup=False to prevent surprise backfills\n# 2. Backfills run concurrently (all 31 days at once by default)\n#    \u2192 set max_active_runs=1 to run one day at a time\n# 3. Tasks may not be idempotent \u2192 backfill causes duplicate data\n#    \u2192 always make your pipeline idempotent!</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Dynamic DAGs + Design Best Practices</div><div class=\"rich\"><h4>Dynamic DAGs \u2014 Generating at Scale</h4><p>When you need dozens or hundreds of similar pipelines (one DAG per client, per table, per region), manually writing each DAG is impractical. <strong>Dynamic DAGs</strong> generate the DAG structure programmatically from a config or database.</p><pre># Pattern: generate one DAG per client from a config file\n# clients.yaml: [{name: \"acme\", tables: [\"orders\",\"users\"]}, ...]\n\nimport yaml\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\nwith open(\"/dags/config/clients.yaml\") as f:\n    clients = yaml.safe_load(f)[\"clients\"]\n\nfor client in clients:\n    dag_id = f\"etl_{client['name']}\"\n\n    with DAG(\n        dag_id=dag_id,\n        schedule_interval=\"0 4 * * *\",\n        ...\n    ) as dag:\n        for table in client[\"tables\"]:\n            task = PythonOperator(\n                task_id=f\"process_{table}\",\n                python_callable=process_table,\n                op_args=[client[\"name\"], table],\n            )\n        globals()[dag_id] = dag  # register DAG globally for Airflow scanner\n\n# Result: one DAG per client, all generated from config\n# Adding a new client = add one line to clients.yaml</pre><h4>FAANG Interview: DAG Design Best Practices</h4><ul><li><strong>Idempotent tasks:</strong> running the same task twice for the same date should produce the same result. No duplicate inserts. Use INSERT ... ON CONFLICT or DELETE then INSERT.</li><li><strong>Atomic writes:</strong> write to a staging location, atomically move to final location. Never overwrite production data partially.</li><li><strong>Alerting:</strong> every production DAG should have email_on_failure=True and PagerDuty/Slack integration for SLA misses.</li><li><strong>SLA monitoring:</strong> if a DAG that normally finishes at 4am does not complete by 6am, trigger an alert \u2014 the downstream team needs to know.</li><li><strong>Prefer sensors over long-running tasks:</strong> a FileSensor that checks every 60 seconds uses minimal resources. A sleep loop in a Python task holds an executor slot.</li></ul></div></div></div>",
        "KeyConcepts": [
            "DAG = Directed Acyclic Graph. Nodes are tasks, edges are dependencies. Acyclic = no circular dependencies.",
            "Airflow: Python-defined DAGs. schedule_interval (cron), start_date, catchup, max_active_runs.",
            "Operator types: PythonOperator, BashOperator, SparkSubmitOperator, SqlOperator, S3ToRedshiftOperator.",
            "Sensor: waits for a condition. FileSensor, HttpSensor, S3KeySensor. Uses poke_interval to check periodically.",
            "XCom: small values passed between tasks (paths, counts, flags). Never XCom large data \u2014 use S3 path instead.",
            "catchup=False: prevents Airflow from backfilling all missed runs on restart. Almost always set this.",
            "Idempotent tasks: running twice for the same date = same result. Essential for safe retries and backfills.",
            "Dynamic DAGs: generate multiple DAGs from a config (loop in Python file). Use globals() to register."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write an Airflow DAG with 3 tasks: extract (PythonOperator), transform (SparkSubmitOperator), load (PythonOperator). Set up correct dependencies.",
            "<strong>Step 2:</strong> Add a FileSensor before extract that waits for a source file to appear in S3. Set poke_interval=60 and timeout=3600.",
            "<strong>Step 3:</strong> Make the extract task push a row count to XCom. Have the validate task pull it and raise ValueError if count < 1000.",
            "<strong>Step 4:</strong> Write a dynamic DAG generator that creates one DAG per client from a JSON config file with 5 client entries. Verify all 5 DAGs appear in Airflow UI."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Explain limits of XComs (Data Size)",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Explain limits of XComs (Data Size) \u2014 trace through 2-3 edge cases before writing any code.",
                "catchup=True (default) can auto-run hundreds of missed historical DAG runs \u2014 almost always set catchup=False.",
                "XCom best practice: pass S3 paths, not data. The data lives in S3, the path lives in XCom.",
                "Sensor vs sleep in task: sensor releases executor slot while waiting. Sleep holds the slot \u2014 avoid it.",
                "max_active_runs=1: prevents concurrent DAG runs. Important for pipelines that write to the same partition."
            ]
        },
        "HardProblem": "Boss Problem (Spotify): You manage a daily pipeline: 200 Airflow DAGs, each processing data for one country. DAGs run at different times (by time zone) and share downstream dependencies (a global aggregation DAG that waits for ALL 200 country DAGs to finish). (1) Design the dependency structure. (2) One country DAG fails \u2014 should the global aggregation wait or proceed with 199/200 countries? (3) A new country is added \u2014 how do you add it without rewriting code? (4) The global aggregation consistently misses its 6am SLA by 15 minutes \u2014 diagnose and fix."
    },
    {
        "Week": 10,
        "Day": "Thursday",
        "Phase": "3. Architect (System)",
        "Theme": "Orchestration",
        "SpecificTopic": "Backfilling",
        "ActionItem_Deliverable": "Explain execution_date vs current_date",
        "LeetCodeProblem": "<strong>LC 310 \u2013 Minimum Height Trees</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Operators & Sensors</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 What DAGs Are + Airflow Core Concepts</div><div class=\"rich\"><h4>What is a DAG and Why Orchestration Matters</h4><p>A data pipeline consists of multiple steps: extract from source, validate, transform, load, test, notify. These steps have dependencies \u2014 step 3 cannot start until steps 1 and 2 complete. Some steps can run in parallel (downloading from multiple sources simultaneously). Some steps must run in sequence (load only after validation passes).</p><p>A <strong>Directed Acyclic Graph (DAG)</strong> is the formal way to represent these dependencies: nodes are tasks, directed edges represent \"must complete before.\" Acyclic means there are no circular dependencies (which would cause deadlock). Apache Airflow is the dominant DAG orchestration platform at FAANG companies.</p><pre># Example DAG: daily customer revenue pipeline\n\n     [extract_orders]   [extract_customers]   \u2190 parallel execution\n            \\                 /\n          [validate_data]            \u2190 waits for both\n                  |\n     [compute_revenue]              \u2190 runs after validation\n         /          \\\\\n[load_to_dw]  [send_slack_report]   \u2190 parallel execution\n         \\\\\n       [run_dbt_tests]              \u2190 runs after load</pre><h4>Airflow Core Concepts</h4><table><tr><th>Concept</th><td>What It Does</td></tr><tr><th>DAG</th><td>A Python file defining the workflow graph</td></tr><tr><th>Task</th><td>A single unit of work (one node in the graph)</td></tr><tr><th>Operator</th><td>Type of task: PythonOperator, BashOperator, SparkSubmitOperator, etc.</td></tr><tr><th>Sensor</th><td>Special operator that waits for a condition (file arrives, API responds)</td></tr><tr><th>Schedule</th><td>Cron expression or timedelta defining when DAG runs</td></tr><tr><th>DAG Run</th><td>One execution instance of a DAG for a specific logical date</td></tr><tr><th>Task Instance</th><td>One execution of one task within a DAG Run</td></tr><tr><th>XCom</th><td>Mechanism for tasks to pass small values to downstream tasks</td></tr></table></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Operators, Sensors, Real DAG Code</div><div class=\"rich\"><h4>Operators and Sensors \u2014 The Building Blocks</h4><p>An <strong>Operator</strong> defines what a task does. Airflow provides built-in operators for common actions, and you can write custom operators for anything else.</p><pre>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nfrom airflow.sensors.filesystem import FileSensor\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    \"owner\": \"data-team\",\n    \"retries\": 3,\n    \"retry_delay\": timedelta(minutes=5),\n    \"email_on_failure\": True,\n}\n\nwith DAG(\n    dag_id=\"daily_revenue_pipeline\",\n    schedule_interval=\"0 3 * * *\",    # 3am UTC daily\n    start_date=datetime(2024, 1, 1),\n    catchup=False,                    # don't backfill old runs\n    default_args=default_args,\n) as dag:\n\n    wait_for_source = FileSensor(\n        task_id=\"wait_for_source_file\",\n        filepath=\"/data/source/orders_{{ ds }}.csv\",\n        poke_interval=60,   # check every 60 seconds\n        timeout=3600,       # fail if not found within 1 hour\n    )\n\n    run_spark = SparkSubmitOperator(\n        task_id=\"compute_revenue\",\n        application=\"s3://jobs/revenue_job.py\",\n        conf={\"spark.executor.memory\": \"8g\"},\n    )\n\n    def validate(**context):\n        # access the execution date via context\n        ds = context[\"ds\"]  # \"2024-01-15\"\n        count = check_row_count(ds)\n        if count < 1000:\n            raise ValueError(f\"Only {count} rows \u2014 expected > 1000\")\n\n    validate_task = PythonOperator(\n        task_id=\"validate_output\",\n        python_callable=validate,\n    )\n\n    # Dependency chain\n    wait_for_source >> run_spark >> validate_task</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 XComs + Backfilling Pitfalls</div><div class=\"rich\"><h4>XComs \u2014 Passing Data Between Tasks</h4><p>XCom (cross-communication) allows tasks to pass small values to downstream tasks. <strong>Critical rule:</strong> XComs are stored in the Airflow metadata database. They are for small values only: row counts, file paths, status strings, timestamps. Never XCom a DataFrame or large file \u2014 use S3/GCS paths instead.</p><pre># Task 1: pushes a value via XCom (auto-push with return)\ndef extract(**context):\n    rows = run_extraction()\n    return len(rows)  # auto-pushed to XCom key \"return_value\"\n\n# Task 2: pulls the value from upstream task\ndef validate(**context):\n    row_count = context[\"ti\"].xcom_pull(\n        task_ids=\"extract_task\",  # which task\n        key=\"return_value\"        # which XCom key\n    )\n    if row_count < 1000:\n        raise ValueError(f\"Too few rows: {row_count}\")\n\n# Better pattern for large data: pass S3 path, not the data itself\ndef extract(**context):\n    ds = context[\"ds\"]\n    path = f\"s3://staging/orders/{ds}/data.parquet\"\n    write_data_to_s3(path)\n    return path  # XCom the path, not the data</pre><h4>Backfilling \u2014 Running Historical Dates</h4><p><strong>Backfilling</strong> means running a DAG for historical dates \u2014 for example, you added a new column to your pipeline and need to reprocess the last 90 days of data. Airflow can trigger DAG runs for any historical date via the CLI or UI.</p><pre># Backfill via CLI: run every day from Jan 1 to Jan 31\nairflow dags backfill \\\n  --dag-id daily_revenue_pipeline \\\n  --start-date 2024-01-01 \\\n  --end-date 2024-01-31\n\n# \u26a0\ufe0f  Backfill pitfalls:\n# 1. catchup=True (default) auto-backfills missed runs on restart\n#    \u2192 set catchup=False to prevent surprise backfills\n# 2. Backfills run concurrently (all 31 days at once by default)\n#    \u2192 set max_active_runs=1 to run one day at a time\n# 3. Tasks may not be idempotent \u2192 backfill causes duplicate data\n#    \u2192 always make your pipeline idempotent!</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Dynamic DAGs + Design Best Practices</div><div class=\"rich\"><h4>Dynamic DAGs \u2014 Generating at Scale</h4><p>When you need dozens or hundreds of similar pipelines (one DAG per client, per table, per region), manually writing each DAG is impractical. <strong>Dynamic DAGs</strong> generate the DAG structure programmatically from a config or database.</p><pre># Pattern: generate one DAG per client from a config file\n# clients.yaml: [{name: \"acme\", tables: [\"orders\",\"users\"]}, ...]\n\nimport yaml\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\nwith open(\"/dags/config/clients.yaml\") as f:\n    clients = yaml.safe_load(f)[\"clients\"]\n\nfor client in clients:\n    dag_id = f\"etl_{client['name']}\"\n\n    with DAG(\n        dag_id=dag_id,\n        schedule_interval=\"0 4 * * *\",\n        ...\n    ) as dag:\n        for table in client[\"tables\"]:\n            task = PythonOperator(\n                task_id=f\"process_{table}\",\n                python_callable=process_table,\n                op_args=[client[\"name\"], table],\n            )\n        globals()[dag_id] = dag  # register DAG globally for Airflow scanner\n\n# Result: one DAG per client, all generated from config\n# Adding a new client = add one line to clients.yaml</pre><h4>FAANG Interview: DAG Design Best Practices</h4><ul><li><strong>Idempotent tasks:</strong> running the same task twice for the same date should produce the same result. No duplicate inserts. Use INSERT ... ON CONFLICT or DELETE then INSERT.</li><li><strong>Atomic writes:</strong> write to a staging location, atomically move to final location. Never overwrite production data partially.</li><li><strong>Alerting:</strong> every production DAG should have email_on_failure=True and PagerDuty/Slack integration for SLA misses.</li><li><strong>SLA monitoring:</strong> if a DAG that normally finishes at 4am does not complete by 6am, trigger an alert \u2014 the downstream team needs to know.</li><li><strong>Prefer sensors over long-running tasks:</strong> a FileSensor that checks every 60 seconds uses minimal resources. A sleep loop in a Python task holds an executor slot.</li></ul></div></div></div>",
        "KeyConcepts": [
            "DAG = Directed Acyclic Graph. Nodes are tasks, edges are dependencies. Acyclic = no circular dependencies.",
            "Airflow: Python-defined DAGs. schedule_interval (cron), start_date, catchup, max_active_runs.",
            "Operator types: PythonOperator, BashOperator, SparkSubmitOperator, SqlOperator, S3ToRedshiftOperator.",
            "Sensor: waits for a condition. FileSensor, HttpSensor, S3KeySensor. Uses poke_interval to check periodically.",
            "XCom: small values passed between tasks (paths, counts, flags). Never XCom large data \u2014 use S3 path instead.",
            "catchup=False: prevents Airflow from backfilling all missed runs on restart. Almost always set this.",
            "Idempotent tasks: running twice for the same date = same result. Essential for safe retries and backfills.",
            "Dynamic DAGs: generate multiple DAGs from a config (loop in Python file). Use globals() to register."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write an Airflow DAG with 3 tasks: extract (PythonOperator), transform (SparkSubmitOperator), load (PythonOperator). Set up correct dependencies.",
            "<strong>Step 2:</strong> Add a FileSensor before extract that waits for a source file to appear in S3. Set poke_interval=60 and timeout=3600.",
            "<strong>Step 3:</strong> Make the extract task push a row count to XCom. Have the validate task pull it and raise ValueError if count < 1000.",
            "<strong>Step 4:</strong> Write a dynamic DAG generator that creates one DAG per client from a JSON config file with 5 client entries. Verify all 5 DAGs appear in Airflow UI."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Explain execution_date vs current_date",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Explain execution_date vs current_date \u2014 trace through 2-3 edge cases before writing any code.",
                "catchup=True (default) can auto-run hundreds of missed historical DAG runs \u2014 almost always set catchup=False.",
                "XCom best practice: pass S3 paths, not data. The data lives in S3, the path lives in XCom.",
                "Sensor vs sleep in task: sensor releases executor slot while waiting. Sleep holds the slot \u2014 avoid it.",
                "max_active_runs=1: prevents concurrent DAG runs. Important for pipelines that write to the same partition."
            ]
        },
        "HardProblem": "Boss Problem (Spotify): You manage a daily pipeline: 200 Airflow DAGs, each processing data for one country. DAGs run at different times (by time zone) and share downstream dependencies (a global aggregation DAG that waits for ALL 200 country DAGs to finish). (1) Design the dependency structure. (2) One country DAG fails \u2014 should the global aggregation wait or proceed with 199/200 countries? (3) A new country is added \u2014 how do you add it without rewriting code? (4) The global aggregation consistently misses its 6am SLA by 15 minutes \u2014 diagnose and fix."
    },
    {
        "Week": 10,
        "Day": "Friday",
        "Phase": "3. Architect (System)",
        "Theme": "Orchestration",
        "SpecificTopic": "Dynamic DAGs",
        "ActionItem_Deliverable": "Generate DAG structure from a Config file",
        "LeetCodeProblem": "<strong>LC 329 \u2013 Longest Increasing Path Matrix</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Gaps & Islands (Logic)</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 What DAGs Are + Airflow Core Concepts</div><div class=\"rich\"><h4>What is a DAG and Why Orchestration Matters</h4><p>A data pipeline consists of multiple steps: extract from source, validate, transform, load, test, notify. These steps have dependencies \u2014 step 3 cannot start until steps 1 and 2 complete. Some steps can run in parallel (downloading from multiple sources simultaneously). Some steps must run in sequence (load only after validation passes).</p><p>A <strong>Directed Acyclic Graph (DAG)</strong> is the formal way to represent these dependencies: nodes are tasks, directed edges represent \"must complete before.\" Acyclic means there are no circular dependencies (which would cause deadlock). Apache Airflow is the dominant DAG orchestration platform at FAANG companies.</p><pre># Example DAG: daily customer revenue pipeline\n\n     [extract_orders]   [extract_customers]   \u2190 parallel execution\n            \\                 /\n          [validate_data]            \u2190 waits for both\n                  |\n     [compute_revenue]              \u2190 runs after validation\n         /          \\\\\n[load_to_dw]  [send_slack_report]   \u2190 parallel execution\n         \\\\\n       [run_dbt_tests]              \u2190 runs after load</pre><h4>Airflow Core Concepts</h4><table><tr><th>Concept</th><td>What It Does</td></tr><tr><th>DAG</th><td>A Python file defining the workflow graph</td></tr><tr><th>Task</th><td>A single unit of work (one node in the graph)</td></tr><tr><th>Operator</th><td>Type of task: PythonOperator, BashOperator, SparkSubmitOperator, etc.</td></tr><tr><th>Sensor</th><td>Special operator that waits for a condition (file arrives, API responds)</td></tr><tr><th>Schedule</th><td>Cron expression or timedelta defining when DAG runs</td></tr><tr><th>DAG Run</th><td>One execution instance of a DAG for a specific logical date</td></tr><tr><th>Task Instance</th><td>One execution of one task within a DAG Run</td></tr><tr><th>XCom</th><td>Mechanism for tasks to pass small values to downstream tasks</td></tr></table></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Operators, Sensors, Real DAG Code</div><div class=\"rich\"><h4>Operators and Sensors \u2014 The Building Blocks</h4><p>An <strong>Operator</strong> defines what a task does. Airflow provides built-in operators for common actions, and you can write custom operators for anything else.</p><pre>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nfrom airflow.sensors.filesystem import FileSensor\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    \"owner\": \"data-team\",\n    \"retries\": 3,\n    \"retry_delay\": timedelta(minutes=5),\n    \"email_on_failure\": True,\n}\n\nwith DAG(\n    dag_id=\"daily_revenue_pipeline\",\n    schedule_interval=\"0 3 * * *\",    # 3am UTC daily\n    start_date=datetime(2024, 1, 1),\n    catchup=False,                    # don't backfill old runs\n    default_args=default_args,\n) as dag:\n\n    wait_for_source = FileSensor(\n        task_id=\"wait_for_source_file\",\n        filepath=\"/data/source/orders_{{ ds }}.csv\",\n        poke_interval=60,   # check every 60 seconds\n        timeout=3600,       # fail if not found within 1 hour\n    )\n\n    run_spark = SparkSubmitOperator(\n        task_id=\"compute_revenue\",\n        application=\"s3://jobs/revenue_job.py\",\n        conf={\"spark.executor.memory\": \"8g\"},\n    )\n\n    def validate(**context):\n        # access the execution date via context\n        ds = context[\"ds\"]  # \"2024-01-15\"\n        count = check_row_count(ds)\n        if count < 1000:\n            raise ValueError(f\"Only {count} rows \u2014 expected > 1000\")\n\n    validate_task = PythonOperator(\n        task_id=\"validate_output\",\n        python_callable=validate,\n    )\n\n    # Dependency chain\n    wait_for_source >> run_spark >> validate_task</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 XComs + Backfilling Pitfalls</div><div class=\"rich\"><h4>XComs \u2014 Passing Data Between Tasks</h4><p>XCom (cross-communication) allows tasks to pass small values to downstream tasks. <strong>Critical rule:</strong> XComs are stored in the Airflow metadata database. They are for small values only: row counts, file paths, status strings, timestamps. Never XCom a DataFrame or large file \u2014 use S3/GCS paths instead.</p><pre># Task 1: pushes a value via XCom (auto-push with return)\ndef extract(**context):\n    rows = run_extraction()\n    return len(rows)  # auto-pushed to XCom key \"return_value\"\n\n# Task 2: pulls the value from upstream task\ndef validate(**context):\n    row_count = context[\"ti\"].xcom_pull(\n        task_ids=\"extract_task\",  # which task\n        key=\"return_value\"        # which XCom key\n    )\n    if row_count < 1000:\n        raise ValueError(f\"Too few rows: {row_count}\")\n\n# Better pattern for large data: pass S3 path, not the data itself\ndef extract(**context):\n    ds = context[\"ds\"]\n    path = f\"s3://staging/orders/{ds}/data.parquet\"\n    write_data_to_s3(path)\n    return path  # XCom the path, not the data</pre><h4>Backfilling \u2014 Running Historical Dates</h4><p><strong>Backfilling</strong> means running a DAG for historical dates \u2014 for example, you added a new column to your pipeline and need to reprocess the last 90 days of data. Airflow can trigger DAG runs for any historical date via the CLI or UI.</p><pre># Backfill via CLI: run every day from Jan 1 to Jan 31\nairflow dags backfill \\\n  --dag-id daily_revenue_pipeline \\\n  --start-date 2024-01-01 \\\n  --end-date 2024-01-31\n\n# \u26a0\ufe0f  Backfill pitfalls:\n# 1. catchup=True (default) auto-backfills missed runs on restart\n#    \u2192 set catchup=False to prevent surprise backfills\n# 2. Backfills run concurrently (all 31 days at once by default)\n#    \u2192 set max_active_runs=1 to run one day at a time\n# 3. Tasks may not be idempotent \u2192 backfill causes duplicate data\n#    \u2192 always make your pipeline idempotent!</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Dynamic DAGs + Design Best Practices</div><div class=\"rich\"><h4>Dynamic DAGs \u2014 Generating at Scale</h4><p>When you need dozens or hundreds of similar pipelines (one DAG per client, per table, per region), manually writing each DAG is impractical. <strong>Dynamic DAGs</strong> generate the DAG structure programmatically from a config or database.</p><pre># Pattern: generate one DAG per client from a config file\n# clients.yaml: [{name: \"acme\", tables: [\"orders\",\"users\"]}, ...]\n\nimport yaml\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\nwith open(\"/dags/config/clients.yaml\") as f:\n    clients = yaml.safe_load(f)[\"clients\"]\n\nfor client in clients:\n    dag_id = f\"etl_{client['name']}\"\n\n    with DAG(\n        dag_id=dag_id,\n        schedule_interval=\"0 4 * * *\",\n        ...\n    ) as dag:\n        for table in client[\"tables\"]:\n            task = PythonOperator(\n                task_id=f\"process_{table}\",\n                python_callable=process_table,\n                op_args=[client[\"name\"], table],\n            )\n        globals()[dag_id] = dag  # register DAG globally for Airflow scanner\n\n# Result: one DAG per client, all generated from config\n# Adding a new client = add one line to clients.yaml</pre><h4>FAANG Interview: DAG Design Best Practices</h4><ul><li><strong>Idempotent tasks:</strong> running the same task twice for the same date should produce the same result. No duplicate inserts. Use INSERT ... ON CONFLICT or DELETE then INSERT.</li><li><strong>Atomic writes:</strong> write to a staging location, atomically move to final location. Never overwrite production data partially.</li><li><strong>Alerting:</strong> every production DAG should have email_on_failure=True and PagerDuty/Slack integration for SLA misses.</li><li><strong>SLA monitoring:</strong> if a DAG that normally finishes at 4am does not complete by 6am, trigger an alert \u2014 the downstream team needs to know.</li><li><strong>Prefer sensors over long-running tasks:</strong> a FileSensor that checks every 60 seconds uses minimal resources. A sleep loop in a Python task holds an executor slot.</li></ul></div></div></div>",
        "KeyConcepts": [
            "DAG = Directed Acyclic Graph. Nodes are tasks, edges are dependencies. Acyclic = no circular dependencies.",
            "Airflow: Python-defined DAGs. schedule_interval (cron), start_date, catchup, max_active_runs.",
            "Operator types: PythonOperator, BashOperator, SparkSubmitOperator, SqlOperator, S3ToRedshiftOperator.",
            "Sensor: waits for a condition. FileSensor, HttpSensor, S3KeySensor. Uses poke_interval to check periodically.",
            "XCom: small values passed between tasks (paths, counts, flags). Never XCom large data \u2014 use S3 path instead.",
            "catchup=False: prevents Airflow from backfilling all missed runs on restart. Almost always set this.",
            "Idempotent tasks: running twice for the same date = same result. Essential for safe retries and backfills.",
            "Dynamic DAGs: generate multiple DAGs from a config (loop in Python file). Use globals() to register."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write an Airflow DAG with 3 tasks: extract (PythonOperator), transform (SparkSubmitOperator), load (PythonOperator). Set up correct dependencies.",
            "<strong>Step 2:</strong> Add a FileSensor before extract that waits for a source file to appear in S3. Set poke_interval=60 and timeout=3600.",
            "<strong>Step 3:</strong> Make the extract task push a row count to XCom. Have the validate task pull it and raise ValueError if count < 1000.",
            "<strong>Step 4:</strong> Write a dynamic DAG generator that creates one DAG per client from a JSON config file with 5 client entries. Verify all 5 DAGs appear in Airflow UI."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Generate DAG structure from a Config file",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Generate DAG structure from a Config file \u2014 trace through 2-3 edge cases before writing any code.",
                "catchup=True (default) can auto-run hundreds of missed historical DAG runs \u2014 almost always set catchup=False.",
                "XCom best practice: pass S3 paths, not data. The data lives in S3, the path lives in XCom.",
                "Sensor vs sleep in task: sensor releases executor slot while waiting. Sleep holds the slot \u2014 avoid it.",
                "max_active_runs=1: prevents concurrent DAG runs. Important for pipelines that write to the same partition."
            ]
        },
        "HardProblem": "Boss Problem (Spotify): You manage a daily pipeline: 200 Airflow DAGs, each processing data for one country. DAGs run at different times (by time zone) and share downstream dependencies (a global aggregation DAG that waits for ALL 200 country DAGs to finish). (1) Design the dependency structure. (2) One country DAG fails \u2014 should the global aggregation wait or proceed with 199/200 countries? (3) A new country is added \u2014 how do you add it without rewriting code? (4) The global aggregation consistently misses its 6am SLA by 15 minutes \u2014 diagnose and fix."
    },
    {
        "Week": 10,
        "Day": "Saturday",
        "Phase": "3. Architect (System)",
        "Theme": "Orchestration + Resume",
        "SpecificTopic": "Lab + Resume Polish",
        "ActionItem_Deliverable": "Install Airflow (Docker), run Hello World + Polish Resume & LinkedIn",
        "LeetCodeProblem": "<strong>LC 332 \u2013 Reconstruct Itinerary</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": "NEW: Resume polish moved to W10",
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>The Shuffle</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 What DAGs Are + Airflow Core Concepts</div><div class=\"rich\"><h4>What is a DAG and Why Orchestration Matters</h4><p>A data pipeline consists of multiple steps: extract from source, validate, transform, load, test, notify. These steps have dependencies \u2014 step 3 cannot start until steps 1 and 2 complete. Some steps can run in parallel (downloading from multiple sources simultaneously). Some steps must run in sequence (load only after validation passes).</p><p>A <strong>Directed Acyclic Graph (DAG)</strong> is the formal way to represent these dependencies: nodes are tasks, directed edges represent \"must complete before.\" Acyclic means there are no circular dependencies (which would cause deadlock). Apache Airflow is the dominant DAG orchestration platform at FAANG companies.</p><pre># Example DAG: daily customer revenue pipeline\n\n     [extract_orders]   [extract_customers]   \u2190 parallel execution\n            \\                 /\n          [validate_data]            \u2190 waits for both\n                  |\n     [compute_revenue]              \u2190 runs after validation\n         /          \\\\\n[load_to_dw]  [send_slack_report]   \u2190 parallel execution\n         \\\\\n       [run_dbt_tests]              \u2190 runs after load</pre><h4>Airflow Core Concepts</h4><table><tr><th>Concept</th><td>What It Does</td></tr><tr><th>DAG</th><td>A Python file defining the workflow graph</td></tr><tr><th>Task</th><td>A single unit of work (one node in the graph)</td></tr><tr><th>Operator</th><td>Type of task: PythonOperator, BashOperator, SparkSubmitOperator, etc.</td></tr><tr><th>Sensor</th><td>Special operator that waits for a condition (file arrives, API responds)</td></tr><tr><th>Schedule</th><td>Cron expression or timedelta defining when DAG runs</td></tr><tr><th>DAG Run</th><td>One execution instance of a DAG for a specific logical date</td></tr><tr><th>Task Instance</th><td>One execution of one task within a DAG Run</td></tr><tr><th>XCom</th><td>Mechanism for tasks to pass small values to downstream tasks</td></tr></table></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Operators, Sensors, Real DAG Code</div><div class=\"rich\"><h4>Operators and Sensors \u2014 The Building Blocks</h4><p>An <strong>Operator</strong> defines what a task does. Airflow provides built-in operators for common actions, and you can write custom operators for anything else.</p><pre>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nfrom airflow.sensors.filesystem import FileSensor\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    \"owner\": \"data-team\",\n    \"retries\": 3,\n    \"retry_delay\": timedelta(minutes=5),\n    \"email_on_failure\": True,\n}\n\nwith DAG(\n    dag_id=\"daily_revenue_pipeline\",\n    schedule_interval=\"0 3 * * *\",    # 3am UTC daily\n    start_date=datetime(2024, 1, 1),\n    catchup=False,                    # don't backfill old runs\n    default_args=default_args,\n) as dag:\n\n    wait_for_source = FileSensor(\n        task_id=\"wait_for_source_file\",\n        filepath=\"/data/source/orders_{{ ds }}.csv\",\n        poke_interval=60,   # check every 60 seconds\n        timeout=3600,       # fail if not found within 1 hour\n    )\n\n    run_spark = SparkSubmitOperator(\n        task_id=\"compute_revenue\",\n        application=\"s3://jobs/revenue_job.py\",\n        conf={\"spark.executor.memory\": \"8g\"},\n    )\n\n    def validate(**context):\n        # access the execution date via context\n        ds = context[\"ds\"]  # \"2024-01-15\"\n        count = check_row_count(ds)\n        if count < 1000:\n            raise ValueError(f\"Only {count} rows \u2014 expected > 1000\")\n\n    validate_task = PythonOperator(\n        task_id=\"validate_output\",\n        python_callable=validate,\n    )\n\n    # Dependency chain\n    wait_for_source >> run_spark >> validate_task</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 XComs + Backfilling Pitfalls</div><div class=\"rich\"><h4>XComs \u2014 Passing Data Between Tasks</h4><p>XCom (cross-communication) allows tasks to pass small values to downstream tasks. <strong>Critical rule:</strong> XComs are stored in the Airflow metadata database. They are for small values only: row counts, file paths, status strings, timestamps. Never XCom a DataFrame or large file \u2014 use S3/GCS paths instead.</p><pre># Task 1: pushes a value via XCom (auto-push with return)\ndef extract(**context):\n    rows = run_extraction()\n    return len(rows)  # auto-pushed to XCom key \"return_value\"\n\n# Task 2: pulls the value from upstream task\ndef validate(**context):\n    row_count = context[\"ti\"].xcom_pull(\n        task_ids=\"extract_task\",  # which task\n        key=\"return_value\"        # which XCom key\n    )\n    if row_count < 1000:\n        raise ValueError(f\"Too few rows: {row_count}\")\n\n# Better pattern for large data: pass S3 path, not the data itself\ndef extract(**context):\n    ds = context[\"ds\"]\n    path = f\"s3://staging/orders/{ds}/data.parquet\"\n    write_data_to_s3(path)\n    return path  # XCom the path, not the data</pre><h4>Backfilling \u2014 Running Historical Dates</h4><p><strong>Backfilling</strong> means running a DAG for historical dates \u2014 for example, you added a new column to your pipeline and need to reprocess the last 90 days of data. Airflow can trigger DAG runs for any historical date via the CLI or UI.</p><pre># Backfill via CLI: run every day from Jan 1 to Jan 31\nairflow dags backfill \\\n  --dag-id daily_revenue_pipeline \\\n  --start-date 2024-01-01 \\\n  --end-date 2024-01-31\n\n# \u26a0\ufe0f  Backfill pitfalls:\n# 1. catchup=True (default) auto-backfills missed runs on restart\n#    \u2192 set catchup=False to prevent surprise backfills\n# 2. Backfills run concurrently (all 31 days at once by default)\n#    \u2192 set max_active_runs=1 to run one day at a time\n# 3. Tasks may not be idempotent \u2192 backfill causes duplicate data\n#    \u2192 always make your pipeline idempotent!</pre></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Dynamic DAGs + Design Best Practices</div><div class=\"rich\"><h4>Dynamic DAGs \u2014 Generating at Scale</h4><p>When you need dozens or hundreds of similar pipelines (one DAG per client, per table, per region), manually writing each DAG is impractical. <strong>Dynamic DAGs</strong> generate the DAG structure programmatically from a config or database.</p><pre># Pattern: generate one DAG per client from a config file\n# clients.yaml: [{name: \"acme\", tables: [\"orders\",\"users\"]}, ...]\n\nimport yaml\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\nwith open(\"/dags/config/clients.yaml\") as f:\n    clients = yaml.safe_load(f)[\"clients\"]\n\nfor client in clients:\n    dag_id = f\"etl_{client['name']}\"\n\n    with DAG(\n        dag_id=dag_id,\n        schedule_interval=\"0 4 * * *\",\n        ...\n    ) as dag:\n        for table in client[\"tables\"]:\n            task = PythonOperator(\n                task_id=f\"process_{table}\",\n                python_callable=process_table,\n                op_args=[client[\"name\"], table],\n            )\n        globals()[dag_id] = dag  # register DAG globally for Airflow scanner\n\n# Result: one DAG per client, all generated from config\n# Adding a new client = add one line to clients.yaml</pre><h4>FAANG Interview: DAG Design Best Practices</h4><ul><li><strong>Idempotent tasks:</strong> running the same task twice for the same date should produce the same result. No duplicate inserts. Use INSERT ... ON CONFLICT or DELETE then INSERT.</li><li><strong>Atomic writes:</strong> write to a staging location, atomically move to final location. Never overwrite production data partially.</li><li><strong>Alerting:</strong> every production DAG should have email_on_failure=True and PagerDuty/Slack integration for SLA misses.</li><li><strong>SLA monitoring:</strong> if a DAG that normally finishes at 4am does not complete by 6am, trigger an alert \u2014 the downstream team needs to know.</li><li><strong>Prefer sensors over long-running tasks:</strong> a FileSensor that checks every 60 seconds uses minimal resources. A sleep loop in a Python task holds an executor slot.</li></ul></div></div></div>",
        "KeyConcepts": [
            "DAG = Directed Acyclic Graph. Nodes are tasks, edges are dependencies. Acyclic = no circular dependencies.",
            "Airflow: Python-defined DAGs. schedule_interval (cron), start_date, catchup, max_active_runs.",
            "Operator types: PythonOperator, BashOperator, SparkSubmitOperator, SqlOperator, S3ToRedshiftOperator.",
            "Sensor: waits for a condition. FileSensor, HttpSensor, S3KeySensor. Uses poke_interval to check periodically.",
            "XCom: small values passed between tasks (paths, counts, flags). Never XCom large data \u2014 use S3 path instead.",
            "catchup=False: prevents Airflow from backfilling all missed runs on restart. Almost always set this.",
            "Idempotent tasks: running twice for the same date = same result. Essential for safe retries and backfills.",
            "Dynamic DAGs: generate multiple DAGs from a config (loop in Python file). Use globals() to register."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Write an Airflow DAG with 3 tasks: extract (PythonOperator), transform (SparkSubmitOperator), load (PythonOperator). Set up correct dependencies.",
            "<strong>Step 2:</strong> Add a FileSensor before extract that waits for a source file to appear in S3. Set poke_interval=60 and timeout=3600.",
            "<strong>Step 3:</strong> Make the extract task push a row count to XCom. Have the validate task pull it and raise ValueError if count < 1000.",
            "<strong>Step 4:</strong> Write a dynamic DAG generator that creates one DAG per client from a JSON config file with 5 client entries. Verify all 5 DAGs appear in Airflow UI."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Install Airflow (Docker), run Hello World + Polish Resume & LinkedIn",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Install Airflow (Docker), run Hello World + Polish Resume & LinkedIn \u2014 trace through 2-3 edge cases before writing any code.",
                "catchup=True (default) can auto-run hundreds of missed historical DAG runs \u2014 almost always set catchup=False.",
                "XCom best practice: pass S3 paths, not data. The data lives in S3, the path lives in XCom.",
                "Sensor vs sleep in task: sensor releases executor slot while waiting. Sleep holds the slot \u2014 avoid it.",
                "max_active_runs=1: prevents concurrent DAG runs. Important for pipelines that write to the same partition."
            ]
        },
        "HardProblem": "Boss Problem (Spotify): You manage a daily pipeline: 200 Airflow DAGs, each processing data for one country. DAGs run at different times (by time zone) and share downstream dependencies (a global aggregation DAG that waits for ALL 200 country DAGs to finish). (1) Design the dependency structure. (2) One country DAG fails \u2014 should the global aggregation wait or proceed with 199/200 countries? (3) A new country is added \u2014 how do you add it without rewriting code? (4) The global aggregation consistently misses its 6am SLA by 15 minutes \u2014 diagnose and fix."
    },
    {
        "Week": 10,
        "Day": "Sunday",
        "Phase": "3. Architect (System)",
        "Theme": "Rest",
        "SpecificTopic": "Rest & Review",
        "ActionItem_Deliverable": "Review Airflow Best Practices",
        "LeetCodeProblem": null,
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Hash Maps (Dicts)</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Active Rest \u2014 How Memory Consolidates</div>\n<div class=\"rich\">\n<h4>The Neuroscience of Spaced Repetition</h4>\n<p>Rest days are not wasted days. During rest, your brain replays what it learned \u2014 a process called <strong>memory consolidation</strong>. Research consistently shows that <em>attempting to recall from memory</em> is 3\u00d7 more effective at building durable knowledge than re-reading notes. The act of struggling to remember \u2014 even failing \u2014 strengthens the neural pathway more than passive review.</p>\n<p>Think of it like muscle recovery: a weightlifter who trains 7 days straight without rest gets weaker, not stronger. The growth happens during recovery. Your brain is the same.</p>\n<h4>Today's Protocol (4 hours, structured)</h4>\n<ol>\n  <li><strong>Active Recall (60 min):</strong> Close everything. On a blank page, write every concept, formula, and pattern from this week from memory. Then check.</li>\n  <li><strong>Error Review (60 min):</strong> Find the hardest problem you struggled with. Attempt it again completely from scratch \u2014 no notes.</li>\n  <li><strong>Teach It (45 min):</strong> Open the AI chatbot and explain Gaps & Islands to it as if it's a junior engineer. If you stumble, that's your gap.</li>\n  <li><strong>Next Week Preview (15 min):</strong> Skim next Monday's topic at headline level only \u2014 priming improves retention when you study it deeply.</li>\n</ol>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Active Recall Quiz \u2014 No Notes Allowed</div>\n<div class=\"rich\">\n<h4>Answer These From Memory. Then Check.</h4>\n<pre>Window Functions:\n\u25a1 What is the difference between ROW_NUMBER, RANK, and DENSE_RANK?\n  (Write the output for a table with two tied rows)\n\u25a1 Write the syntax for a 7-day rolling average from memory\n\u25a1 What does PARTITION BY do differently from GROUP BY?\n\u25a1 Write the complete Gaps & Islands CTE from memory\n\u25a1 What is the difference between ROWS BETWEEN and RANGE BETWEEN?\n\u25a1 LAG(col, 1, 0) vs LAG(col, 1) \u2014 what is different?\n\u25a1 In what SQL execution order step do window functions run?\n\u25a1 Why can't you write WHERE rank &lt;= 3 in the same query as ROW_NUMBER?\n\nScore yourself:\n8/8 = \ud83c\udfc6 Mastery level \u2014 you're ready for interviews on this\n5-7/8 = \ud83d\udfe1 Good \u2014 review the 1-2 concepts you missed\n&lt; 5/8 = \ud83d\udd34 Go back to the specific day's content, re-read Level 1-2</pre>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Spaced Repetition Schedule</div>\n<div class=\"rich\">\n<h4>When to Review This Week's Material</h4>\n<table>\n<tr><th>Review this content</th><th>When</th><th>How</th></tr>\n<tr><td>This week</td><td>Today</td><td>Active recall quiz above</td></tr>\n<tr><td>This week</td><td>In 3 days</td><td>Re-solve the hard problem from Day 4</td></tr>\n<tr><td>This week</td><td>In 7 days</td><td>LeetCode: solve problems 185, 180, 196</td></tr>\n<tr><td>This week</td><td>In 30 days</td><td>Explain all 4 concepts to someone else</td></tr>\n</table>\n<h4>The Feynman Technique</h4>\n<p>After studying any concept, close your notes and explain it as if you're teaching a newcomer. Use only simple words. Avoid jargon. If you stumble, that's exactly where your understanding breaks down \u2014 return to just that part. This technique built Richard Feynman's legendary ability to explain complex physics simply.</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Interview Readiness Self-Assessment</div>\n<div class=\"rich\">\n<pre>Rate yourself 1\u20135 on each topic (honest self-assessment):\n  1 = \"I need to re-read from scratch\"\n  3 = \"I understand but need to look at notes\"\n  5 = \"I can code this under pressure from memory\"\n\nROW_NUMBER / RANK / DENSE_RANK:        ___/5\nRolling windows (ROWS BETWEEN):        ___/5\nLAG/LEAD (adjacent row comparison):    ___/5\nGaps & Islands (date streaks):         ___/5\nSession detection (timestamp version): ___/5\n\nAction:\n  Score &lt;3: revisit that day's Level 1 and Level 2 content tomorrow\n  Score 3:  do one LeetCode problem on that specific topic\n  Score 5:  you're interview-ready \u2705 on this topic</pre>\n<h4>FAANG Interview Reality Check</h4>\n<p>At meta, Amazon, and Google data engineering interviews, you'll be asked to write SQL live on a shared screen without autocomplete or documentation. The most common reason candidates fail: they know the concept but freeze on the syntax under pressure. The fix: write every pattern from memory at least 5 times this week. Muscle memory is more reliable than conscious recall under stress.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Memory consolidation happens during rest \u2014 the brain replays and stabilizes learned patterns during downtime.",
            "Active recall (testing yourself) builds 3x stronger memory than re-reading notes.",
            "Spaced repetition: review at day 1, day 3, day 7, day 30 for maximum retention.",
            "The Feynman Technique: if you can't explain it simply, you don't fully understand it yet.",
            "Error analysis: the problems you got WRONG are your highest-value review items \u2014 spend the most time there."
        ],
        "Tasks": [
            "<strong>Active Recall:</strong> Write the Gaps & Islands CTE from memory on paper. Check it against Day 4.",
            "<strong>Re-attempt:</strong> Pick the hardest problem from this week and solve it in 20 minutes from scratch.",
            "<strong>Teach it:</strong> Explain RANK vs DENSE_RANK to the AI chatbot as if it's a junior. Get it right without notes.",
            "<strong>Preview:</strong> Skim next week's Monday topic for 10 minutes \u2014 just headline concepts."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Review Airflow Best Practices",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Review Airflow Best Practices \u2014 trace through 2-3 edge cases before writing any code.",
                "Use the AI chatbot to quiz yourself \u2014 ask it 'Give me a SQL window function interview question.'",
                "Write SQL from memory in a plain text editor \u2014 no autocomplete. This builds true recall.",
                "The hardest concept this week: spend 30 minutes writing a full explanation in your own words."
            ]
        },
        "HardProblem": "Connect-the-dots: Write 2-3 paragraphs explaining how ROW_NUMBER, rolling averages, LAG, and Gaps & Islands could ALL be used in a single production pipeline for a ride-sharing company. What problem does each solve? How do they complement each other? What would break if you removed any one of them?"
    },
    {
        "Week": 11,
        "Day": "Monday",
        "Phase": "3. Architect (System)",
        "Theme": "Streaming",
        "SpecificTopic": "Pub/Sub Model",
        "ActionItem_Deliverable": "Explain Topics/Producers/Consumers",
        "LeetCodeProblem": "<strong>LC 703 \u2013 Kth Largest in Stream</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Delta Lake / Iceberg</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Why Kafka Exists \u2014 Core Concepts</div><div class=\"rich\"><h4>Why Kafka Exists \u2014 The Message Bus Problem</h4><p>In 2010, LinkedIn had a problem: dozens of services needed to share data in real time. Payment service needed transaction events. Recommendation engine needed click events. Analytics needed all of it. Point-to-point connections between all services would require N\u00b2 connections and N\u00b2 custom APIs. Every new consumer meant updating every producer.</p><p>LinkedIn built <strong>Apache Kafka</strong> as a distributed, append-only commit log that acts as a central data bus. Producers write events to Kafka topics. Any number of consumers read those events independently, at their own pace, without affecting other consumers or the producers.</p><h4>Core Concepts</h4><table><tr><th>Concept</th><td>What it is</td><td>Analogy</td></tr><tr><th>Topic</th><td>Named channel of events</td><td>A TV channel \u2014 broadcast to all who tune in</td></tr><tr><th>Partition</th><td>Ordered, immutable log within a topic</td><td>A video tape \u2014 you can only append, reads have an offset</td></tr><tr><th>Offset</th><td>Position of a message within a partition</td><td>Page number in a book \u2014 resume from where you left off</td></tr><tr><th>Producer</th><td>Writes messages to a topic</td><td>A journalist who publishes articles</td></tr><tr><th>Consumer</th><td>Reads messages from a topic</td><td>A reader who subscribes to the newspaper</td></tr><tr><th>Consumer Group</th><td>Set of consumers sharing work</td><td>A team reading different sections of the newspaper</td></tr><tr><th>Broker</th><td>A Kafka server storing partitions</td><td>A post office branch</td></tr></table><p>\u270d\ufe0f <strong>Key insight:</strong> Messages in Kafka are NOT deleted when consumed. They are retained for a configurable period (typically 7 days). Multiple consumers read the same message independently \u2014 each gets their own offset pointer.</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Partitions + Consumer Groups</div><div class=\"rich\"><h4>Partitions \u2014 The Key to Kafka's Scalability</h4><p>A topic is split into partitions. Each partition is an ordered, immutable log stored on one broker. Producers can write to any partition (using a key for deterministic routing). Each consumer in a group is assigned one or more partitions exclusively.</p><p><strong>The fundamental rule:</strong> one partition is consumed by exactly one consumer in a group at any time. This is how Kafka provides ordering guarantees and prevents duplicate processing within a group.</p><pre>Topic \"user_events\" with 4 partitions, Consumer Group A with 3 consumers:\n\nPartition 0 \u2192 Consumer 1 (assigned exclusively)\nPartition 1 \u2192 Consumer 1 (one consumer can have multiple partitions)\nPartition 2 \u2192 Consumer 2\nPartition 3 \u2192 Consumer 3\n\nIf Consumer 1 crashes:\n  Kafka rebalances \u2192 Consumer 2 picks up Partition 0 and 1\n\n# Scaling rule: max parallelism = number of partitions\n# 4 partitions \u2192 max 4 consumers in a group can work in parallel\n# Adding a 5th consumer \u2192 it sits idle (no partition to consume)\n\n# Partition key routing:\n# producer.send(\"user_events\", key=\"user_42\", value=event)\n# \u2192 All events for user_42 always go to the SAME partition\n# \u2192 All events for user_42 are processed in ORDER by the same consumer</pre><h4>Consumer Group \u2014 Independent Consumers Read the Same Stream</h4><pre>Topic \"orders\":\n  Group A (analytics): reads orders \u2192 writes to DW \u2192 at offset 50,000\n  Group B (fraud):     reads orders \u2192 checks for fraud \u2192 at offset 49,500\n  Group C (email):     reads orders \u2192 sends shipping emails \u2192 at offset 51,000\n\n# Each group has its own independent offset tracking\n# Producer's throughput is unaffected by how fast consumers read\n# If Group B (fraud) is slow, it does not slow down Group A or C</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Delivery Semantics + Kafka vs Queue</div><div class=\"rich\"><h4>Delivery Semantics \u2014 At-Most-Once, At-Least-Once, Exactly-Once</h4><p>The most common Kafka interview question: what delivery guarantee does Kafka provide?</p><table><tr><th>Semantic</th><td>Messages delivered</td><td>Duplicates?</td><td>Data loss?</td><td>When to use</td></tr><tr><th>At-most-once</th><td>Maybe delivered</td><td>Never</td><td>Possible</td><td>Metrics sampling, non-critical logs</td></tr><tr><th>At-least-once</th><td>Guaranteed delivered</td><td>Possible</td><td>Never</td><td>Most common: process with idempotent consumer</td></tr><tr><th>Exactly-once</th><td>Exactly once end-to-end</td><td>Never</td><td>Never</td><td>Financial transactions, audit logs</td></tr></table><p><strong>How at-least-once works:</strong> Consumer reads message \u2192 processes it \u2192 commits offset. If the consumer crashes AFTER processing but BEFORE committing, the next consumer starts from the last committed offset and re-processes the same message. This is why consuming applications must be idempotent \u2014 processing the same message twice produces the correct result.</p><pre># Idempotent consumer pattern: INSERT ... ON CONFLICT DO NOTHING\nINSERT INTO processed_orders (order_id, status, updated_at)\nVALUES (%(order_id)s, %(status)s, %(ts)s)\nON CONFLICT (order_id) DO UPDATE\n  SET status = EXCLUDED.status,\n      updated_at = EXCLUDED.updated_at\n  WHERE orders.updated_at < EXCLUDED.updated_at;\n-- Running this query twice with the same data = same result (idempotent)</pre><h4>Kafka vs Message Queue \u2014 A Critical Distinction</h4><table><tr><th>Feature</th><td>Kafka (Log)</td><td>RabbitMQ/SQS (Queue)</td></tr><tr><th>Message retention</th><td>Retained for days (configurable)</td><td>Deleted after consumption</td></tr><tr><th>Multiple consumers</th><td>Yes \u2014 each group reads independently</td><td>No \u2014 one consumer gets each message</td></tr><tr><th>Replay</th><td>Yes \u2014 seek to any offset</td><td>No \u2014 once consumed, gone</td></tr><tr><th>Ordering</th><td>Guaranteed within partition</td><td>Generally no guarantee</td></tr><tr><th>Throughput</th><td>Millions/sec (sequential disk)</td><td>Thousands/sec</td></tr><tr><th>Best for</th><td>Event streaming, audit log, multiple consumers</td><td>Task queues, job distribution</td></tr></table></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Windowing + Real-Time Analytics Design</div><div class=\"rich\"><h4>Windowing \u2014 Aggregating Over Time in a Stream</h4><p>Streams are infinite, but analytics need finite windows: \"revenue per hour,\" \"error rate per 5-minute window,\" \"top 10 pages per day.\"</p><pre># Three types of windows in stream processing (Flink/Kafka Streams/Spark Structured Streaming):\n\n# 1. Tumbling Window: fixed, non-overlapping\n#    |---1min---|---1min---|---1min---|\n#    Each event belongs to exactly one window.\n\n# 2. Sliding Window: fixed size, overlapping (stride < window size)\n#    |---10min---|\n#        |---10min---|\n#            |---10min---|\n#    Each event appears in multiple windows.\n\n# 3. Session Window: dynamic, gap-based\n#    A session starts on first event, ends after N seconds of inactivity.\n#    Common for: user session analytics, page view sessions.\n\n# Late data problem:\n#   Event timestamp: 14:02 (when it happened on the device)\n#   Processing timestamp: 14:09 (when Kafka received it \u2014 7min late)\n#   The 14:00-14:05 window is already closed! What do you do?\n\n# Solutions:\n#   Watermark: allow data up to N minutes late, then close the window\n#   Reprocess: store events in a buffer period, re-aggregate after watermark\n#   Lambda architecture: batch layer handles late data correctness,\n#   speed layer handles real-time approximate results</pre><h4>FAANG Interview: Design a Real-Time Analytics Pipeline</h4><p>Question: \"Design a system to display real-time metrics for a ride-sharing app: active riders, active drivers, rides per minute by city, surge pricing signal.\"</p><pre>Sources: mobile apps emit GPS pings, ride state changes \u2192 Kafka topics\n\nKafka topics:\n  - driver_pings (partitioned by driver_id) \u2192 100K msgs/sec\n  - ride_events  (partitioned by city)      \u2192  10K msgs/sec\n\nStream processing (Flink):\n  - 1-minute tumbling window on ride_events \u2192 count rides per city\n  - 5-minute window on driver_pings \u2192 count active drivers per city\n  - Session window on driver_pings \u2192 detect driver going offline\n\nOutput sinks:\n  - Redis (for dashboard): SET city:rides:nyc 125   (1-min rolling count)\n  - Kafka topic \"metrics\" \u2192 dashboard WebSocket feed\n  - S3 Parquet \u2192 hourly batch for BI reporting\n\nSurge pricing signal:\n  - rides_requested / active_drivers > 1.5 in last 5 minutes \u2192 surge!</pre></div></div></div>",
        "KeyConcepts": [
            "Kafka = distributed append-only commit log. Producers write, consumers read independently.",
            "Topic: named channel. Partition: ordered immutable log within a topic. Offset: position in partition.",
            "Consumer group: each partition served by exactly one consumer in the group. Max parallelism = partition count.",
            "Partition key routing: all events with the same key always go to the same partition (and same consumer).",
            "Messages NOT deleted after consumption \u2014 retained for configurable period. Multiple groups read independently.",
            "At-most-once: may lose. At-least-once: may duplicate (most common). Exactly-once: hardest, needed for finance.",
            "Consumer must be idempotent for at-least-once: INSERT ON CONFLICT DO NOTHING, or overwrite partition.",
            "Kafka vs queue: Kafka retains messages for replay. Queue deletes after consumption. Kafka supports many consumers.",
            "Windowing: tumbling (non-overlapping), sliding (overlapping), session (gap-based). Late data needs watermarks."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Design a Kafka topic layout for an e-commerce order pipeline: what topics? How many partitions each? What partition key?",
            "<strong>Step 2:</strong> Write an idempotent consumer for an orders topic: receive order events, INSERT into PostgreSQL with ON CONFLICT DO NOTHING.",
            "<strong>Step 3:</strong> Design: multiple consumers reading the same 'orders' topic independently \u2014 analytics, fraud detection, inventory update. How many consumer groups?",
            "<strong>Step 4:</strong> Model a late-arriving event problem: events arrive 5 minutes late. You have a 1-minute tumbling window for revenue. How do you handle late data?"
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Explain Topics/Producers/Consumers",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Explain Topics/Producers/Consumers \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: 'guarantee each message processed exactly once?' \u2192 explain at-least-once + idempotent consumer.",
                "Partition count limits parallelism: 4 partitions \u2192 max 4 consumers per group. Adding a 5th consumer idles.",
                "Use partition key = user_id when ordering per-user matters (e.g., user's event stream must be ordered).",
                "Kafka is NOT a database. Don't use it as a primary store. Consumers should always write to a real store."
            ]
        },
        "HardProblem": "Boss Problem (LinkedIn): Design Kafka's original use case \u2014 the activity feed pipeline. Every LinkedIn user action (post, like, comment, view) is an event streamed to Kafka. (1) What topics and partitions? (2) 3 consumers need the same data: feed renderer, analytics, notifications \u2014 how many consumer groups? (3) A user with 50M followers posts \u2014 fan-out-on-write creates 50M messages. How does Kafka handle this? (4) You need to replay the last 7 days of events to rebuild the recommendation model. Are 7-day retention and 50M events/day feasible on Kafka?"
    },
    {
        "Week": 11,
        "Day": "Tuesday",
        "Phase": "3. Architect (System)",
        "Theme": "Streaming",
        "SpecificTopic": "Kafka Internals",
        "ActionItem_Deliverable": "Explain Offsets and Commit Log",
        "LeetCodeProblem": "<strong>LC 239 \u2013 Sliding Window Maximum</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Surrogate Keys</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Why Kafka Exists \u2014 Core Concepts</div><div class=\"rich\"><h4>Why Kafka Exists \u2014 The Message Bus Problem</h4><p>In 2010, LinkedIn had a problem: dozens of services needed to share data in real time. Payment service needed transaction events. Recommendation engine needed click events. Analytics needed all of it. Point-to-point connections between all services would require N\u00b2 connections and N\u00b2 custom APIs. Every new consumer meant updating every producer.</p><p>LinkedIn built <strong>Apache Kafka</strong> as a distributed, append-only commit log that acts as a central data bus. Producers write events to Kafka topics. Any number of consumers read those events independently, at their own pace, without affecting other consumers or the producers.</p><h4>Core Concepts</h4><table><tr><th>Concept</th><td>What it is</td><td>Analogy</td></tr><tr><th>Topic</th><td>Named channel of events</td><td>A TV channel \u2014 broadcast to all who tune in</td></tr><tr><th>Partition</th><td>Ordered, immutable log within a topic</td><td>A video tape \u2014 you can only append, reads have an offset</td></tr><tr><th>Offset</th><td>Position of a message within a partition</td><td>Page number in a book \u2014 resume from where you left off</td></tr><tr><th>Producer</th><td>Writes messages to a topic</td><td>A journalist who publishes articles</td></tr><tr><th>Consumer</th><td>Reads messages from a topic</td><td>A reader who subscribes to the newspaper</td></tr><tr><th>Consumer Group</th><td>Set of consumers sharing work</td><td>A team reading different sections of the newspaper</td></tr><tr><th>Broker</th><td>A Kafka server storing partitions</td><td>A post office branch</td></tr></table><p>\u270d\ufe0f <strong>Key insight:</strong> Messages in Kafka are NOT deleted when consumed. They are retained for a configurable period (typically 7 days). Multiple consumers read the same message independently \u2014 each gets their own offset pointer.</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Partitions + Consumer Groups</div><div class=\"rich\"><h4>Partitions \u2014 The Key to Kafka's Scalability</h4><p>A topic is split into partitions. Each partition is an ordered, immutable log stored on one broker. Producers can write to any partition (using a key for deterministic routing). Each consumer in a group is assigned one or more partitions exclusively.</p><p><strong>The fundamental rule:</strong> one partition is consumed by exactly one consumer in a group at any time. This is how Kafka provides ordering guarantees and prevents duplicate processing within a group.</p><pre>Topic \"user_events\" with 4 partitions, Consumer Group A with 3 consumers:\n\nPartition 0 \u2192 Consumer 1 (assigned exclusively)\nPartition 1 \u2192 Consumer 1 (one consumer can have multiple partitions)\nPartition 2 \u2192 Consumer 2\nPartition 3 \u2192 Consumer 3\n\nIf Consumer 1 crashes:\n  Kafka rebalances \u2192 Consumer 2 picks up Partition 0 and 1\n\n# Scaling rule: max parallelism = number of partitions\n# 4 partitions \u2192 max 4 consumers in a group can work in parallel\n# Adding a 5th consumer \u2192 it sits idle (no partition to consume)\n\n# Partition key routing:\n# producer.send(\"user_events\", key=\"user_42\", value=event)\n# \u2192 All events for user_42 always go to the SAME partition\n# \u2192 All events for user_42 are processed in ORDER by the same consumer</pre><h4>Consumer Group \u2014 Independent Consumers Read the Same Stream</h4><pre>Topic \"orders\":\n  Group A (analytics): reads orders \u2192 writes to DW \u2192 at offset 50,000\n  Group B (fraud):     reads orders \u2192 checks for fraud \u2192 at offset 49,500\n  Group C (email):     reads orders \u2192 sends shipping emails \u2192 at offset 51,000\n\n# Each group has its own independent offset tracking\n# Producer's throughput is unaffected by how fast consumers read\n# If Group B (fraud) is slow, it does not slow down Group A or C</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Delivery Semantics + Kafka vs Queue</div><div class=\"rich\"><h4>Delivery Semantics \u2014 At-Most-Once, At-Least-Once, Exactly-Once</h4><p>The most common Kafka interview question: what delivery guarantee does Kafka provide?</p><table><tr><th>Semantic</th><td>Messages delivered</td><td>Duplicates?</td><td>Data loss?</td><td>When to use</td></tr><tr><th>At-most-once</th><td>Maybe delivered</td><td>Never</td><td>Possible</td><td>Metrics sampling, non-critical logs</td></tr><tr><th>At-least-once</th><td>Guaranteed delivered</td><td>Possible</td><td>Never</td><td>Most common: process with idempotent consumer</td></tr><tr><th>Exactly-once</th><td>Exactly once end-to-end</td><td>Never</td><td>Never</td><td>Financial transactions, audit logs</td></tr></table><p><strong>How at-least-once works:</strong> Consumer reads message \u2192 processes it \u2192 commits offset. If the consumer crashes AFTER processing but BEFORE committing, the next consumer starts from the last committed offset and re-processes the same message. This is why consuming applications must be idempotent \u2014 processing the same message twice produces the correct result.</p><pre># Idempotent consumer pattern: INSERT ... ON CONFLICT DO NOTHING\nINSERT INTO processed_orders (order_id, status, updated_at)\nVALUES (%(order_id)s, %(status)s, %(ts)s)\nON CONFLICT (order_id) DO UPDATE\n  SET status = EXCLUDED.status,\n      updated_at = EXCLUDED.updated_at\n  WHERE orders.updated_at < EXCLUDED.updated_at;\n-- Running this query twice with the same data = same result (idempotent)</pre><h4>Kafka vs Message Queue \u2014 A Critical Distinction</h4><table><tr><th>Feature</th><td>Kafka (Log)</td><td>RabbitMQ/SQS (Queue)</td></tr><tr><th>Message retention</th><td>Retained for days (configurable)</td><td>Deleted after consumption</td></tr><tr><th>Multiple consumers</th><td>Yes \u2014 each group reads independently</td><td>No \u2014 one consumer gets each message</td></tr><tr><th>Replay</th><td>Yes \u2014 seek to any offset</td><td>No \u2014 once consumed, gone</td></tr><tr><th>Ordering</th><td>Guaranteed within partition</td><td>Generally no guarantee</td></tr><tr><th>Throughput</th><td>Millions/sec (sequential disk)</td><td>Thousands/sec</td></tr><tr><th>Best for</th><td>Event streaming, audit log, multiple consumers</td><td>Task queues, job distribution</td></tr></table></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Windowing + Real-Time Analytics Design</div><div class=\"rich\"><h4>Windowing \u2014 Aggregating Over Time in a Stream</h4><p>Streams are infinite, but analytics need finite windows: \"revenue per hour,\" \"error rate per 5-minute window,\" \"top 10 pages per day.\"</p><pre># Three types of windows in stream processing (Flink/Kafka Streams/Spark Structured Streaming):\n\n# 1. Tumbling Window: fixed, non-overlapping\n#    |---1min---|---1min---|---1min---|\n#    Each event belongs to exactly one window.\n\n# 2. Sliding Window: fixed size, overlapping (stride < window size)\n#    |---10min---|\n#        |---10min---|\n#            |---10min---|\n#    Each event appears in multiple windows.\n\n# 3. Session Window: dynamic, gap-based\n#    A session starts on first event, ends after N seconds of inactivity.\n#    Common for: user session analytics, page view sessions.\n\n# Late data problem:\n#   Event timestamp: 14:02 (when it happened on the device)\n#   Processing timestamp: 14:09 (when Kafka received it \u2014 7min late)\n#   The 14:00-14:05 window is already closed! What do you do?\n\n# Solutions:\n#   Watermark: allow data up to N minutes late, then close the window\n#   Reprocess: store events in a buffer period, re-aggregate after watermark\n#   Lambda architecture: batch layer handles late data correctness,\n#   speed layer handles real-time approximate results</pre><h4>FAANG Interview: Design a Real-Time Analytics Pipeline</h4><p>Question: \"Design a system to display real-time metrics for a ride-sharing app: active riders, active drivers, rides per minute by city, surge pricing signal.\"</p><pre>Sources: mobile apps emit GPS pings, ride state changes \u2192 Kafka topics\n\nKafka topics:\n  - driver_pings (partitioned by driver_id) \u2192 100K msgs/sec\n  - ride_events  (partitioned by city)      \u2192  10K msgs/sec\n\nStream processing (Flink):\n  - 1-minute tumbling window on ride_events \u2192 count rides per city\n  - 5-minute window on driver_pings \u2192 count active drivers per city\n  - Session window on driver_pings \u2192 detect driver going offline\n\nOutput sinks:\n  - Redis (for dashboard): SET city:rides:nyc 125   (1-min rolling count)\n  - Kafka topic \"metrics\" \u2192 dashboard WebSocket feed\n  - S3 Parquet \u2192 hourly batch for BI reporting\n\nSurge pricing signal:\n  - rides_requested / active_drivers > 1.5 in last 5 minutes \u2192 surge!</pre></div></div></div>",
        "KeyConcepts": [
            "Kafka = distributed append-only commit log. Producers write, consumers read independently.",
            "Topic: named channel. Partition: ordered immutable log within a topic. Offset: position in partition.",
            "Consumer group: each partition served by exactly one consumer in the group. Max parallelism = partition count.",
            "Partition key routing: all events with the same key always go to the same partition (and same consumer).",
            "Messages NOT deleted after consumption \u2014 retained for configurable period. Multiple groups read independently.",
            "At-most-once: may lose. At-least-once: may duplicate (most common). Exactly-once: hardest, needed for finance.",
            "Consumer must be idempotent for at-least-once: INSERT ON CONFLICT DO NOTHING, or overwrite partition.",
            "Kafka vs queue: Kafka retains messages for replay. Queue deletes after consumption. Kafka supports many consumers.",
            "Windowing: tumbling (non-overlapping), sliding (overlapping), session (gap-based). Late data needs watermarks."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Design a Kafka topic layout for an e-commerce order pipeline: what topics? How many partitions each? What partition key?",
            "<strong>Step 2:</strong> Write an idempotent consumer for an orders topic: receive order events, INSERT into PostgreSQL with ON CONFLICT DO NOTHING.",
            "<strong>Step 3:</strong> Design: multiple consumers reading the same 'orders' topic independently \u2014 analytics, fraud detection, inventory update. How many consumer groups?",
            "<strong>Step 4:</strong> Model a late-arriving event problem: events arrive 5 minutes late. You have a 1-minute tumbling window for revenue. How do you handle late data?"
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Explain Offsets and Commit Log",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Explain Offsets and Commit Log \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: 'guarantee each message processed exactly once?' \u2192 explain at-least-once + idempotent consumer.",
                "Partition count limits parallelism: 4 partitions \u2192 max 4 consumers per group. Adding a 5th consumer idles.",
                "Use partition key = user_id when ordering per-user matters (e.g., user's event stream must be ordered).",
                "Kafka is NOT a database. Don't use it as a primary store. Consumers should always write to a real store."
            ]
        },
        "HardProblem": "Boss Problem (LinkedIn): Design Kafka's original use case \u2014 the activity feed pipeline. Every LinkedIn user action (post, like, comment, view) is an event streamed to Kafka. (1) What topics and partitions? (2) 3 consumers need the same data: feed renderer, analytics, notifications \u2014 how many consumer groups? (3) A user with 50M followers posts \u2014 fan-out-on-write creates 50M messages. How does Kafka handle this? (4) You need to replay the last 7 days of events to rebuild the recommendation model. Are 7-day retention and 50M events/day feasible on Kafka?"
    },
    {
        "Week": 11,
        "Day": "Wednesday",
        "Phase": "3. Architect (System)",
        "Theme": "Streaming",
        "SpecificTopic": "Consumer Groups",
        "ActionItem_Deliverable": "Explain how Consumer Groups scale reads",
        "LeetCodeProblem": "<strong>LC 480 \u2013 Sliding Window Median</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>File I/O</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Why Kafka Exists \u2014 Core Concepts</div><div class=\"rich\"><h4>Why Kafka Exists \u2014 The Message Bus Problem</h4><p>In 2010, LinkedIn had a problem: dozens of services needed to share data in real time. Payment service needed transaction events. Recommendation engine needed click events. Analytics needed all of it. Point-to-point connections between all services would require N\u00b2 connections and N\u00b2 custom APIs. Every new consumer meant updating every producer.</p><p>LinkedIn built <strong>Apache Kafka</strong> as a distributed, append-only commit log that acts as a central data bus. Producers write events to Kafka topics. Any number of consumers read those events independently, at their own pace, without affecting other consumers or the producers.</p><h4>Core Concepts</h4><table><tr><th>Concept</th><td>What it is</td><td>Analogy</td></tr><tr><th>Topic</th><td>Named channel of events</td><td>A TV channel \u2014 broadcast to all who tune in</td></tr><tr><th>Partition</th><td>Ordered, immutable log within a topic</td><td>A video tape \u2014 you can only append, reads have an offset</td></tr><tr><th>Offset</th><td>Position of a message within a partition</td><td>Page number in a book \u2014 resume from where you left off</td></tr><tr><th>Producer</th><td>Writes messages to a topic</td><td>A journalist who publishes articles</td></tr><tr><th>Consumer</th><td>Reads messages from a topic</td><td>A reader who subscribes to the newspaper</td></tr><tr><th>Consumer Group</th><td>Set of consumers sharing work</td><td>A team reading different sections of the newspaper</td></tr><tr><th>Broker</th><td>A Kafka server storing partitions</td><td>A post office branch</td></tr></table><p>\u270d\ufe0f <strong>Key insight:</strong> Messages in Kafka are NOT deleted when consumed. They are retained for a configurable period (typically 7 days). Multiple consumers read the same message independently \u2014 each gets their own offset pointer.</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Partitions + Consumer Groups</div><div class=\"rich\"><h4>Partitions \u2014 The Key to Kafka's Scalability</h4><p>A topic is split into partitions. Each partition is an ordered, immutable log stored on one broker. Producers can write to any partition (using a key for deterministic routing). Each consumer in a group is assigned one or more partitions exclusively.</p><p><strong>The fundamental rule:</strong> one partition is consumed by exactly one consumer in a group at any time. This is how Kafka provides ordering guarantees and prevents duplicate processing within a group.</p><pre>Topic \"user_events\" with 4 partitions, Consumer Group A with 3 consumers:\n\nPartition 0 \u2192 Consumer 1 (assigned exclusively)\nPartition 1 \u2192 Consumer 1 (one consumer can have multiple partitions)\nPartition 2 \u2192 Consumer 2\nPartition 3 \u2192 Consumer 3\n\nIf Consumer 1 crashes:\n  Kafka rebalances \u2192 Consumer 2 picks up Partition 0 and 1\n\n# Scaling rule: max parallelism = number of partitions\n# 4 partitions \u2192 max 4 consumers in a group can work in parallel\n# Adding a 5th consumer \u2192 it sits idle (no partition to consume)\n\n# Partition key routing:\n# producer.send(\"user_events\", key=\"user_42\", value=event)\n# \u2192 All events for user_42 always go to the SAME partition\n# \u2192 All events for user_42 are processed in ORDER by the same consumer</pre><h4>Consumer Group \u2014 Independent Consumers Read the Same Stream</h4><pre>Topic \"orders\":\n  Group A (analytics): reads orders \u2192 writes to DW \u2192 at offset 50,000\n  Group B (fraud):     reads orders \u2192 checks for fraud \u2192 at offset 49,500\n  Group C (email):     reads orders \u2192 sends shipping emails \u2192 at offset 51,000\n\n# Each group has its own independent offset tracking\n# Producer's throughput is unaffected by how fast consumers read\n# If Group B (fraud) is slow, it does not slow down Group A or C</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Delivery Semantics + Kafka vs Queue</div><div class=\"rich\"><h4>Delivery Semantics \u2014 At-Most-Once, At-Least-Once, Exactly-Once</h4><p>The most common Kafka interview question: what delivery guarantee does Kafka provide?</p><table><tr><th>Semantic</th><td>Messages delivered</td><td>Duplicates?</td><td>Data loss?</td><td>When to use</td></tr><tr><th>At-most-once</th><td>Maybe delivered</td><td>Never</td><td>Possible</td><td>Metrics sampling, non-critical logs</td></tr><tr><th>At-least-once</th><td>Guaranteed delivered</td><td>Possible</td><td>Never</td><td>Most common: process with idempotent consumer</td></tr><tr><th>Exactly-once</th><td>Exactly once end-to-end</td><td>Never</td><td>Never</td><td>Financial transactions, audit logs</td></tr></table><p><strong>How at-least-once works:</strong> Consumer reads message \u2192 processes it \u2192 commits offset. If the consumer crashes AFTER processing but BEFORE committing, the next consumer starts from the last committed offset and re-processes the same message. This is why consuming applications must be idempotent \u2014 processing the same message twice produces the correct result.</p><pre># Idempotent consumer pattern: INSERT ... ON CONFLICT DO NOTHING\nINSERT INTO processed_orders (order_id, status, updated_at)\nVALUES (%(order_id)s, %(status)s, %(ts)s)\nON CONFLICT (order_id) DO UPDATE\n  SET status = EXCLUDED.status,\n      updated_at = EXCLUDED.updated_at\n  WHERE orders.updated_at < EXCLUDED.updated_at;\n-- Running this query twice with the same data = same result (idempotent)</pre><h4>Kafka vs Message Queue \u2014 A Critical Distinction</h4><table><tr><th>Feature</th><td>Kafka (Log)</td><td>RabbitMQ/SQS (Queue)</td></tr><tr><th>Message retention</th><td>Retained for days (configurable)</td><td>Deleted after consumption</td></tr><tr><th>Multiple consumers</th><td>Yes \u2014 each group reads independently</td><td>No \u2014 one consumer gets each message</td></tr><tr><th>Replay</th><td>Yes \u2014 seek to any offset</td><td>No \u2014 once consumed, gone</td></tr><tr><th>Ordering</th><td>Guaranteed within partition</td><td>Generally no guarantee</td></tr><tr><th>Throughput</th><td>Millions/sec (sequential disk)</td><td>Thousands/sec</td></tr><tr><th>Best for</th><td>Event streaming, audit log, multiple consumers</td><td>Task queues, job distribution</td></tr></table></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Windowing + Real-Time Analytics Design</div><div class=\"rich\"><h4>Windowing \u2014 Aggregating Over Time in a Stream</h4><p>Streams are infinite, but analytics need finite windows: \"revenue per hour,\" \"error rate per 5-minute window,\" \"top 10 pages per day.\"</p><pre># Three types of windows in stream processing (Flink/Kafka Streams/Spark Structured Streaming):\n\n# 1. Tumbling Window: fixed, non-overlapping\n#    |---1min---|---1min---|---1min---|\n#    Each event belongs to exactly one window.\n\n# 2. Sliding Window: fixed size, overlapping (stride < window size)\n#    |---10min---|\n#        |---10min---|\n#            |---10min---|\n#    Each event appears in multiple windows.\n\n# 3. Session Window: dynamic, gap-based\n#    A session starts on first event, ends after N seconds of inactivity.\n#    Common for: user session analytics, page view sessions.\n\n# Late data problem:\n#   Event timestamp: 14:02 (when it happened on the device)\n#   Processing timestamp: 14:09 (when Kafka received it \u2014 7min late)\n#   The 14:00-14:05 window is already closed! What do you do?\n\n# Solutions:\n#   Watermark: allow data up to N minutes late, then close the window\n#   Reprocess: store events in a buffer period, re-aggregate after watermark\n#   Lambda architecture: batch layer handles late data correctness,\n#   speed layer handles real-time approximate results</pre><h4>FAANG Interview: Design a Real-Time Analytics Pipeline</h4><p>Question: \"Design a system to display real-time metrics for a ride-sharing app: active riders, active drivers, rides per minute by city, surge pricing signal.\"</p><pre>Sources: mobile apps emit GPS pings, ride state changes \u2192 Kafka topics\n\nKafka topics:\n  - driver_pings (partitioned by driver_id) \u2192 100K msgs/sec\n  - ride_events  (partitioned by city)      \u2192  10K msgs/sec\n\nStream processing (Flink):\n  - 1-minute tumbling window on ride_events \u2192 count rides per city\n  - 5-minute window on driver_pings \u2192 count active drivers per city\n  - Session window on driver_pings \u2192 detect driver going offline\n\nOutput sinks:\n  - Redis (for dashboard): SET city:rides:nyc 125   (1-min rolling count)\n  - Kafka topic \"metrics\" \u2192 dashboard WebSocket feed\n  - S3 Parquet \u2192 hourly batch for BI reporting\n\nSurge pricing signal:\n  - rides_requested / active_drivers > 1.5 in last 5 minutes \u2192 surge!</pre></div></div></div>",
        "KeyConcepts": [
            "Kafka = distributed append-only commit log. Producers write, consumers read independently.",
            "Topic: named channel. Partition: ordered immutable log within a topic. Offset: position in partition.",
            "Consumer group: each partition served by exactly one consumer in the group. Max parallelism = partition count.",
            "Partition key routing: all events with the same key always go to the same partition (and same consumer).",
            "Messages NOT deleted after consumption \u2014 retained for configurable period. Multiple groups read independently.",
            "At-most-once: may lose. At-least-once: may duplicate (most common). Exactly-once: hardest, needed for finance.",
            "Consumer must be idempotent for at-least-once: INSERT ON CONFLICT DO NOTHING, or overwrite partition.",
            "Kafka vs queue: Kafka retains messages for replay. Queue deletes after consumption. Kafka supports many consumers.",
            "Windowing: tumbling (non-overlapping), sliding (overlapping), session (gap-based). Late data needs watermarks."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Design a Kafka topic layout for an e-commerce order pipeline: what topics? How many partitions each? What partition key?",
            "<strong>Step 2:</strong> Write an idempotent consumer for an orders topic: receive order events, INSERT into PostgreSQL with ON CONFLICT DO NOTHING.",
            "<strong>Step 3:</strong> Design: multiple consumers reading the same 'orders' topic independently \u2014 analytics, fraud detection, inventory update. How many consumer groups?",
            "<strong>Step 4:</strong> Model a late-arriving event problem: events arrive 5 minutes late. You have a 1-minute tumbling window for revenue. How do you handle late data?"
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Explain how Consumer Groups scale reads",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Explain how Consumer Groups scale reads \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: 'guarantee each message processed exactly once?' \u2192 explain at-least-once + idempotent consumer.",
                "Partition count limits parallelism: 4 partitions \u2192 max 4 consumers per group. Adding a 5th consumer idles.",
                "Use partition key = user_id when ordering per-user matters (e.g., user's event stream must be ordered).",
                "Kafka is NOT a database. Don't use it as a primary store. Consumers should always write to a real store."
            ]
        },
        "HardProblem": "Boss Problem (LinkedIn): Design Kafka's original use case \u2014 the activity feed pipeline. Every LinkedIn user action (post, like, comment, view) is an event streamed to Kafka. (1) What topics and partitions? (2) 3 consumers need the same data: feed renderer, analytics, notifications \u2014 how many consumer groups? (3) A user with 50M followers posts \u2014 fan-out-on-write creates 50M messages. How does Kafka handle this? (4) You need to replay the last 7 days of events to rebuild the recommendation model. Are 7-day retention and 50M events/day feasible on Kafka?"
    },
    {
        "Week": 11,
        "Day": "Thursday",
        "Phase": "3. Architect (System)",
        "Theme": "Streaming",
        "SpecificTopic": "Semantics",
        "ActionItem_Deliverable": "Explain At-least-once vs Exactly-once",
        "LeetCodeProblem": "<strong>LC 23 \u2013 Merge K Sorted Lists</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Backfilling</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Why Kafka Exists \u2014 Core Concepts</div><div class=\"rich\"><h4>Why Kafka Exists \u2014 The Message Bus Problem</h4><p>In 2010, LinkedIn had a problem: dozens of services needed to share data in real time. Payment service needed transaction events. Recommendation engine needed click events. Analytics needed all of it. Point-to-point connections between all services would require N\u00b2 connections and N\u00b2 custom APIs. Every new consumer meant updating every producer.</p><p>LinkedIn built <strong>Apache Kafka</strong> as a distributed, append-only commit log that acts as a central data bus. Producers write events to Kafka topics. Any number of consumers read those events independently, at their own pace, without affecting other consumers or the producers.</p><h4>Core Concepts</h4><table><tr><th>Concept</th><td>What it is</td><td>Analogy</td></tr><tr><th>Topic</th><td>Named channel of events</td><td>A TV channel \u2014 broadcast to all who tune in</td></tr><tr><th>Partition</th><td>Ordered, immutable log within a topic</td><td>A video tape \u2014 you can only append, reads have an offset</td></tr><tr><th>Offset</th><td>Position of a message within a partition</td><td>Page number in a book \u2014 resume from where you left off</td></tr><tr><th>Producer</th><td>Writes messages to a topic</td><td>A journalist who publishes articles</td></tr><tr><th>Consumer</th><td>Reads messages from a topic</td><td>A reader who subscribes to the newspaper</td></tr><tr><th>Consumer Group</th><td>Set of consumers sharing work</td><td>A team reading different sections of the newspaper</td></tr><tr><th>Broker</th><td>A Kafka server storing partitions</td><td>A post office branch</td></tr></table><p>\u270d\ufe0f <strong>Key insight:</strong> Messages in Kafka are NOT deleted when consumed. They are retained for a configurable period (typically 7 days). Multiple consumers read the same message independently \u2014 each gets their own offset pointer.</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Partitions + Consumer Groups</div><div class=\"rich\"><h4>Partitions \u2014 The Key to Kafka's Scalability</h4><p>A topic is split into partitions. Each partition is an ordered, immutable log stored on one broker. Producers can write to any partition (using a key for deterministic routing). Each consumer in a group is assigned one or more partitions exclusively.</p><p><strong>The fundamental rule:</strong> one partition is consumed by exactly one consumer in a group at any time. This is how Kafka provides ordering guarantees and prevents duplicate processing within a group.</p><pre>Topic \"user_events\" with 4 partitions, Consumer Group A with 3 consumers:\n\nPartition 0 \u2192 Consumer 1 (assigned exclusively)\nPartition 1 \u2192 Consumer 1 (one consumer can have multiple partitions)\nPartition 2 \u2192 Consumer 2\nPartition 3 \u2192 Consumer 3\n\nIf Consumer 1 crashes:\n  Kafka rebalances \u2192 Consumer 2 picks up Partition 0 and 1\n\n# Scaling rule: max parallelism = number of partitions\n# 4 partitions \u2192 max 4 consumers in a group can work in parallel\n# Adding a 5th consumer \u2192 it sits idle (no partition to consume)\n\n# Partition key routing:\n# producer.send(\"user_events\", key=\"user_42\", value=event)\n# \u2192 All events for user_42 always go to the SAME partition\n# \u2192 All events for user_42 are processed in ORDER by the same consumer</pre><h4>Consumer Group \u2014 Independent Consumers Read the Same Stream</h4><pre>Topic \"orders\":\n  Group A (analytics): reads orders \u2192 writes to DW \u2192 at offset 50,000\n  Group B (fraud):     reads orders \u2192 checks for fraud \u2192 at offset 49,500\n  Group C (email):     reads orders \u2192 sends shipping emails \u2192 at offset 51,000\n\n# Each group has its own independent offset tracking\n# Producer's throughput is unaffected by how fast consumers read\n# If Group B (fraud) is slow, it does not slow down Group A or C</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Delivery Semantics + Kafka vs Queue</div><div class=\"rich\"><h4>Delivery Semantics \u2014 At-Most-Once, At-Least-Once, Exactly-Once</h4><p>The most common Kafka interview question: what delivery guarantee does Kafka provide?</p><table><tr><th>Semantic</th><td>Messages delivered</td><td>Duplicates?</td><td>Data loss?</td><td>When to use</td></tr><tr><th>At-most-once</th><td>Maybe delivered</td><td>Never</td><td>Possible</td><td>Metrics sampling, non-critical logs</td></tr><tr><th>At-least-once</th><td>Guaranteed delivered</td><td>Possible</td><td>Never</td><td>Most common: process with idempotent consumer</td></tr><tr><th>Exactly-once</th><td>Exactly once end-to-end</td><td>Never</td><td>Never</td><td>Financial transactions, audit logs</td></tr></table><p><strong>How at-least-once works:</strong> Consumer reads message \u2192 processes it \u2192 commits offset. If the consumer crashes AFTER processing but BEFORE committing, the next consumer starts from the last committed offset and re-processes the same message. This is why consuming applications must be idempotent \u2014 processing the same message twice produces the correct result.</p><pre># Idempotent consumer pattern: INSERT ... ON CONFLICT DO NOTHING\nINSERT INTO processed_orders (order_id, status, updated_at)\nVALUES (%(order_id)s, %(status)s, %(ts)s)\nON CONFLICT (order_id) DO UPDATE\n  SET status = EXCLUDED.status,\n      updated_at = EXCLUDED.updated_at\n  WHERE orders.updated_at < EXCLUDED.updated_at;\n-- Running this query twice with the same data = same result (idempotent)</pre><h4>Kafka vs Message Queue \u2014 A Critical Distinction</h4><table><tr><th>Feature</th><td>Kafka (Log)</td><td>RabbitMQ/SQS (Queue)</td></tr><tr><th>Message retention</th><td>Retained for days (configurable)</td><td>Deleted after consumption</td></tr><tr><th>Multiple consumers</th><td>Yes \u2014 each group reads independently</td><td>No \u2014 one consumer gets each message</td></tr><tr><th>Replay</th><td>Yes \u2014 seek to any offset</td><td>No \u2014 once consumed, gone</td></tr><tr><th>Ordering</th><td>Guaranteed within partition</td><td>Generally no guarantee</td></tr><tr><th>Throughput</th><td>Millions/sec (sequential disk)</td><td>Thousands/sec</td></tr><tr><th>Best for</th><td>Event streaming, audit log, multiple consumers</td><td>Task queues, job distribution</td></tr></table></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Windowing + Real-Time Analytics Design</div><div class=\"rich\"><h4>Windowing \u2014 Aggregating Over Time in a Stream</h4><p>Streams are infinite, but analytics need finite windows: \"revenue per hour,\" \"error rate per 5-minute window,\" \"top 10 pages per day.\"</p><pre># Three types of windows in stream processing (Flink/Kafka Streams/Spark Structured Streaming):\n\n# 1. Tumbling Window: fixed, non-overlapping\n#    |---1min---|---1min---|---1min---|\n#    Each event belongs to exactly one window.\n\n# 2. Sliding Window: fixed size, overlapping (stride < window size)\n#    |---10min---|\n#        |---10min---|\n#            |---10min---|\n#    Each event appears in multiple windows.\n\n# 3. Session Window: dynamic, gap-based\n#    A session starts on first event, ends after N seconds of inactivity.\n#    Common for: user session analytics, page view sessions.\n\n# Late data problem:\n#   Event timestamp: 14:02 (when it happened on the device)\n#   Processing timestamp: 14:09 (when Kafka received it \u2014 7min late)\n#   The 14:00-14:05 window is already closed! What do you do?\n\n# Solutions:\n#   Watermark: allow data up to N minutes late, then close the window\n#   Reprocess: store events in a buffer period, re-aggregate after watermark\n#   Lambda architecture: batch layer handles late data correctness,\n#   speed layer handles real-time approximate results</pre><h4>FAANG Interview: Design a Real-Time Analytics Pipeline</h4><p>Question: \"Design a system to display real-time metrics for a ride-sharing app: active riders, active drivers, rides per minute by city, surge pricing signal.\"</p><pre>Sources: mobile apps emit GPS pings, ride state changes \u2192 Kafka topics\n\nKafka topics:\n  - driver_pings (partitioned by driver_id) \u2192 100K msgs/sec\n  - ride_events  (partitioned by city)      \u2192  10K msgs/sec\n\nStream processing (Flink):\n  - 1-minute tumbling window on ride_events \u2192 count rides per city\n  - 5-minute window on driver_pings \u2192 count active drivers per city\n  - Session window on driver_pings \u2192 detect driver going offline\n\nOutput sinks:\n  - Redis (for dashboard): SET city:rides:nyc 125   (1-min rolling count)\n  - Kafka topic \"metrics\" \u2192 dashboard WebSocket feed\n  - S3 Parquet \u2192 hourly batch for BI reporting\n\nSurge pricing signal:\n  - rides_requested / active_drivers > 1.5 in last 5 minutes \u2192 surge!</pre></div></div></div>",
        "KeyConcepts": [
            "Kafka = distributed append-only commit log. Producers write, consumers read independently.",
            "Topic: named channel. Partition: ordered immutable log within a topic. Offset: position in partition.",
            "Consumer group: each partition served by exactly one consumer in the group. Max parallelism = partition count.",
            "Partition key routing: all events with the same key always go to the same partition (and same consumer).",
            "Messages NOT deleted after consumption \u2014 retained for configurable period. Multiple groups read independently.",
            "At-most-once: may lose. At-least-once: may duplicate (most common). Exactly-once: hardest, needed for finance.",
            "Consumer must be idempotent for at-least-once: INSERT ON CONFLICT DO NOTHING, or overwrite partition.",
            "Kafka vs queue: Kafka retains messages for replay. Queue deletes after consumption. Kafka supports many consumers.",
            "Windowing: tumbling (non-overlapping), sliding (overlapping), session (gap-based). Late data needs watermarks."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Design a Kafka topic layout for an e-commerce order pipeline: what topics? How many partitions each? What partition key?",
            "<strong>Step 2:</strong> Write an idempotent consumer for an orders topic: receive order events, INSERT into PostgreSQL with ON CONFLICT DO NOTHING.",
            "<strong>Step 3:</strong> Design: multiple consumers reading the same 'orders' topic independently \u2014 analytics, fraud detection, inventory update. How many consumer groups?",
            "<strong>Step 4:</strong> Model a late-arriving event problem: events arrive 5 minutes late. You have a 1-minute tumbling window for revenue. How do you handle late data?"
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Explain At-least-once vs Exactly-once",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Explain At-least-once vs Exactly-once \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: 'guarantee each message processed exactly once?' \u2192 explain at-least-once + idempotent consumer.",
                "Partition count limits parallelism: 4 partitions \u2192 max 4 consumers per group. Adding a 5th consumer idles.",
                "Use partition key = user_id when ordering per-user matters (e.g., user's event stream must be ordered).",
                "Kafka is NOT a database. Don't use it as a primary store. Consumers should always write to a real store."
            ]
        },
        "HardProblem": "Boss Problem (LinkedIn): Design Kafka's original use case \u2014 the activity feed pipeline. Every LinkedIn user action (post, like, comment, view) is an event streamed to Kafka. (1) What topics and partitions? (2) 3 consumers need the same data: feed renderer, analytics, notifications \u2014 how many consumer groups? (3) A user with 50M followers posts \u2014 fan-out-on-write creates 50M messages. How does Kafka handle this? (4) You need to replay the last 7 days of events to rebuild the recommendation model. Are 7-day retention and 50M events/day feasible on Kafka?"
    },
    {
        "Week": 11,
        "Day": "Friday",
        "Phase": "3. Architect (System)",
        "Theme": "Streaming",
        "SpecificTopic": "Windowing",
        "ActionItem_Deliverable": "Draw Tumbling vs Sliding Window",
        "LeetCodeProblem": "<strong>LC 621 \u2013 Task Scheduler</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Row vs Columnar</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Why Kafka Exists \u2014 Core Concepts</div><div class=\"rich\"><h4>Why Kafka Exists \u2014 The Message Bus Problem</h4><p>In 2010, LinkedIn had a problem: dozens of services needed to share data in real time. Payment service needed transaction events. Recommendation engine needed click events. Analytics needed all of it. Point-to-point connections between all services would require N\u00b2 connections and N\u00b2 custom APIs. Every new consumer meant updating every producer.</p><p>LinkedIn built <strong>Apache Kafka</strong> as a distributed, append-only commit log that acts as a central data bus. Producers write events to Kafka topics. Any number of consumers read those events independently, at their own pace, without affecting other consumers or the producers.</p><h4>Core Concepts</h4><table><tr><th>Concept</th><td>What it is</td><td>Analogy</td></tr><tr><th>Topic</th><td>Named channel of events</td><td>A TV channel \u2014 broadcast to all who tune in</td></tr><tr><th>Partition</th><td>Ordered, immutable log within a topic</td><td>A video tape \u2014 you can only append, reads have an offset</td></tr><tr><th>Offset</th><td>Position of a message within a partition</td><td>Page number in a book \u2014 resume from where you left off</td></tr><tr><th>Producer</th><td>Writes messages to a topic</td><td>A journalist who publishes articles</td></tr><tr><th>Consumer</th><td>Reads messages from a topic</td><td>A reader who subscribes to the newspaper</td></tr><tr><th>Consumer Group</th><td>Set of consumers sharing work</td><td>A team reading different sections of the newspaper</td></tr><tr><th>Broker</th><td>A Kafka server storing partitions</td><td>A post office branch</td></tr></table><p>\u270d\ufe0f <strong>Key insight:</strong> Messages in Kafka are NOT deleted when consumed. They are retained for a configurable period (typically 7 days). Multiple consumers read the same message independently \u2014 each gets their own offset pointer.</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Partitions + Consumer Groups</div><div class=\"rich\"><h4>Partitions \u2014 The Key to Kafka's Scalability</h4><p>A topic is split into partitions. Each partition is an ordered, immutable log stored on one broker. Producers can write to any partition (using a key for deterministic routing). Each consumer in a group is assigned one or more partitions exclusively.</p><p><strong>The fundamental rule:</strong> one partition is consumed by exactly one consumer in a group at any time. This is how Kafka provides ordering guarantees and prevents duplicate processing within a group.</p><pre>Topic \"user_events\" with 4 partitions, Consumer Group A with 3 consumers:\n\nPartition 0 \u2192 Consumer 1 (assigned exclusively)\nPartition 1 \u2192 Consumer 1 (one consumer can have multiple partitions)\nPartition 2 \u2192 Consumer 2\nPartition 3 \u2192 Consumer 3\n\nIf Consumer 1 crashes:\n  Kafka rebalances \u2192 Consumer 2 picks up Partition 0 and 1\n\n# Scaling rule: max parallelism = number of partitions\n# 4 partitions \u2192 max 4 consumers in a group can work in parallel\n# Adding a 5th consumer \u2192 it sits idle (no partition to consume)\n\n# Partition key routing:\n# producer.send(\"user_events\", key=\"user_42\", value=event)\n# \u2192 All events for user_42 always go to the SAME partition\n# \u2192 All events for user_42 are processed in ORDER by the same consumer</pre><h4>Consumer Group \u2014 Independent Consumers Read the Same Stream</h4><pre>Topic \"orders\":\n  Group A (analytics): reads orders \u2192 writes to DW \u2192 at offset 50,000\n  Group B (fraud):     reads orders \u2192 checks for fraud \u2192 at offset 49,500\n  Group C (email):     reads orders \u2192 sends shipping emails \u2192 at offset 51,000\n\n# Each group has its own independent offset tracking\n# Producer's throughput is unaffected by how fast consumers read\n# If Group B (fraud) is slow, it does not slow down Group A or C</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Delivery Semantics + Kafka vs Queue</div><div class=\"rich\"><h4>Delivery Semantics \u2014 At-Most-Once, At-Least-Once, Exactly-Once</h4><p>The most common Kafka interview question: what delivery guarantee does Kafka provide?</p><table><tr><th>Semantic</th><td>Messages delivered</td><td>Duplicates?</td><td>Data loss?</td><td>When to use</td></tr><tr><th>At-most-once</th><td>Maybe delivered</td><td>Never</td><td>Possible</td><td>Metrics sampling, non-critical logs</td></tr><tr><th>At-least-once</th><td>Guaranteed delivered</td><td>Possible</td><td>Never</td><td>Most common: process with idempotent consumer</td></tr><tr><th>Exactly-once</th><td>Exactly once end-to-end</td><td>Never</td><td>Never</td><td>Financial transactions, audit logs</td></tr></table><p><strong>How at-least-once works:</strong> Consumer reads message \u2192 processes it \u2192 commits offset. If the consumer crashes AFTER processing but BEFORE committing, the next consumer starts from the last committed offset and re-processes the same message. This is why consuming applications must be idempotent \u2014 processing the same message twice produces the correct result.</p><pre># Idempotent consumer pattern: INSERT ... ON CONFLICT DO NOTHING\nINSERT INTO processed_orders (order_id, status, updated_at)\nVALUES (%(order_id)s, %(status)s, %(ts)s)\nON CONFLICT (order_id) DO UPDATE\n  SET status = EXCLUDED.status,\n      updated_at = EXCLUDED.updated_at\n  WHERE orders.updated_at < EXCLUDED.updated_at;\n-- Running this query twice with the same data = same result (idempotent)</pre><h4>Kafka vs Message Queue \u2014 A Critical Distinction</h4><table><tr><th>Feature</th><td>Kafka (Log)</td><td>RabbitMQ/SQS (Queue)</td></tr><tr><th>Message retention</th><td>Retained for days (configurable)</td><td>Deleted after consumption</td></tr><tr><th>Multiple consumers</th><td>Yes \u2014 each group reads independently</td><td>No \u2014 one consumer gets each message</td></tr><tr><th>Replay</th><td>Yes \u2014 seek to any offset</td><td>No \u2014 once consumed, gone</td></tr><tr><th>Ordering</th><td>Guaranteed within partition</td><td>Generally no guarantee</td></tr><tr><th>Throughput</th><td>Millions/sec (sequential disk)</td><td>Thousands/sec</td></tr><tr><th>Best for</th><td>Event streaming, audit log, multiple consumers</td><td>Task queues, job distribution</td></tr></table></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Windowing + Real-Time Analytics Design</div><div class=\"rich\"><h4>Windowing \u2014 Aggregating Over Time in a Stream</h4><p>Streams are infinite, but analytics need finite windows: \"revenue per hour,\" \"error rate per 5-minute window,\" \"top 10 pages per day.\"</p><pre># Three types of windows in stream processing (Flink/Kafka Streams/Spark Structured Streaming):\n\n# 1. Tumbling Window: fixed, non-overlapping\n#    |---1min---|---1min---|---1min---|\n#    Each event belongs to exactly one window.\n\n# 2. Sliding Window: fixed size, overlapping (stride < window size)\n#    |---10min---|\n#        |---10min---|\n#            |---10min---|\n#    Each event appears in multiple windows.\n\n# 3. Session Window: dynamic, gap-based\n#    A session starts on first event, ends after N seconds of inactivity.\n#    Common for: user session analytics, page view sessions.\n\n# Late data problem:\n#   Event timestamp: 14:02 (when it happened on the device)\n#   Processing timestamp: 14:09 (when Kafka received it \u2014 7min late)\n#   The 14:00-14:05 window is already closed! What do you do?\n\n# Solutions:\n#   Watermark: allow data up to N minutes late, then close the window\n#   Reprocess: store events in a buffer period, re-aggregate after watermark\n#   Lambda architecture: batch layer handles late data correctness,\n#   speed layer handles real-time approximate results</pre><h4>FAANG Interview: Design a Real-Time Analytics Pipeline</h4><p>Question: \"Design a system to display real-time metrics for a ride-sharing app: active riders, active drivers, rides per minute by city, surge pricing signal.\"</p><pre>Sources: mobile apps emit GPS pings, ride state changes \u2192 Kafka topics\n\nKafka topics:\n  - driver_pings (partitioned by driver_id) \u2192 100K msgs/sec\n  - ride_events  (partitioned by city)      \u2192  10K msgs/sec\n\nStream processing (Flink):\n  - 1-minute tumbling window on ride_events \u2192 count rides per city\n  - 5-minute window on driver_pings \u2192 count active drivers per city\n  - Session window on driver_pings \u2192 detect driver going offline\n\nOutput sinks:\n  - Redis (for dashboard): SET city:rides:nyc 125   (1-min rolling count)\n  - Kafka topic \"metrics\" \u2192 dashboard WebSocket feed\n  - S3 Parquet \u2192 hourly batch for BI reporting\n\nSurge pricing signal:\n  - rides_requested / active_drivers > 1.5 in last 5 minutes \u2192 surge!</pre></div></div></div>",
        "KeyConcepts": [
            "Kafka = distributed append-only commit log. Producers write, consumers read independently.",
            "Topic: named channel. Partition: ordered immutable log within a topic. Offset: position in partition.",
            "Consumer group: each partition served by exactly one consumer in the group. Max parallelism = partition count.",
            "Partition key routing: all events with the same key always go to the same partition (and same consumer).",
            "Messages NOT deleted after consumption \u2014 retained for configurable period. Multiple groups read independently.",
            "At-most-once: may lose. At-least-once: may duplicate (most common). Exactly-once: hardest, needed for finance.",
            "Consumer must be idempotent for at-least-once: INSERT ON CONFLICT DO NOTHING, or overwrite partition.",
            "Kafka vs queue: Kafka retains messages for replay. Queue deletes after consumption. Kafka supports many consumers.",
            "Windowing: tumbling (non-overlapping), sliding (overlapping), session (gap-based). Late data needs watermarks."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Design a Kafka topic layout for an e-commerce order pipeline: what topics? How many partitions each? What partition key?",
            "<strong>Step 2:</strong> Write an idempotent consumer for an orders topic: receive order events, INSERT into PostgreSQL with ON CONFLICT DO NOTHING.",
            "<strong>Step 3:</strong> Design: multiple consumers reading the same 'orders' topic independently \u2014 analytics, fraud detection, inventory update. How many consumer groups?",
            "<strong>Step 4:</strong> Model a late-arriving event problem: events arrive 5 minutes late. You have a 1-minute tumbling window for revenue. How do you handle late data?"
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Draw Tumbling vs Sliding Window",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Draw Tumbling vs Sliding Window \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: 'guarantee each message processed exactly once?' \u2192 explain at-least-once + idempotent consumer.",
                "Partition count limits parallelism: 4 partitions \u2192 max 4 consumers per group. Adding a 5th consumer idles.",
                "Use partition key = user_id when ordering per-user matters (e.g., user's event stream must be ordered).",
                "Kafka is NOT a database. Don't use it as a primary store. Consumers should always write to a real store."
            ]
        },
        "HardProblem": "Boss Problem (LinkedIn): Design Kafka's original use case \u2014 the activity feed pipeline. Every LinkedIn user action (post, like, comment, view) is an event streamed to Kafka. (1) What topics and partitions? (2) 3 consumers need the same data: feed renderer, analytics, notifications \u2014 how many consumer groups? (3) A user with 50M followers posts \u2014 fan-out-on-write creates 50M messages. How does Kafka handle this? (4) You need to replay the last 7 days of events to rebuild the recommendation model. Are 7-day retention and 50M events/day feasible on Kafka?"
    },
    {
        "Week": 11,
        "Day": "Saturday",
        "Phase": "3. Architect (System)",
        "Theme": "Streaming",
        "SpecificTopic": "Deep Dive",
        "ActionItem_Deliverable": "Watch Kafka at LinkedIn Tech Talk",
        "LeetCodeProblem": "<strong>LC 895 \u2013 Maximum Frequency Stack</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Mock Assessment</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Why Kafka Exists \u2014 Core Concepts</div><div class=\"rich\"><h4>Why Kafka Exists \u2014 The Message Bus Problem</h4><p>In 2010, LinkedIn had a problem: dozens of services needed to share data in real time. Payment service needed transaction events. Recommendation engine needed click events. Analytics needed all of it. Point-to-point connections between all services would require N\u00b2 connections and N\u00b2 custom APIs. Every new consumer meant updating every producer.</p><p>LinkedIn built <strong>Apache Kafka</strong> as a distributed, append-only commit log that acts as a central data bus. Producers write events to Kafka topics. Any number of consumers read those events independently, at their own pace, without affecting other consumers or the producers.</p><h4>Core Concepts</h4><table><tr><th>Concept</th><td>What it is</td><td>Analogy</td></tr><tr><th>Topic</th><td>Named channel of events</td><td>A TV channel \u2014 broadcast to all who tune in</td></tr><tr><th>Partition</th><td>Ordered, immutable log within a topic</td><td>A video tape \u2014 you can only append, reads have an offset</td></tr><tr><th>Offset</th><td>Position of a message within a partition</td><td>Page number in a book \u2014 resume from where you left off</td></tr><tr><th>Producer</th><td>Writes messages to a topic</td><td>A journalist who publishes articles</td></tr><tr><th>Consumer</th><td>Reads messages from a topic</td><td>A reader who subscribes to the newspaper</td></tr><tr><th>Consumer Group</th><td>Set of consumers sharing work</td><td>A team reading different sections of the newspaper</td></tr><tr><th>Broker</th><td>A Kafka server storing partitions</td><td>A post office branch</td></tr></table><p>\u270d\ufe0f <strong>Key insight:</strong> Messages in Kafka are NOT deleted when consumed. They are retained for a configurable period (typically 7 days). Multiple consumers read the same message independently \u2014 each gets their own offset pointer.</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Partitions + Consumer Groups</div><div class=\"rich\"><h4>Partitions \u2014 The Key to Kafka's Scalability</h4><p>A topic is split into partitions. Each partition is an ordered, immutable log stored on one broker. Producers can write to any partition (using a key for deterministic routing). Each consumer in a group is assigned one or more partitions exclusively.</p><p><strong>The fundamental rule:</strong> one partition is consumed by exactly one consumer in a group at any time. This is how Kafka provides ordering guarantees and prevents duplicate processing within a group.</p><pre>Topic \"user_events\" with 4 partitions, Consumer Group A with 3 consumers:\n\nPartition 0 \u2192 Consumer 1 (assigned exclusively)\nPartition 1 \u2192 Consumer 1 (one consumer can have multiple partitions)\nPartition 2 \u2192 Consumer 2\nPartition 3 \u2192 Consumer 3\n\nIf Consumer 1 crashes:\n  Kafka rebalances \u2192 Consumer 2 picks up Partition 0 and 1\n\n# Scaling rule: max parallelism = number of partitions\n# 4 partitions \u2192 max 4 consumers in a group can work in parallel\n# Adding a 5th consumer \u2192 it sits idle (no partition to consume)\n\n# Partition key routing:\n# producer.send(\"user_events\", key=\"user_42\", value=event)\n# \u2192 All events for user_42 always go to the SAME partition\n# \u2192 All events for user_42 are processed in ORDER by the same consumer</pre><h4>Consumer Group \u2014 Independent Consumers Read the Same Stream</h4><pre>Topic \"orders\":\n  Group A (analytics): reads orders \u2192 writes to DW \u2192 at offset 50,000\n  Group B (fraud):     reads orders \u2192 checks for fraud \u2192 at offset 49,500\n  Group C (email):     reads orders \u2192 sends shipping emails \u2192 at offset 51,000\n\n# Each group has its own independent offset tracking\n# Producer's throughput is unaffected by how fast consumers read\n# If Group B (fraud) is slow, it does not slow down Group A or C</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Delivery Semantics + Kafka vs Queue</div><div class=\"rich\"><h4>Delivery Semantics \u2014 At-Most-Once, At-Least-Once, Exactly-Once</h4><p>The most common Kafka interview question: what delivery guarantee does Kafka provide?</p><table><tr><th>Semantic</th><td>Messages delivered</td><td>Duplicates?</td><td>Data loss?</td><td>When to use</td></tr><tr><th>At-most-once</th><td>Maybe delivered</td><td>Never</td><td>Possible</td><td>Metrics sampling, non-critical logs</td></tr><tr><th>At-least-once</th><td>Guaranteed delivered</td><td>Possible</td><td>Never</td><td>Most common: process with idempotent consumer</td></tr><tr><th>Exactly-once</th><td>Exactly once end-to-end</td><td>Never</td><td>Never</td><td>Financial transactions, audit logs</td></tr></table><p><strong>How at-least-once works:</strong> Consumer reads message \u2192 processes it \u2192 commits offset. If the consumer crashes AFTER processing but BEFORE committing, the next consumer starts from the last committed offset and re-processes the same message. This is why consuming applications must be idempotent \u2014 processing the same message twice produces the correct result.</p><pre># Idempotent consumer pattern: INSERT ... ON CONFLICT DO NOTHING\nINSERT INTO processed_orders (order_id, status, updated_at)\nVALUES (%(order_id)s, %(status)s, %(ts)s)\nON CONFLICT (order_id) DO UPDATE\n  SET status = EXCLUDED.status,\n      updated_at = EXCLUDED.updated_at\n  WHERE orders.updated_at < EXCLUDED.updated_at;\n-- Running this query twice with the same data = same result (idempotent)</pre><h4>Kafka vs Message Queue \u2014 A Critical Distinction</h4><table><tr><th>Feature</th><td>Kafka (Log)</td><td>RabbitMQ/SQS (Queue)</td></tr><tr><th>Message retention</th><td>Retained for days (configurable)</td><td>Deleted after consumption</td></tr><tr><th>Multiple consumers</th><td>Yes \u2014 each group reads independently</td><td>No \u2014 one consumer gets each message</td></tr><tr><th>Replay</th><td>Yes \u2014 seek to any offset</td><td>No \u2014 once consumed, gone</td></tr><tr><th>Ordering</th><td>Guaranteed within partition</td><td>Generally no guarantee</td></tr><tr><th>Throughput</th><td>Millions/sec (sequential disk)</td><td>Thousands/sec</td></tr><tr><th>Best for</th><td>Event streaming, audit log, multiple consumers</td><td>Task queues, job distribution</td></tr></table></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Windowing + Real-Time Analytics Design</div><div class=\"rich\"><h4>Windowing \u2014 Aggregating Over Time in a Stream</h4><p>Streams are infinite, but analytics need finite windows: \"revenue per hour,\" \"error rate per 5-minute window,\" \"top 10 pages per day.\"</p><pre># Three types of windows in stream processing (Flink/Kafka Streams/Spark Structured Streaming):\n\n# 1. Tumbling Window: fixed, non-overlapping\n#    |---1min---|---1min---|---1min---|\n#    Each event belongs to exactly one window.\n\n# 2. Sliding Window: fixed size, overlapping (stride < window size)\n#    |---10min---|\n#        |---10min---|\n#            |---10min---|\n#    Each event appears in multiple windows.\n\n# 3. Session Window: dynamic, gap-based\n#    A session starts on first event, ends after N seconds of inactivity.\n#    Common for: user session analytics, page view sessions.\n\n# Late data problem:\n#   Event timestamp: 14:02 (when it happened on the device)\n#   Processing timestamp: 14:09 (when Kafka received it \u2014 7min late)\n#   The 14:00-14:05 window is already closed! What do you do?\n\n# Solutions:\n#   Watermark: allow data up to N minutes late, then close the window\n#   Reprocess: store events in a buffer period, re-aggregate after watermark\n#   Lambda architecture: batch layer handles late data correctness,\n#   speed layer handles real-time approximate results</pre><h4>FAANG Interview: Design a Real-Time Analytics Pipeline</h4><p>Question: \"Design a system to display real-time metrics for a ride-sharing app: active riders, active drivers, rides per minute by city, surge pricing signal.\"</p><pre>Sources: mobile apps emit GPS pings, ride state changes \u2192 Kafka topics\n\nKafka topics:\n  - driver_pings (partitioned by driver_id) \u2192 100K msgs/sec\n  - ride_events  (partitioned by city)      \u2192  10K msgs/sec\n\nStream processing (Flink):\n  - 1-minute tumbling window on ride_events \u2192 count rides per city\n  - 5-minute window on driver_pings \u2192 count active drivers per city\n  - Session window on driver_pings \u2192 detect driver going offline\n\nOutput sinks:\n  - Redis (for dashboard): SET city:rides:nyc 125   (1-min rolling count)\n  - Kafka topic \"metrics\" \u2192 dashboard WebSocket feed\n  - S3 Parquet \u2192 hourly batch for BI reporting\n\nSurge pricing signal:\n  - rides_requested / active_drivers > 1.5 in last 5 minutes \u2192 surge!</pre></div></div></div>",
        "KeyConcepts": [
            "Kafka = distributed append-only commit log. Producers write, consumers read independently.",
            "Topic: named channel. Partition: ordered immutable log within a topic. Offset: position in partition.",
            "Consumer group: each partition served by exactly one consumer in the group. Max parallelism = partition count.",
            "Partition key routing: all events with the same key always go to the same partition (and same consumer).",
            "Messages NOT deleted after consumption \u2014 retained for configurable period. Multiple groups read independently.",
            "At-most-once: may lose. At-least-once: may duplicate (most common). Exactly-once: hardest, needed for finance.",
            "Consumer must be idempotent for at-least-once: INSERT ON CONFLICT DO NOTHING, or overwrite partition.",
            "Kafka vs queue: Kafka retains messages for replay. Queue deletes after consumption. Kafka supports many consumers.",
            "Windowing: tumbling (non-overlapping), sliding (overlapping), session (gap-based). Late data needs watermarks."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Design a Kafka topic layout for an e-commerce order pipeline: what topics? How many partitions each? What partition key?",
            "<strong>Step 2:</strong> Write an idempotent consumer for an orders topic: receive order events, INSERT into PostgreSQL with ON CONFLICT DO NOTHING.",
            "<strong>Step 3:</strong> Design: multiple consumers reading the same 'orders' topic independently \u2014 analytics, fraud detection, inventory update. How many consumer groups?",
            "<strong>Step 4:</strong> Model a late-arriving event problem: events arrive 5 minutes late. You have a 1-minute tumbling window for revenue. How do you handle late data?"
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Watch Kafka at LinkedIn Tech Talk",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Watch Kafka at LinkedIn Tech Talk \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: 'guarantee each message processed exactly once?' \u2192 explain at-least-once + idempotent consumer.",
                "Partition count limits parallelism: 4 partitions \u2192 max 4 consumers per group. Adding a 5th consumer idles.",
                "Use partition key = user_id when ordering per-user matters (e.g., user's event stream must be ordered).",
                "Kafka is NOT a database. Don't use it as a primary store. Consumers should always write to a real store."
            ]
        },
        "HardProblem": "Boss Problem (LinkedIn): Design Kafka's original use case \u2014 the activity feed pipeline. Every LinkedIn user action (post, like, comment, view) is an event streamed to Kafka. (1) What topics and partitions? (2) 3 consumers need the same data: feed renderer, analytics, notifications \u2014 how many consumer groups? (3) A user with 50M followers posts \u2014 fan-out-on-write creates 50M messages. How does Kafka handle this? (4) You need to replay the last 7 days of events to rebuild the recommendation model. Are 7-day retention and 50M events/day feasible on Kafka?"
    },
    {
        "Week": 11,
        "Day": "Sunday",
        "Phase": "3. Architect (System)",
        "Theme": "Rest",
        "SpecificTopic": "Rest & Review",
        "ActionItem_Deliverable": "Review Streaming Concepts",
        "LeetCodeProblem": null,
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>SCD Types (1, 2, 3)</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Active Rest \u2014 How Memory Consolidates</div>\n<div class=\"rich\">\n<h4>The Neuroscience of Spaced Repetition</h4>\n<p>Rest days are not wasted days. During rest, your brain replays what it learned \u2014 a process called <strong>memory consolidation</strong>. Research consistently shows that <em>attempting to recall from memory</em> is 3\u00d7 more effective at building durable knowledge than re-reading notes. The act of struggling to remember \u2014 even failing \u2014 strengthens the neural pathway more than passive review.</p>\n<p>Think of it like muscle recovery: a weightlifter who trains 7 days straight without rest gets weaker, not stronger. The growth happens during recovery. Your brain is the same.</p>\n<h4>Today's Protocol (4 hours, structured)</h4>\n<ol>\n  <li><strong>Active Recall (60 min):</strong> Close everything. On a blank page, write every concept, formula, and pattern from this week from memory. Then check.</li>\n  <li><strong>Error Review (60 min):</strong> Find the hardest problem you struggled with. Attempt it again completely from scratch \u2014 no notes.</li>\n  <li><strong>Teach It (45 min):</strong> Open the AI chatbot and explain Gaps & Islands to it as if it's a junior engineer. If you stumble, that's your gap.</li>\n  <li><strong>Next Week Preview (15 min):</strong> Skim next Monday's topic at headline level only \u2014 priming improves retention when you study it deeply.</li>\n</ol>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Active Recall Quiz \u2014 No Notes Allowed</div>\n<div class=\"rich\">\n<h4>Answer These From Memory. Then Check.</h4>\n<pre>Window Functions:\n\u25a1 What is the difference between ROW_NUMBER, RANK, and DENSE_RANK?\n  (Write the output for a table with two tied rows)\n\u25a1 Write the syntax for a 7-day rolling average from memory\n\u25a1 What does PARTITION BY do differently from GROUP BY?\n\u25a1 Write the complete Gaps & Islands CTE from memory\n\u25a1 What is the difference between ROWS BETWEEN and RANGE BETWEEN?\n\u25a1 LAG(col, 1, 0) vs LAG(col, 1) \u2014 what is different?\n\u25a1 In what SQL execution order step do window functions run?\n\u25a1 Why can't you write WHERE rank &lt;= 3 in the same query as ROW_NUMBER?\n\nScore yourself:\n8/8 = \ud83c\udfc6 Mastery level \u2014 you're ready for interviews on this\n5-7/8 = \ud83d\udfe1 Good \u2014 review the 1-2 concepts you missed\n&lt; 5/8 = \ud83d\udd34 Go back to the specific day's content, re-read Level 1-2</pre>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Spaced Repetition Schedule</div>\n<div class=\"rich\">\n<h4>When to Review This Week's Material</h4>\n<table>\n<tr><th>Review this content</th><th>When</th><th>How</th></tr>\n<tr><td>This week</td><td>Today</td><td>Active recall quiz above</td></tr>\n<tr><td>This week</td><td>In 3 days</td><td>Re-solve the hard problem from Day 4</td></tr>\n<tr><td>This week</td><td>In 7 days</td><td>LeetCode: solve problems 185, 180, 196</td></tr>\n<tr><td>This week</td><td>In 30 days</td><td>Explain all 4 concepts to someone else</td></tr>\n</table>\n<h4>The Feynman Technique</h4>\n<p>After studying any concept, close your notes and explain it as if you're teaching a newcomer. Use only simple words. Avoid jargon. If you stumble, that's exactly where your understanding breaks down \u2014 return to just that part. This technique built Richard Feynman's legendary ability to explain complex physics simply.</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Interview Readiness Self-Assessment</div>\n<div class=\"rich\">\n<pre>Rate yourself 1\u20135 on each topic (honest self-assessment):\n  1 = \"I need to re-read from scratch\"\n  3 = \"I understand but need to look at notes\"\n  5 = \"I can code this under pressure from memory\"\n\nROW_NUMBER / RANK / DENSE_RANK:        ___/5\nRolling windows (ROWS BETWEEN):        ___/5\nLAG/LEAD (adjacent row comparison):    ___/5\nGaps & Islands (date streaks):         ___/5\nSession detection (timestamp version): ___/5\n\nAction:\n  Score &lt;3: revisit that day's Level 1 and Level 2 content tomorrow\n  Score 3:  do one LeetCode problem on that specific topic\n  Score 5:  you're interview-ready \u2705 on this topic</pre>\n<h4>FAANG Interview Reality Check</h4>\n<p>At meta, Amazon, and Google data engineering interviews, you'll be asked to write SQL live on a shared screen without autocomplete or documentation. The most common reason candidates fail: they know the concept but freeze on the syntax under pressure. The fix: write every pattern from memory at least 5 times this week. Muscle memory is more reliable than conscious recall under stress.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Memory consolidation happens during rest \u2014 the brain replays and stabilizes learned patterns during downtime.",
            "Active recall (testing yourself) builds 3x stronger memory than re-reading notes.",
            "Spaced repetition: review at day 1, day 3, day 7, day 30 for maximum retention.",
            "The Feynman Technique: if you can't explain it simply, you don't fully understand it yet.",
            "Error analysis: the problems you got WRONG are your highest-value review items \u2014 spend the most time there."
        ],
        "Tasks": [
            "<strong>Active Recall:</strong> Write the Gaps & Islands CTE from memory on paper. Check it against Day 4.",
            "<strong>Re-attempt:</strong> Pick the hardest problem from this week and solve it in 20 minutes from scratch.",
            "<strong>Teach it:</strong> Explain RANK vs DENSE_RANK to the AI chatbot as if it's a junior. Get it right without notes.",
            "<strong>Preview:</strong> Skim next week's Monday topic for 10 minutes \u2014 just headline concepts."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Review Streaming Concepts",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Review Streaming Concepts \u2014 trace through 2-3 edge cases before writing any code.",
                "Use the AI chatbot to quiz yourself \u2014 ask it 'Give me a SQL window function interview question.'",
                "Write SQL from memory in a plain text editor \u2014 no autocomplete. This builds true recall.",
                "The hardest concept this week: spend 30 minutes writing a full explanation in your own words."
            ]
        },
        "HardProblem": "Connect-the-dots: Write 2-3 paragraphs explaining how ROW_NUMBER, rolling averages, LAG, and Gaps & Islands could ALL be used in a single production pipeline for a ride-sharing company. What problem does each solve? How do they complement each other? What would break if you removed any one of them?"
    },
    {
        "Week": 12,
        "Day": "Monday",
        "Phase": "3. Architect (System)",
        "Theme": "System Design",
        "SpecificTopic": "Back-of-Envelope Math",
        "ActionItem_Deliverable": "Memorize Bytes/Secs in Day / Requests per Sec",
        "LeetCodeProblem": "<strong>LC 155 \u2013 Min Stack</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Mock Design Round</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 DE System Design + Back-of-Envelope Math</div><div class=\"rich\"><h4>Data Engineering System Design \u2014 What's Different from Software Engineering</h4><p>Software engineering system design interviews focus on: web services, APIs, databases, caching, load balancers. Data engineering system design adds: data volume math, pipeline latency, storage formats, streaming vs batch trade-offs, failure modes at scale, and cost optimization.</p><h4>Back-of-Envelope Math \u2014 Essential for DE Interviews</h4><p>FAANG interviewers expect you to quickly estimate: how much storage do we need? How long will it take to process? What network bandwidth is required? This is called \"back-of-envelope\" calculation. Here are the key numbers to memorize:</p><pre>Useful numbers:\n  1 byte       = 1 character\n  1 KB         = 1,000 bytes\n  1 MB         = 1,000 KB\n  1 GB         = 1,000 MB\n  1 TB         = 1,000 GB\n  1 PB         = 1,000 TB\n\n  SSD read     = 500 MB/sec\n  HDD read     = 100 MB/sec\n  Network LAN  = 1 GB/sec\n  S3/cloud     = 100-500 MB/sec (per file, in parallel: much more)\n\n  1 million events/day \u00d7 100 bytes/event = 100 MB/day\n  1 billion events/day \u00d7 100 bytes/event = 100 GB/day\n  = 3 TB/month \u2248 36 TB/year (uncompressed)\n  Parquet at 5x compression = 7 TB/year \u2014 easily fits in S3\n\n  100 million users \u00d7 1 KB profile = 100 GB \u2014 fits in memory on a large machine!</pre><h4>Batch ETL vs Streaming \u2014 The Core Trade-Off</h4><table><tr><th>Dimension</th><td>Batch ETL</td><td>Streaming</td></tr><tr><th>Data freshness</th><td>Hours to daily</td><td>Seconds to minutes</td></tr><tr><th>Complexity</th><td>Lower (simpler tooling)</td><td>Higher (state, ordering, fault tolerance)</td></tr><tr><th>Cost</th><td>Lower (bursty compute)</td><td>Higher (always-running compute)</td></tr><tr><th>Failure recovery</th><td>Simple: re-run the batch</td><td>Complex: replay from Kafka offset</td></tr><tr><th>Best for</th><td>Daily reports, historical analysis</td><td>Fraud detection, live dashboards, alerting</td></tr><tr><th>Tooling</th><td>Spark, dbt, Airflow</td><td>Kafka, Flink, Spark Structured Streaming</td></tr></table></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Batch ETL Architecture + CDC</div><div class=\"rich\"><h4>Pattern: Designing a Batch ETL System</h4><p>Interview: \"Design a system to compute daily sales metrics for a 50M-user e-commerce platform.\"</p><pre>=== Requirements ===\nFunctional:\n  - Daily revenue by product category and region\n  - Top 100 products by revenue per day\n  - Customer lifetime value (CLV) updated daily\n\nNon-functional:\n  - Pipeline must complete by 6am (SLA: 3 hours after midnight)\n  - Data must be queryable by the BI team by 7am\n  - Failures must auto-retry, team alerted if SLA missed\n\n=== Volume math ===\n  50M users \u00d7 avg 1 order/day = 50M orders/day\n  50M orders \u00d7 3 items avg   = 150M order items/day\n  150M rows \u00d7 200 bytes      = 30GB/day uncompressed\n  Parquet compression (5x)   = 6GB/day to write\n  Running 1 year             = 2.2TB stored \u2014 very manageable\n\n=== Architecture ===\n  Source: OLTP PostgreSQL \u2192 CDC (Debezium) \u2192 Kafka topics\n  Landing zone: Kafka \u2192 raw Parquet on S3 (append every 5 min)\n  Daily batch: Airflow DAG at 00:30 UTC\n    Task 1: Spark reads raw S3 Parquet, computes dim + fact tables\n    Task 2: dbt runs transformation models + tests\n    Task 3: Snowflake/BigQuery tables updated for BI queries\n    Task 4: SLA check \u2014 alert if not done by 03:00 UTC</pre><h4>CDC \u2014 Change Data Capture</h4><p><strong>CDC (Change Data Capture)</strong> captures database changes (INSERT/UPDATE/DELETE) in real time from the database's transaction log, without polling. Debezium is the open-source tool that reads PostgreSQL/MySQL WAL logs and publishes changes as Kafka messages.</p><pre># Why CDC instead of \"SELECT * WHERE updated_at > last_run_time\"?\n\n# Problem with polling:\n  1. Misses DELETEs (deleted rows cannot be queried)\n  2. Missing updated_at on some tables\n  3. High load on source DB (full table scans)\n  4. Possible race condition: row updated between runs\n\n# CDC reads from the WAL (Write-Ahead Log) directly:\n  - Captures every INSERT/UPDATE/DELETE as it happens\n  - Zero load on source tables\n  - No missed changes\n  - Before/after images of every row change\n\n# Debezium Kafka message format:\n{\n  \"op\": \"u\",         # u=update, c=create, d=delete\n  \"before\": {\"user_id\": 42, \"tier\": \"silver\"},\n  \"after\":  {\"user_id\": 42, \"tier\": \"gold\"},\n  \"ts_ms\": 1704067200000\n}</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Lambda vs Kappa Architecture</div><div class=\"rich\"><h4>Pattern: Lambda Architecture + Lambda vs Kappa</h4><p><strong>Lambda Architecture:</strong> run two parallel systems \u2014 a batch layer for accuracy (recomputes everything correctly, but slowly) and a speed layer for freshness (approximate real-time results using streaming). Serving layer merges both.</p><pre>Lambda Architecture:\n\nRaw data \u2500\u2500\u252c\u2500\u2500 Batch Layer (Spark, daily job) \u2500\u2500\u2500\u2500\u2500\u2500\u25ba Batch views (exact)\n           \u2502                                                  \u2502\n           \u2514\u2500\u2500 Speed Layer (Kafka+Flink, real-time) \u2500\u2500\u25ba Real-time views \u2500\u2500\u25ba Serving Layer \u2500\u2500\u25ba App\n\nPros: batch layer corrects errors in real-time layer\nCons: two codebases to maintain, possible inconsistencies between layers\n\nKappa Architecture:\n  Only a speed layer. Use streaming for EVERYTHING, including historical reprocessing.\n  If you need to reprocess, replay from Kafka (if retention long enough) or S3.\n  Pros: one codebase. Cons: streaming code is harder, Kafka storage costs.\n\nTrend at FAANG: Kappa is winning for new systems.\nLambda still common in legacy systems built 2015-2020.</pre><h4>Trade-Offs Matrix \u2014 For System Design Interviews</h4><table><tr><th>Requirement</th><td>Batch</td><td>Micro-batch (Spark SS)</td><td>Streaming (Flink/Kafka)</td></tr><tr><th>Data freshness needed</th><td>Hours/daily</td><td>5-30 min</td><td><1 min</td></tr><tr><th>Historical reprocessing</th><td>Easy: re-run job</td><td>Moderate</td><td>Complex: replay from log</td></tr><tr><th>Exactly-once guarantee</th><td>Easy: idempotent writes</td><td>Possible with checkpointing</td><td>Hard: requires distributed transactions</td></tr><tr><th>Dev complexity</th><td>Low</td><td>Medium</td><td>High</td></tr><tr><th>Infrastructure cost</th><td>Low (bursty)</td><td>Medium (semi-continuous)</td><td>High (always-on)</td></tr></table></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Real-Time Fraud Detection Design</div><div class=\"rich\"><h4>Complete System Design: Real-Time Fraud Detection</h4><pre>Interview: \"Design a real-time fraud detection system for a payments company.\"\n\n=== Functional Requirements ===\n  - Flag suspicious transactions within 500ms of payment attempt\n  - 10,000 transactions/second peak\n  - Fraud signal based on: velocity (too many txns), location change,\n    unusual amount, high-risk merchant category\n\n=== Architecture ===\n  Step 1: Ingestion\n    Payment service \u2192 Kafka topic \"transactions\" (partitioned by user_id)\n    Partitioning by user_id ensures all events for one user \u2192 same consumer\n\n  Step 2: Feature computation (Flink, <100ms)\n    - 5-min tumbling window: count txns per user_id \u2192 velocity feature\n    - 1-hour rolling window: SUM(amount) per user \u2192 amount feature\n    - Point lookup: last known location from Redis \u2192 location change\n\n  Step 3: Fraud scoring\n    - ML model (loaded in Flink job): score transaction given features\n    - Or: rule engine: IF velocity > 10 AND amount > $1000 \u2192 high risk\n\n  Step 4: Decision output\n    - Low risk: allow \u2192 write event to \"fraud_scores\" Kafka topic\n    - High risk: block \u2192 write to \"blocked_txns\" + alert fraud team\n\n  Step 5: Storage\n    - All scored transactions \u2192 Kafka \u2192 S3 Parquet (audit log)\n    - Daily batch: Spark recomputes features with full history \u2192 retrain ML model\n\n=== Key design decisions to state ===\n  1. Why Kafka? \u2192 decouples payment service from fraud service\n  2. Why Flink? \u2192 stateful streaming, low latency, exactly-once\n  3. Why Redis? \u2192 O(1) user state lookups (cannot query DW in 500ms)\n  4. Why batch layer too? \u2192 retrain ML model with corrected labels weekly</pre></div></div></div>",
        "KeyConcepts": [
            "DE system design adds: volume math, latency requirements, storage formats, streaming vs batch trade-offs.",
            "Back-of-envelope: 1B events/day \u00d7 100 bytes = 100GB/day. With 5x Parquet compression = 20GB/day.",
            "Batch: simple, cheaper, hours of latency. Streaming: complex, expensive, seconds of latency.",
            "CDC: reads database transaction log (WAL). Captures INSERT/UPDATE/DELETE including hard DELETES. No source load.",
            "Lambda: batch (accurate) + streaming (fast) layers. Complex: two codebases. Used in legacy systems.",
            "Kappa: streaming only. Simpler codebase. Historical reprocessing via log replay. Winning in new systems.",
            "Fraud detection: Kafka \u2192 Flink (stateful windows) \u2192 ML scoring \u2192 Redis (block/allow) \u2192 result topic.",
            "Always justify architecture choices: 'I chose Kafka because...', 'I chose Flink over Spark SS because...'"
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Back-of-envelope: 100M daily active users, each viewing 20 pages/day, each page view is 500 bytes. Calculate: daily data volume (uncompressed + Parquet compressed), monthly storage cost at $0.023/GB-month on S3.",
            "<strong>Step 2:</strong> Design a batch ETL system for a SaaS company: daily computation of MRR (monthly recurring revenue) by plan tier and country. Draw the pipeline from OLTP to BI-ready table.",
            "<strong>Step 3:</strong> Explain CDC to a non-technical stakeholder in 3 sentences. Then explain to a technical interviewer why it is better than polling.",
            "<strong>Step 4:</strong> Mock interview: 'Design a real-time recommendation system for Netflix (movies to show on homepage, updated every 5 minutes based on viewing history).' Use the structured framework: requirements \u2192 volume math \u2192 architecture \u2192 trade-offs."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Memorize Bytes/Secs in Day / Requests per Sec",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Memorize Bytes/Secs in Day / Requests per Sec \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: always start with volume math before architecture. Shows structured engineering thinking.",
                "CDC vs polling: CDC catches DELETEs (polling cannot). CDC has zero source DB load. CDC is always preferred.",
                "Lambda vs Kappa: Lambda = two codebases (batch + streaming). Kappa = one codebase (streaming only). New systems = Kappa.",
                "Fraud detection latency budget: 500ms total = 50ms Kafka \u2192 200ms Flink \u2192 100ms Redis lookup \u2192 150ms ML scoring."
            ]
        },
        "HardProblem": "Boss Problem (Stripe): Design a complete data platform for Stripe's analytics needs. Requirements: (1) 100M transactions/day, real-time fraud detection within 200ms; (2) Daily financial reports (revenue by country, product, merchant type) available by 6am UTC; (3) Ad-hoc SQL queries by 200 data analysts on 5 years of historical data; (4) GDPR: delete all data for a user within 30 days of request; (5) Data catalog: any analyst can discover what tables exist and their meaning. Design: the complete architecture (streaming + batch + storage + catalog + governance), justify every component choice, and explain the failure modes."
    },
    {
        "Week": 12,
        "Day": "Tuesday",
        "Phase": "3. Architect (System)",
        "Theme": "System Design",
        "SpecificTopic": "Pattern: Batch ETL",
        "ActionItem_Deliverable": "Draw S3 -> Spark -> Warehouse Architecture",
        "LeetCodeProblem": "<strong>LC 232 \u2013 Implement Queue Using Stacks</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Mock Assessment + Mock #1</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 DE System Design + Back-of-Envelope Math</div><div class=\"rich\"><h4>Data Engineering System Design \u2014 What's Different from Software Engineering</h4><p>Software engineering system design interviews focus on: web services, APIs, databases, caching, load balancers. Data engineering system design adds: data volume math, pipeline latency, storage formats, streaming vs batch trade-offs, failure modes at scale, and cost optimization.</p><h4>Back-of-Envelope Math \u2014 Essential for DE Interviews</h4><p>FAANG interviewers expect you to quickly estimate: how much storage do we need? How long will it take to process? What network bandwidth is required? This is called \"back-of-envelope\" calculation. Here are the key numbers to memorize:</p><pre>Useful numbers:\n  1 byte       = 1 character\n  1 KB         = 1,000 bytes\n  1 MB         = 1,000 KB\n  1 GB         = 1,000 MB\n  1 TB         = 1,000 GB\n  1 PB         = 1,000 TB\n\n  SSD read     = 500 MB/sec\n  HDD read     = 100 MB/sec\n  Network LAN  = 1 GB/sec\n  S3/cloud     = 100-500 MB/sec (per file, in parallel: much more)\n\n  1 million events/day \u00d7 100 bytes/event = 100 MB/day\n  1 billion events/day \u00d7 100 bytes/event = 100 GB/day\n  = 3 TB/month \u2248 36 TB/year (uncompressed)\n  Parquet at 5x compression = 7 TB/year \u2014 easily fits in S3\n\n  100 million users \u00d7 1 KB profile = 100 GB \u2014 fits in memory on a large machine!</pre><h4>Batch ETL vs Streaming \u2014 The Core Trade-Off</h4><table><tr><th>Dimension</th><td>Batch ETL</td><td>Streaming</td></tr><tr><th>Data freshness</th><td>Hours to daily</td><td>Seconds to minutes</td></tr><tr><th>Complexity</th><td>Lower (simpler tooling)</td><td>Higher (state, ordering, fault tolerance)</td></tr><tr><th>Cost</th><td>Lower (bursty compute)</td><td>Higher (always-running compute)</td></tr><tr><th>Failure recovery</th><td>Simple: re-run the batch</td><td>Complex: replay from Kafka offset</td></tr><tr><th>Best for</th><td>Daily reports, historical analysis</td><td>Fraud detection, live dashboards, alerting</td></tr><tr><th>Tooling</th><td>Spark, dbt, Airflow</td><td>Kafka, Flink, Spark Structured Streaming</td></tr></table></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Batch ETL Architecture + CDC</div><div class=\"rich\"><h4>Pattern: Designing a Batch ETL System</h4><p>Interview: \"Design a system to compute daily sales metrics for a 50M-user e-commerce platform.\"</p><pre>=== Requirements ===\nFunctional:\n  - Daily revenue by product category and region\n  - Top 100 products by revenue per day\n  - Customer lifetime value (CLV) updated daily\n\nNon-functional:\n  - Pipeline must complete by 6am (SLA: 3 hours after midnight)\n  - Data must be queryable by the BI team by 7am\n  - Failures must auto-retry, team alerted if SLA missed\n\n=== Volume math ===\n  50M users \u00d7 avg 1 order/day = 50M orders/day\n  50M orders \u00d7 3 items avg   = 150M order items/day\n  150M rows \u00d7 200 bytes      = 30GB/day uncompressed\n  Parquet compression (5x)   = 6GB/day to write\n  Running 1 year             = 2.2TB stored \u2014 very manageable\n\n=== Architecture ===\n  Source: OLTP PostgreSQL \u2192 CDC (Debezium) \u2192 Kafka topics\n  Landing zone: Kafka \u2192 raw Parquet on S3 (append every 5 min)\n  Daily batch: Airflow DAG at 00:30 UTC\n    Task 1: Spark reads raw S3 Parquet, computes dim + fact tables\n    Task 2: dbt runs transformation models + tests\n    Task 3: Snowflake/BigQuery tables updated for BI queries\n    Task 4: SLA check \u2014 alert if not done by 03:00 UTC</pre><h4>CDC \u2014 Change Data Capture</h4><p><strong>CDC (Change Data Capture)</strong> captures database changes (INSERT/UPDATE/DELETE) in real time from the database's transaction log, without polling. Debezium is the open-source tool that reads PostgreSQL/MySQL WAL logs and publishes changes as Kafka messages.</p><pre># Why CDC instead of \"SELECT * WHERE updated_at > last_run_time\"?\n\n# Problem with polling:\n  1. Misses DELETEs (deleted rows cannot be queried)\n  2. Missing updated_at on some tables\n  3. High load on source DB (full table scans)\n  4. Possible race condition: row updated between runs\n\n# CDC reads from the WAL (Write-Ahead Log) directly:\n  - Captures every INSERT/UPDATE/DELETE as it happens\n  - Zero load on source tables\n  - No missed changes\n  - Before/after images of every row change\n\n# Debezium Kafka message format:\n{\n  \"op\": \"u\",         # u=update, c=create, d=delete\n  \"before\": {\"user_id\": 42, \"tier\": \"silver\"},\n  \"after\":  {\"user_id\": 42, \"tier\": \"gold\"},\n  \"ts_ms\": 1704067200000\n}</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Lambda vs Kappa Architecture</div><div class=\"rich\"><h4>Pattern: Lambda Architecture + Lambda vs Kappa</h4><p><strong>Lambda Architecture:</strong> run two parallel systems \u2014 a batch layer for accuracy (recomputes everything correctly, but slowly) and a speed layer for freshness (approximate real-time results using streaming). Serving layer merges both.</p><pre>Lambda Architecture:\n\nRaw data \u2500\u2500\u252c\u2500\u2500 Batch Layer (Spark, daily job) \u2500\u2500\u2500\u2500\u2500\u2500\u25ba Batch views (exact)\n           \u2502                                                  \u2502\n           \u2514\u2500\u2500 Speed Layer (Kafka+Flink, real-time) \u2500\u2500\u25ba Real-time views \u2500\u2500\u25ba Serving Layer \u2500\u2500\u25ba App\n\nPros: batch layer corrects errors in real-time layer\nCons: two codebases to maintain, possible inconsistencies between layers\n\nKappa Architecture:\n  Only a speed layer. Use streaming for EVERYTHING, including historical reprocessing.\n  If you need to reprocess, replay from Kafka (if retention long enough) or S3.\n  Pros: one codebase. Cons: streaming code is harder, Kafka storage costs.\n\nTrend at FAANG: Kappa is winning for new systems.\nLambda still common in legacy systems built 2015-2020.</pre><h4>Trade-Offs Matrix \u2014 For System Design Interviews</h4><table><tr><th>Requirement</th><td>Batch</td><td>Micro-batch (Spark SS)</td><td>Streaming (Flink/Kafka)</td></tr><tr><th>Data freshness needed</th><td>Hours/daily</td><td>5-30 min</td><td><1 min</td></tr><tr><th>Historical reprocessing</th><td>Easy: re-run job</td><td>Moderate</td><td>Complex: replay from log</td></tr><tr><th>Exactly-once guarantee</th><td>Easy: idempotent writes</td><td>Possible with checkpointing</td><td>Hard: requires distributed transactions</td></tr><tr><th>Dev complexity</th><td>Low</td><td>Medium</td><td>High</td></tr><tr><th>Infrastructure cost</th><td>Low (bursty)</td><td>Medium (semi-continuous)</td><td>High (always-on)</td></tr></table></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Real-Time Fraud Detection Design</div><div class=\"rich\"><h4>Complete System Design: Real-Time Fraud Detection</h4><pre>Interview: \"Design a real-time fraud detection system for a payments company.\"\n\n=== Functional Requirements ===\n  - Flag suspicious transactions within 500ms of payment attempt\n  - 10,000 transactions/second peak\n  - Fraud signal based on: velocity (too many txns), location change,\n    unusual amount, high-risk merchant category\n\n=== Architecture ===\n  Step 1: Ingestion\n    Payment service \u2192 Kafka topic \"transactions\" (partitioned by user_id)\n    Partitioning by user_id ensures all events for one user \u2192 same consumer\n\n  Step 2: Feature computation (Flink, <100ms)\n    - 5-min tumbling window: count txns per user_id \u2192 velocity feature\n    - 1-hour rolling window: SUM(amount) per user \u2192 amount feature\n    - Point lookup: last known location from Redis \u2192 location change\n\n  Step 3: Fraud scoring\n    - ML model (loaded in Flink job): score transaction given features\n    - Or: rule engine: IF velocity > 10 AND amount > $1000 \u2192 high risk\n\n  Step 4: Decision output\n    - Low risk: allow \u2192 write event to \"fraud_scores\" Kafka topic\n    - High risk: block \u2192 write to \"blocked_txns\" + alert fraud team\n\n  Step 5: Storage\n    - All scored transactions \u2192 Kafka \u2192 S3 Parquet (audit log)\n    - Daily batch: Spark recomputes features with full history \u2192 retrain ML model\n\n=== Key design decisions to state ===\n  1. Why Kafka? \u2192 decouples payment service from fraud service\n  2. Why Flink? \u2192 stateful streaming, low latency, exactly-once\n  3. Why Redis? \u2192 O(1) user state lookups (cannot query DW in 500ms)\n  4. Why batch layer too? \u2192 retrain ML model with corrected labels weekly</pre></div></div></div>",
        "KeyConcepts": [
            "DE system design adds: volume math, latency requirements, storage formats, streaming vs batch trade-offs.",
            "Back-of-envelope: 1B events/day \u00d7 100 bytes = 100GB/day. With 5x Parquet compression = 20GB/day.",
            "Batch: simple, cheaper, hours of latency. Streaming: complex, expensive, seconds of latency.",
            "CDC: reads database transaction log (WAL). Captures INSERT/UPDATE/DELETE including hard DELETES. No source load.",
            "Lambda: batch (accurate) + streaming (fast) layers. Complex: two codebases. Used in legacy systems.",
            "Kappa: streaming only. Simpler codebase. Historical reprocessing via log replay. Winning in new systems.",
            "Fraud detection: Kafka \u2192 Flink (stateful windows) \u2192 ML scoring \u2192 Redis (block/allow) \u2192 result topic.",
            "Always justify architecture choices: 'I chose Kafka because...', 'I chose Flink over Spark SS because...'"
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Back-of-envelope: 100M daily active users, each viewing 20 pages/day, each page view is 500 bytes. Calculate: daily data volume (uncompressed + Parquet compressed), monthly storage cost at $0.023/GB-month on S3.",
            "<strong>Step 2:</strong> Design a batch ETL system for a SaaS company: daily computation of MRR (monthly recurring revenue) by plan tier and country. Draw the pipeline from OLTP to BI-ready table.",
            "<strong>Step 3:</strong> Explain CDC to a non-technical stakeholder in 3 sentences. Then explain to a technical interviewer why it is better than polling.",
            "<strong>Step 4:</strong> Mock interview: 'Design a real-time recommendation system for Netflix (movies to show on homepage, updated every 5 minutes based on viewing history).' Use the structured framework: requirements \u2192 volume math \u2192 architecture \u2192 trade-offs."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Draw S3 -> Spark -> Warehouse Architecture",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Draw S3 -> Spark -> Warehouse Architecture \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: always start with volume math before architecture. Shows structured engineering thinking.",
                "CDC vs polling: CDC catches DELETEs (polling cannot). CDC has zero source DB load. CDC is always preferred.",
                "Lambda vs Kappa: Lambda = two codebases (batch + streaming). Kappa = one codebase (streaming only). New systems = Kappa.",
                "Fraud detection latency budget: 500ms total = 50ms Kafka \u2192 200ms Flink \u2192 100ms Redis lookup \u2192 150ms ML scoring."
            ]
        },
        "HardProblem": "Boss Problem (Stripe): Design a complete data platform for Stripe's analytics needs. Requirements: (1) 100M transactions/day, real-time fraud detection within 200ms; (2) Daily financial reports (revenue by country, product, merchant type) available by 6am UTC; (3) Ad-hoc SQL queries by 200 data analysts on 5 years of historical data; (4) GDPR: delete all data for a user within 30 days of request; (5) Data catalog: any analyst can discover what tables exist and their meaning. Design: the complete architecture (streaming + batch + storage + catalog + governance), justify every component choice, and explain the failure modes."
    },
    {
        "Week": 12,
        "Day": "Wednesday",
        "Phase": "3. Architect (System)",
        "Theme": "System Design",
        "SpecificTopic": "Pattern: Streaming",
        "ActionItem_Deliverable": "Draw Kafka -> Flink -> NoSQL Architecture",
        "LeetCodeProblem": "<strong>LC 84 \u2013 Largest Rectangle in Histogram</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Recursive Parsing</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Why Kafka Exists \u2014 Core Concepts</div><div class=\"rich\"><h4>Why Kafka Exists \u2014 The Message Bus Problem</h4><p>In 2010, LinkedIn had a problem: dozens of services needed to share data in real time. Payment service needed transaction events. Recommendation engine needed click events. Analytics needed all of it. Point-to-point connections between all services would require N\u00b2 connections and N\u00b2 custom APIs. Every new consumer meant updating every producer.</p><p>LinkedIn built <strong>Apache Kafka</strong> as a distributed, append-only commit log that acts as a central data bus. Producers write events to Kafka topics. Any number of consumers read those events independently, at their own pace, without affecting other consumers or the producers.</p><h4>Core Concepts</h4><table><tr><th>Concept</th><td>What it is</td><td>Analogy</td></tr><tr><th>Topic</th><td>Named channel of events</td><td>A TV channel \u2014 broadcast to all who tune in</td></tr><tr><th>Partition</th><td>Ordered, immutable log within a topic</td><td>A video tape \u2014 you can only append, reads have an offset</td></tr><tr><th>Offset</th><td>Position of a message within a partition</td><td>Page number in a book \u2014 resume from where you left off</td></tr><tr><th>Producer</th><td>Writes messages to a topic</td><td>A journalist who publishes articles</td></tr><tr><th>Consumer</th><td>Reads messages from a topic</td><td>A reader who subscribes to the newspaper</td></tr><tr><th>Consumer Group</th><td>Set of consumers sharing work</td><td>A team reading different sections of the newspaper</td></tr><tr><th>Broker</th><td>A Kafka server storing partitions</td><td>A post office branch</td></tr></table><p>\u270d\ufe0f <strong>Key insight:</strong> Messages in Kafka are NOT deleted when consumed. They are retained for a configurable period (typically 7 days). Multiple consumers read the same message independently \u2014 each gets their own offset pointer.</p></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Partitions + Consumer Groups</div><div class=\"rich\"><h4>Partitions \u2014 The Key to Kafka's Scalability</h4><p>A topic is split into partitions. Each partition is an ordered, immutable log stored on one broker. Producers can write to any partition (using a key for deterministic routing). Each consumer in a group is assigned one or more partitions exclusively.</p><p><strong>The fundamental rule:</strong> one partition is consumed by exactly one consumer in a group at any time. This is how Kafka provides ordering guarantees and prevents duplicate processing within a group.</p><pre>Topic \"user_events\" with 4 partitions, Consumer Group A with 3 consumers:\n\nPartition 0 \u2192 Consumer 1 (assigned exclusively)\nPartition 1 \u2192 Consumer 1 (one consumer can have multiple partitions)\nPartition 2 \u2192 Consumer 2\nPartition 3 \u2192 Consumer 3\n\nIf Consumer 1 crashes:\n  Kafka rebalances \u2192 Consumer 2 picks up Partition 0 and 1\n\n# Scaling rule: max parallelism = number of partitions\n# 4 partitions \u2192 max 4 consumers in a group can work in parallel\n# Adding a 5th consumer \u2192 it sits idle (no partition to consume)\n\n# Partition key routing:\n# producer.send(\"user_events\", key=\"user_42\", value=event)\n# \u2192 All events for user_42 always go to the SAME partition\n# \u2192 All events for user_42 are processed in ORDER by the same consumer</pre><h4>Consumer Group \u2014 Independent Consumers Read the Same Stream</h4><pre>Topic \"orders\":\n  Group A (analytics): reads orders \u2192 writes to DW \u2192 at offset 50,000\n  Group B (fraud):     reads orders \u2192 checks for fraud \u2192 at offset 49,500\n  Group C (email):     reads orders \u2192 sends shipping emails \u2192 at offset 51,000\n\n# Each group has its own independent offset tracking\n# Producer's throughput is unaffected by how fast consumers read\n# If Group B (fraud) is slow, it does not slow down Group A or C</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Delivery Semantics + Kafka vs Queue</div><div class=\"rich\"><h4>Delivery Semantics \u2014 At-Most-Once, At-Least-Once, Exactly-Once</h4><p>The most common Kafka interview question: what delivery guarantee does Kafka provide?</p><table><tr><th>Semantic</th><td>Messages delivered</td><td>Duplicates?</td><td>Data loss?</td><td>When to use</td></tr><tr><th>At-most-once</th><td>Maybe delivered</td><td>Never</td><td>Possible</td><td>Metrics sampling, non-critical logs</td></tr><tr><th>At-least-once</th><td>Guaranteed delivered</td><td>Possible</td><td>Never</td><td>Most common: process with idempotent consumer</td></tr><tr><th>Exactly-once</th><td>Exactly once end-to-end</td><td>Never</td><td>Never</td><td>Financial transactions, audit logs</td></tr></table><p><strong>How at-least-once works:</strong> Consumer reads message \u2192 processes it \u2192 commits offset. If the consumer crashes AFTER processing but BEFORE committing, the next consumer starts from the last committed offset and re-processes the same message. This is why consuming applications must be idempotent \u2014 processing the same message twice produces the correct result.</p><pre># Idempotent consumer pattern: INSERT ... ON CONFLICT DO NOTHING\nINSERT INTO processed_orders (order_id, status, updated_at)\nVALUES (%(order_id)s, %(status)s, %(ts)s)\nON CONFLICT (order_id) DO UPDATE\n  SET status = EXCLUDED.status,\n      updated_at = EXCLUDED.updated_at\n  WHERE orders.updated_at < EXCLUDED.updated_at;\n-- Running this query twice with the same data = same result (idempotent)</pre><h4>Kafka vs Message Queue \u2014 A Critical Distinction</h4><table><tr><th>Feature</th><td>Kafka (Log)</td><td>RabbitMQ/SQS (Queue)</td></tr><tr><th>Message retention</th><td>Retained for days (configurable)</td><td>Deleted after consumption</td></tr><tr><th>Multiple consumers</th><td>Yes \u2014 each group reads independently</td><td>No \u2014 one consumer gets each message</td></tr><tr><th>Replay</th><td>Yes \u2014 seek to any offset</td><td>No \u2014 once consumed, gone</td></tr><tr><th>Ordering</th><td>Guaranteed within partition</td><td>Generally no guarantee</td></tr><tr><th>Throughput</th><td>Millions/sec (sequential disk)</td><td>Thousands/sec</td></tr><tr><th>Best for</th><td>Event streaming, audit log, multiple consumers</td><td>Task queues, job distribution</td></tr></table></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Windowing + Real-Time Analytics Design</div><div class=\"rich\"><h4>Windowing \u2014 Aggregating Over Time in a Stream</h4><p>Streams are infinite, but analytics need finite windows: \"revenue per hour,\" \"error rate per 5-minute window,\" \"top 10 pages per day.\"</p><pre># Three types of windows in stream processing (Flink/Kafka Streams/Spark Structured Streaming):\n\n# 1. Tumbling Window: fixed, non-overlapping\n#    |---1min---|---1min---|---1min---|\n#    Each event belongs to exactly one window.\n\n# 2. Sliding Window: fixed size, overlapping (stride < window size)\n#    |---10min---|\n#        |---10min---|\n#            |---10min---|\n#    Each event appears in multiple windows.\n\n# 3. Session Window: dynamic, gap-based\n#    A session starts on first event, ends after N seconds of inactivity.\n#    Common for: user session analytics, page view sessions.\n\n# Late data problem:\n#   Event timestamp: 14:02 (when it happened on the device)\n#   Processing timestamp: 14:09 (when Kafka received it \u2014 7min late)\n#   The 14:00-14:05 window is already closed! What do you do?\n\n# Solutions:\n#   Watermark: allow data up to N minutes late, then close the window\n#   Reprocess: store events in a buffer period, re-aggregate after watermark\n#   Lambda architecture: batch layer handles late data correctness,\n#   speed layer handles real-time approximate results</pre><h4>FAANG Interview: Design a Real-Time Analytics Pipeline</h4><p>Question: \"Design a system to display real-time metrics for a ride-sharing app: active riders, active drivers, rides per minute by city, surge pricing signal.\"</p><pre>Sources: mobile apps emit GPS pings, ride state changes \u2192 Kafka topics\n\nKafka topics:\n  - driver_pings (partitioned by driver_id) \u2192 100K msgs/sec\n  - ride_events  (partitioned by city)      \u2192  10K msgs/sec\n\nStream processing (Flink):\n  - 1-minute tumbling window on ride_events \u2192 count rides per city\n  - 5-minute window on driver_pings \u2192 count active drivers per city\n  - Session window on driver_pings \u2192 detect driver going offline\n\nOutput sinks:\n  - Redis (for dashboard): SET city:rides:nyc 125   (1-min rolling count)\n  - Kafka topic \"metrics\" \u2192 dashboard WebSocket feed\n  - S3 Parquet \u2192 hourly batch for BI reporting\n\nSurge pricing signal:\n  - rides_requested / active_drivers > 1.5 in last 5 minutes \u2192 surge!</pre></div></div></div>",
        "KeyConcepts": [
            "Kafka = distributed append-only commit log. Producers write, consumers read independently.",
            "Topic: named channel. Partition: ordered immutable log within a topic. Offset: position in partition.",
            "Consumer group: each partition served by exactly one consumer in the group. Max parallelism = partition count.",
            "Partition key routing: all events with the same key always go to the same partition (and same consumer).",
            "Messages NOT deleted after consumption \u2014 retained for configurable period. Multiple groups read independently.",
            "At-most-once: may lose. At-least-once: may duplicate (most common). Exactly-once: hardest, needed for finance.",
            "Consumer must be idempotent for at-least-once: INSERT ON CONFLICT DO NOTHING, or overwrite partition.",
            "Kafka vs queue: Kafka retains messages for replay. Queue deletes after consumption. Kafka supports many consumers.",
            "Windowing: tumbling (non-overlapping), sliding (overlapping), session (gap-based). Late data needs watermarks."
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Design a Kafka topic layout for an e-commerce order pipeline: what topics? How many partitions each? What partition key?",
            "<strong>Step 2:</strong> Write an idempotent consumer for an orders topic: receive order events, INSERT into PostgreSQL with ON CONFLICT DO NOTHING.",
            "<strong>Step 3:</strong> Design: multiple consumers reading the same 'orders' topic independently \u2014 analytics, fraud detection, inventory update. How many consumer groups?",
            "<strong>Step 4:</strong> Model a late-arriving event problem: events arrive 5 minutes late. You have a 1-minute tumbling window for revenue. How do you handle late data?"
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Draw Kafka -> Flink -> NoSQL Architecture",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Draw Kafka -> Flink -> NoSQL Architecture \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: 'guarantee each message processed exactly once?' \u2192 explain at-least-once + idempotent consumer.",
                "Partition count limits parallelism: 4 partitions \u2192 max 4 consumers per group. Adding a 5th consumer idles.",
                "Use partition key = user_id when ordering per-user matters (e.g., user's event stream must be ordered).",
                "Kafka is NOT a database. Don't use it as a primary store. Consumers should always write to a real store."
            ]
        },
        "HardProblem": "Boss Problem (LinkedIn): Design Kafka's original use case \u2014 the activity feed pipeline. Every LinkedIn user action (post, like, comment, view) is an event streamed to Kafka. (1) What topics and partitions? (2) 3 consumers need the same data: feed renderer, analytics, notifications \u2014 how many consumer groups? (3) A user with 50M followers posts \u2014 fan-out-on-write creates 50M messages. How does Kafka handle this? (4) You need to replay the last 7 days of events to rebuild the recommendation model. Are 7-day retention and 50M events/day feasible on Kafka?"
    },
    {
        "Week": 12,
        "Day": "Thursday",
        "Phase": "3. Architect (System)",
        "Theme": "System Design",
        "SpecificTopic": "Pattern: CDC",
        "ActionItem_Deliverable": "Draw DB -> Debezium -> Kafka Architecture",
        "LeetCodeProblem": "<strong>LC 85 \u2013 Maximal Rectangle</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Windowing</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 DE System Design + Back-of-Envelope Math</div><div class=\"rich\"><h4>Data Engineering System Design \u2014 What's Different from Software Engineering</h4><p>Software engineering system design interviews focus on: web services, APIs, databases, caching, load balancers. Data engineering system design adds: data volume math, pipeline latency, storage formats, streaming vs batch trade-offs, failure modes at scale, and cost optimization.</p><h4>Back-of-Envelope Math \u2014 Essential for DE Interviews</h4><p>FAANG interviewers expect you to quickly estimate: how much storage do we need? How long will it take to process? What network bandwidth is required? This is called \"back-of-envelope\" calculation. Here are the key numbers to memorize:</p><pre>Useful numbers:\n  1 byte       = 1 character\n  1 KB         = 1,000 bytes\n  1 MB         = 1,000 KB\n  1 GB         = 1,000 MB\n  1 TB         = 1,000 GB\n  1 PB         = 1,000 TB\n\n  SSD read     = 500 MB/sec\n  HDD read     = 100 MB/sec\n  Network LAN  = 1 GB/sec\n  S3/cloud     = 100-500 MB/sec (per file, in parallel: much more)\n\n  1 million events/day \u00d7 100 bytes/event = 100 MB/day\n  1 billion events/day \u00d7 100 bytes/event = 100 GB/day\n  = 3 TB/month \u2248 36 TB/year (uncompressed)\n  Parquet at 5x compression = 7 TB/year \u2014 easily fits in S3\n\n  100 million users \u00d7 1 KB profile = 100 GB \u2014 fits in memory on a large machine!</pre><h4>Batch ETL vs Streaming \u2014 The Core Trade-Off</h4><table><tr><th>Dimension</th><td>Batch ETL</td><td>Streaming</td></tr><tr><th>Data freshness</th><td>Hours to daily</td><td>Seconds to minutes</td></tr><tr><th>Complexity</th><td>Lower (simpler tooling)</td><td>Higher (state, ordering, fault tolerance)</td></tr><tr><th>Cost</th><td>Lower (bursty compute)</td><td>Higher (always-running compute)</td></tr><tr><th>Failure recovery</th><td>Simple: re-run the batch</td><td>Complex: replay from Kafka offset</td></tr><tr><th>Best for</th><td>Daily reports, historical analysis</td><td>Fraud detection, live dashboards, alerting</td></tr><tr><th>Tooling</th><td>Spark, dbt, Airflow</td><td>Kafka, Flink, Spark Structured Streaming</td></tr></table></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Batch ETL Architecture + CDC</div><div class=\"rich\"><h4>Pattern: Designing a Batch ETL System</h4><p>Interview: \"Design a system to compute daily sales metrics for a 50M-user e-commerce platform.\"</p><pre>=== Requirements ===\nFunctional:\n  - Daily revenue by product category and region\n  - Top 100 products by revenue per day\n  - Customer lifetime value (CLV) updated daily\n\nNon-functional:\n  - Pipeline must complete by 6am (SLA: 3 hours after midnight)\n  - Data must be queryable by the BI team by 7am\n  - Failures must auto-retry, team alerted if SLA missed\n\n=== Volume math ===\n  50M users \u00d7 avg 1 order/day = 50M orders/day\n  50M orders \u00d7 3 items avg   = 150M order items/day\n  150M rows \u00d7 200 bytes      = 30GB/day uncompressed\n  Parquet compression (5x)   = 6GB/day to write\n  Running 1 year             = 2.2TB stored \u2014 very manageable\n\n=== Architecture ===\n  Source: OLTP PostgreSQL \u2192 CDC (Debezium) \u2192 Kafka topics\n  Landing zone: Kafka \u2192 raw Parquet on S3 (append every 5 min)\n  Daily batch: Airflow DAG at 00:30 UTC\n    Task 1: Spark reads raw S3 Parquet, computes dim + fact tables\n    Task 2: dbt runs transformation models + tests\n    Task 3: Snowflake/BigQuery tables updated for BI queries\n    Task 4: SLA check \u2014 alert if not done by 03:00 UTC</pre><h4>CDC \u2014 Change Data Capture</h4><p><strong>CDC (Change Data Capture)</strong> captures database changes (INSERT/UPDATE/DELETE) in real time from the database's transaction log, without polling. Debezium is the open-source tool that reads PostgreSQL/MySQL WAL logs and publishes changes as Kafka messages.</p><pre># Why CDC instead of \"SELECT * WHERE updated_at > last_run_time\"?\n\n# Problem with polling:\n  1. Misses DELETEs (deleted rows cannot be queried)\n  2. Missing updated_at on some tables\n  3. High load on source DB (full table scans)\n  4. Possible race condition: row updated between runs\n\n# CDC reads from the WAL (Write-Ahead Log) directly:\n  - Captures every INSERT/UPDATE/DELETE as it happens\n  - Zero load on source tables\n  - No missed changes\n  - Before/after images of every row change\n\n# Debezium Kafka message format:\n{\n  \"op\": \"u\",         # u=update, c=create, d=delete\n  \"before\": {\"user_id\": 42, \"tier\": \"silver\"},\n  \"after\":  {\"user_id\": 42, \"tier\": \"gold\"},\n  \"ts_ms\": 1704067200000\n}</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Lambda vs Kappa Architecture</div><div class=\"rich\"><h4>Pattern: Lambda Architecture + Lambda vs Kappa</h4><p><strong>Lambda Architecture:</strong> run two parallel systems \u2014 a batch layer for accuracy (recomputes everything correctly, but slowly) and a speed layer for freshness (approximate real-time results using streaming). Serving layer merges both.</p><pre>Lambda Architecture:\n\nRaw data \u2500\u2500\u252c\u2500\u2500 Batch Layer (Spark, daily job) \u2500\u2500\u2500\u2500\u2500\u2500\u25ba Batch views (exact)\n           \u2502                                                  \u2502\n           \u2514\u2500\u2500 Speed Layer (Kafka+Flink, real-time) \u2500\u2500\u25ba Real-time views \u2500\u2500\u25ba Serving Layer \u2500\u2500\u25ba App\n\nPros: batch layer corrects errors in real-time layer\nCons: two codebases to maintain, possible inconsistencies between layers\n\nKappa Architecture:\n  Only a speed layer. Use streaming for EVERYTHING, including historical reprocessing.\n  If you need to reprocess, replay from Kafka (if retention long enough) or S3.\n  Pros: one codebase. Cons: streaming code is harder, Kafka storage costs.\n\nTrend at FAANG: Kappa is winning for new systems.\nLambda still common in legacy systems built 2015-2020.</pre><h4>Trade-Offs Matrix \u2014 For System Design Interviews</h4><table><tr><th>Requirement</th><td>Batch</td><td>Micro-batch (Spark SS)</td><td>Streaming (Flink/Kafka)</td></tr><tr><th>Data freshness needed</th><td>Hours/daily</td><td>5-30 min</td><td><1 min</td></tr><tr><th>Historical reprocessing</th><td>Easy: re-run job</td><td>Moderate</td><td>Complex: replay from log</td></tr><tr><th>Exactly-once guarantee</th><td>Easy: idempotent writes</td><td>Possible with checkpointing</td><td>Hard: requires distributed transactions</td></tr><tr><th>Dev complexity</th><td>Low</td><td>Medium</td><td>High</td></tr><tr><th>Infrastructure cost</th><td>Low (bursty)</td><td>Medium (semi-continuous)</td><td>High (always-on)</td></tr></table></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Real-Time Fraud Detection Design</div><div class=\"rich\"><h4>Complete System Design: Real-Time Fraud Detection</h4><pre>Interview: \"Design a real-time fraud detection system for a payments company.\"\n\n=== Functional Requirements ===\n  - Flag suspicious transactions within 500ms of payment attempt\n  - 10,000 transactions/second peak\n  - Fraud signal based on: velocity (too many txns), location change,\n    unusual amount, high-risk merchant category\n\n=== Architecture ===\n  Step 1: Ingestion\n    Payment service \u2192 Kafka topic \"transactions\" (partitioned by user_id)\n    Partitioning by user_id ensures all events for one user \u2192 same consumer\n\n  Step 2: Feature computation (Flink, <100ms)\n    - 5-min tumbling window: count txns per user_id \u2192 velocity feature\n    - 1-hour rolling window: SUM(amount) per user \u2192 amount feature\n    - Point lookup: last known location from Redis \u2192 location change\n\n  Step 3: Fraud scoring\n    - ML model (loaded in Flink job): score transaction given features\n    - Or: rule engine: IF velocity > 10 AND amount > $1000 \u2192 high risk\n\n  Step 4: Decision output\n    - Low risk: allow \u2192 write event to \"fraud_scores\" Kafka topic\n    - High risk: block \u2192 write to \"blocked_txns\" + alert fraud team\n\n  Step 5: Storage\n    - All scored transactions \u2192 Kafka \u2192 S3 Parquet (audit log)\n    - Daily batch: Spark recomputes features with full history \u2192 retrain ML model\n\n=== Key design decisions to state ===\n  1. Why Kafka? \u2192 decouples payment service from fraud service\n  2. Why Flink? \u2192 stateful streaming, low latency, exactly-once\n  3. Why Redis? \u2192 O(1) user state lookups (cannot query DW in 500ms)\n  4. Why batch layer too? \u2192 retrain ML model with corrected labels weekly</pre></div></div></div>",
        "KeyConcepts": [
            "DE system design adds: volume math, latency requirements, storage formats, streaming vs batch trade-offs.",
            "Back-of-envelope: 1B events/day \u00d7 100 bytes = 100GB/day. With 5x Parquet compression = 20GB/day.",
            "Batch: simple, cheaper, hours of latency. Streaming: complex, expensive, seconds of latency.",
            "CDC: reads database transaction log (WAL). Captures INSERT/UPDATE/DELETE including hard DELETES. No source load.",
            "Lambda: batch (accurate) + streaming (fast) layers. Complex: two codebases. Used in legacy systems.",
            "Kappa: streaming only. Simpler codebase. Historical reprocessing via log replay. Winning in new systems.",
            "Fraud detection: Kafka \u2192 Flink (stateful windows) \u2192 ML scoring \u2192 Redis (block/allow) \u2192 result topic.",
            "Always justify architecture choices: 'I chose Kafka because...', 'I chose Flink over Spark SS because...'"
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Back-of-envelope: 100M daily active users, each viewing 20 pages/day, each page view is 500 bytes. Calculate: daily data volume (uncompressed + Parquet compressed), monthly storage cost at $0.023/GB-month on S3.",
            "<strong>Step 2:</strong> Design a batch ETL system for a SaaS company: daily computation of MRR (monthly recurring revenue) by plan tier and country. Draw the pipeline from OLTP to BI-ready table.",
            "<strong>Step 3:</strong> Explain CDC to a non-technical stakeholder in 3 sentences. Then explain to a technical interviewer why it is better than polling.",
            "<strong>Step 4:</strong> Mock interview: 'Design a real-time recommendation system for Netflix (movies to show on homepage, updated every 5 minutes based on viewing history).' Use the structured framework: requirements \u2192 volume math \u2192 architecture \u2192 trade-offs."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Draw DB -> Debezium -> Kafka Architecture",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Draw DB -> Debezium -> Kafka Architecture \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: always start with volume math before architecture. Shows structured engineering thinking.",
                "CDC vs polling: CDC catches DELETEs (polling cannot). CDC has zero source DB load. CDC is always preferred.",
                "Lambda vs Kappa: Lambda = two codebases (batch + streaming). Kappa = one codebase (streaming only). New systems = Kappa.",
                "Fraud detection latency budget: 500ms total = 50ms Kafka \u2192 200ms Flink \u2192 100ms Redis lookup \u2192 150ms ML scoring."
            ]
        },
        "HardProblem": "Boss Problem (Stripe): Design a complete data platform for Stripe's analytics needs. Requirements: (1) 100M transactions/day, real-time fraud detection within 200ms; (2) Daily financial reports (revenue by country, product, merchant type) available by 6am UTC; (3) Ad-hoc SQL queries by 200 data analysts on 5 years of historical data; (4) GDPR: delete all data for a user within 30 days of request; (5) Data catalog: any analyst can discover what tables exist and their meaning. Design: the complete architecture (streaming + batch + storage + catalog + governance), justify every component choice, and explain the failure modes."
    },
    {
        "Week": 12,
        "Day": "Friday",
        "Phase": "3. Architect (System)",
        "Theme": "System Design",
        "SpecificTopic": "Trade-offs",
        "ActionItem_Deliverable": "Compare SQL vs NoSQL & Batch vs Stream",
        "LeetCodeProblem": "<strong>LC 42 \u2013 Trapping Rain Water</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Handling Skew</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 DE System Design + Back-of-Envelope Math</div><div class=\"rich\"><h4>Data Engineering System Design \u2014 What's Different from Software Engineering</h4><p>Software engineering system design interviews focus on: web services, APIs, databases, caching, load balancers. Data engineering system design adds: data volume math, pipeline latency, storage formats, streaming vs batch trade-offs, failure modes at scale, and cost optimization.</p><h4>Back-of-Envelope Math \u2014 Essential for DE Interviews</h4><p>FAANG interviewers expect you to quickly estimate: how much storage do we need? How long will it take to process? What network bandwidth is required? This is called \"back-of-envelope\" calculation. Here are the key numbers to memorize:</p><pre>Useful numbers:\n  1 byte       = 1 character\n  1 KB         = 1,000 bytes\n  1 MB         = 1,000 KB\n  1 GB         = 1,000 MB\n  1 TB         = 1,000 GB\n  1 PB         = 1,000 TB\n\n  SSD read     = 500 MB/sec\n  HDD read     = 100 MB/sec\n  Network LAN  = 1 GB/sec\n  S3/cloud     = 100-500 MB/sec (per file, in parallel: much more)\n\n  1 million events/day \u00d7 100 bytes/event = 100 MB/day\n  1 billion events/day \u00d7 100 bytes/event = 100 GB/day\n  = 3 TB/month \u2248 36 TB/year (uncompressed)\n  Parquet at 5x compression = 7 TB/year \u2014 easily fits in S3\n\n  100 million users \u00d7 1 KB profile = 100 GB \u2014 fits in memory on a large machine!</pre><h4>Batch ETL vs Streaming \u2014 The Core Trade-Off</h4><table><tr><th>Dimension</th><td>Batch ETL</td><td>Streaming</td></tr><tr><th>Data freshness</th><td>Hours to daily</td><td>Seconds to minutes</td></tr><tr><th>Complexity</th><td>Lower (simpler tooling)</td><td>Higher (state, ordering, fault tolerance)</td></tr><tr><th>Cost</th><td>Lower (bursty compute)</td><td>Higher (always-running compute)</td></tr><tr><th>Failure recovery</th><td>Simple: re-run the batch</td><td>Complex: replay from Kafka offset</td></tr><tr><th>Best for</th><td>Daily reports, historical analysis</td><td>Fraud detection, live dashboards, alerting</td></tr><tr><th>Tooling</th><td>Spark, dbt, Airflow</td><td>Kafka, Flink, Spark Structured Streaming</td></tr></table></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Batch ETL Architecture + CDC</div><div class=\"rich\"><h4>Pattern: Designing a Batch ETL System</h4><p>Interview: \"Design a system to compute daily sales metrics for a 50M-user e-commerce platform.\"</p><pre>=== Requirements ===\nFunctional:\n  - Daily revenue by product category and region\n  - Top 100 products by revenue per day\n  - Customer lifetime value (CLV) updated daily\n\nNon-functional:\n  - Pipeline must complete by 6am (SLA: 3 hours after midnight)\n  - Data must be queryable by the BI team by 7am\n  - Failures must auto-retry, team alerted if SLA missed\n\n=== Volume math ===\n  50M users \u00d7 avg 1 order/day = 50M orders/day\n  50M orders \u00d7 3 items avg   = 150M order items/day\n  150M rows \u00d7 200 bytes      = 30GB/day uncompressed\n  Parquet compression (5x)   = 6GB/day to write\n  Running 1 year             = 2.2TB stored \u2014 very manageable\n\n=== Architecture ===\n  Source: OLTP PostgreSQL \u2192 CDC (Debezium) \u2192 Kafka topics\n  Landing zone: Kafka \u2192 raw Parquet on S3 (append every 5 min)\n  Daily batch: Airflow DAG at 00:30 UTC\n    Task 1: Spark reads raw S3 Parquet, computes dim + fact tables\n    Task 2: dbt runs transformation models + tests\n    Task 3: Snowflake/BigQuery tables updated for BI queries\n    Task 4: SLA check \u2014 alert if not done by 03:00 UTC</pre><h4>CDC \u2014 Change Data Capture</h4><p><strong>CDC (Change Data Capture)</strong> captures database changes (INSERT/UPDATE/DELETE) in real time from the database's transaction log, without polling. Debezium is the open-source tool that reads PostgreSQL/MySQL WAL logs and publishes changes as Kafka messages.</p><pre># Why CDC instead of \"SELECT * WHERE updated_at > last_run_time\"?\n\n# Problem with polling:\n  1. Misses DELETEs (deleted rows cannot be queried)\n  2. Missing updated_at on some tables\n  3. High load on source DB (full table scans)\n  4. Possible race condition: row updated between runs\n\n# CDC reads from the WAL (Write-Ahead Log) directly:\n  - Captures every INSERT/UPDATE/DELETE as it happens\n  - Zero load on source tables\n  - No missed changes\n  - Before/after images of every row change\n\n# Debezium Kafka message format:\n{\n  \"op\": \"u\",         # u=update, c=create, d=delete\n  \"before\": {\"user_id\": 42, \"tier\": \"silver\"},\n  \"after\":  {\"user_id\": 42, \"tier\": \"gold\"},\n  \"ts_ms\": 1704067200000\n}</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Lambda vs Kappa Architecture</div><div class=\"rich\"><h4>Pattern: Lambda Architecture + Lambda vs Kappa</h4><p><strong>Lambda Architecture:</strong> run two parallel systems \u2014 a batch layer for accuracy (recomputes everything correctly, but slowly) and a speed layer for freshness (approximate real-time results using streaming). Serving layer merges both.</p><pre>Lambda Architecture:\n\nRaw data \u2500\u2500\u252c\u2500\u2500 Batch Layer (Spark, daily job) \u2500\u2500\u2500\u2500\u2500\u2500\u25ba Batch views (exact)\n           \u2502                                                  \u2502\n           \u2514\u2500\u2500 Speed Layer (Kafka+Flink, real-time) \u2500\u2500\u25ba Real-time views \u2500\u2500\u25ba Serving Layer \u2500\u2500\u25ba App\n\nPros: batch layer corrects errors in real-time layer\nCons: two codebases to maintain, possible inconsistencies between layers\n\nKappa Architecture:\n  Only a speed layer. Use streaming for EVERYTHING, including historical reprocessing.\n  If you need to reprocess, replay from Kafka (if retention long enough) or S3.\n  Pros: one codebase. Cons: streaming code is harder, Kafka storage costs.\n\nTrend at FAANG: Kappa is winning for new systems.\nLambda still common in legacy systems built 2015-2020.</pre><h4>Trade-Offs Matrix \u2014 For System Design Interviews</h4><table><tr><th>Requirement</th><td>Batch</td><td>Micro-batch (Spark SS)</td><td>Streaming (Flink/Kafka)</td></tr><tr><th>Data freshness needed</th><td>Hours/daily</td><td>5-30 min</td><td><1 min</td></tr><tr><th>Historical reprocessing</th><td>Easy: re-run job</td><td>Moderate</td><td>Complex: replay from log</td></tr><tr><th>Exactly-once guarantee</th><td>Easy: idempotent writes</td><td>Possible with checkpointing</td><td>Hard: requires distributed transactions</td></tr><tr><th>Dev complexity</th><td>Low</td><td>Medium</td><td>High</td></tr><tr><th>Infrastructure cost</th><td>Low (bursty)</td><td>Medium (semi-continuous)</td><td>High (always-on)</td></tr></table></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Real-Time Fraud Detection Design</div><div class=\"rich\"><h4>Complete System Design: Real-Time Fraud Detection</h4><pre>Interview: \"Design a real-time fraud detection system for a payments company.\"\n\n=== Functional Requirements ===\n  - Flag suspicious transactions within 500ms of payment attempt\n  - 10,000 transactions/second peak\n  - Fraud signal based on: velocity (too many txns), location change,\n    unusual amount, high-risk merchant category\n\n=== Architecture ===\n  Step 1: Ingestion\n    Payment service \u2192 Kafka topic \"transactions\" (partitioned by user_id)\n    Partitioning by user_id ensures all events for one user \u2192 same consumer\n\n  Step 2: Feature computation (Flink, <100ms)\n    - 5-min tumbling window: count txns per user_id \u2192 velocity feature\n    - 1-hour rolling window: SUM(amount) per user \u2192 amount feature\n    - Point lookup: last known location from Redis \u2192 location change\n\n  Step 3: Fraud scoring\n    - ML model (loaded in Flink job): score transaction given features\n    - Or: rule engine: IF velocity > 10 AND amount > $1000 \u2192 high risk\n\n  Step 4: Decision output\n    - Low risk: allow \u2192 write event to \"fraud_scores\" Kafka topic\n    - High risk: block \u2192 write to \"blocked_txns\" + alert fraud team\n\n  Step 5: Storage\n    - All scored transactions \u2192 Kafka \u2192 S3 Parquet (audit log)\n    - Daily batch: Spark recomputes features with full history \u2192 retrain ML model\n\n=== Key design decisions to state ===\n  1. Why Kafka? \u2192 decouples payment service from fraud service\n  2. Why Flink? \u2192 stateful streaming, low latency, exactly-once\n  3. Why Redis? \u2192 O(1) user state lookups (cannot query DW in 500ms)\n  4. Why batch layer too? \u2192 retrain ML model with corrected labels weekly</pre></div></div></div>",
        "KeyConcepts": [
            "DE system design adds: volume math, latency requirements, storage formats, streaming vs batch trade-offs.",
            "Back-of-envelope: 1B events/day \u00d7 100 bytes = 100GB/day. With 5x Parquet compression = 20GB/day.",
            "Batch: simple, cheaper, hours of latency. Streaming: complex, expensive, seconds of latency.",
            "CDC: reads database transaction log (WAL). Captures INSERT/UPDATE/DELETE including hard DELETES. No source load.",
            "Lambda: batch (accurate) + streaming (fast) layers. Complex: two codebases. Used in legacy systems.",
            "Kappa: streaming only. Simpler codebase. Historical reprocessing via log replay. Winning in new systems.",
            "Fraud detection: Kafka \u2192 Flink (stateful windows) \u2192 ML scoring \u2192 Redis (block/allow) \u2192 result topic.",
            "Always justify architecture choices: 'I chose Kafka because...', 'I chose Flink over Spark SS because...'"
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Back-of-envelope: 100M daily active users, each viewing 20 pages/day, each page view is 500 bytes. Calculate: daily data volume (uncompressed + Parquet compressed), monthly storage cost at $0.023/GB-month on S3.",
            "<strong>Step 2:</strong> Design a batch ETL system for a SaaS company: daily computation of MRR (monthly recurring revenue) by plan tier and country. Draw the pipeline from OLTP to BI-ready table.",
            "<strong>Step 3:</strong> Explain CDC to a non-technical stakeholder in 3 sentences. Then explain to a technical interviewer why it is better than polling.",
            "<strong>Step 4:</strong> Mock interview: 'Design a real-time recommendation system for Netflix (movies to show on homepage, updated every 5 minutes based on viewing history).' Use the structured framework: requirements \u2192 volume math \u2192 architecture \u2192 trade-offs."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Compare SQL vs NoSQL & Batch vs Stream",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Compare SQL vs NoSQL & Batch vs Stream \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: always start with volume math before architecture. Shows structured engineering thinking.",
                "CDC vs polling: CDC catches DELETEs (polling cannot). CDC has zero source DB load. CDC is always preferred.",
                "Lambda vs Kappa: Lambda = two codebases (batch + streaming). Kappa = one codebase (streaming only). New systems = Kappa.",
                "Fraud detection latency budget: 500ms total = 50ms Kafka \u2192 200ms Flink \u2192 100ms Redis lookup \u2192 150ms ML scoring."
            ]
        },
        "HardProblem": "Boss Problem (Stripe): Design a complete data platform for Stripe's analytics needs. Requirements: (1) 100M transactions/day, real-time fraud detection within 200ms; (2) Daily financial reports (revenue by country, product, merchant type) available by 6am UTC; (3) Ad-hoc SQL queries by 200 data analysts on 5 years of historical data; (4) GDPR: delete all data for a user within 30 days of request; (5) Data catalog: any analyst can discover what tables exist and their meaning. Design: the complete architecture (streaming + batch + storage + catalog + governance), justify every component choice, and explain the failure modes."
    },
    {
        "Week": 12,
        "Day": "Saturday",
        "Phase": "3. Architect (System)",
        "Theme": "System Design",
        "SpecificTopic": "Full Mock Design + Mock #3",
        "ActionItem_Deliverable": "Mock Design Spotify Top 10 Songs + Full Mock Interview Loop #3",
        "LeetCodeProblem": "<strong>LC 76 \u2013 Minimum Window Substring</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": "NEW: Mock #3 added",
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>dbt Intro + Mock #2</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\"><div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 DE System Design + Back-of-Envelope Math</div><div class=\"rich\"><h4>Data Engineering System Design \u2014 What's Different from Software Engineering</h4><p>Software engineering system design interviews focus on: web services, APIs, databases, caching, load balancers. Data engineering system design adds: data volume math, pipeline latency, storage formats, streaming vs batch trade-offs, failure modes at scale, and cost optimization.</p><h4>Back-of-Envelope Math \u2014 Essential for DE Interviews</h4><p>FAANG interviewers expect you to quickly estimate: how much storage do we need? How long will it take to process? What network bandwidth is required? This is called \"back-of-envelope\" calculation. Here are the key numbers to memorize:</p><pre>Useful numbers:\n  1 byte       = 1 character\n  1 KB         = 1,000 bytes\n  1 MB         = 1,000 KB\n  1 GB         = 1,000 MB\n  1 TB         = 1,000 GB\n  1 PB         = 1,000 TB\n\n  SSD read     = 500 MB/sec\n  HDD read     = 100 MB/sec\n  Network LAN  = 1 GB/sec\n  S3/cloud     = 100-500 MB/sec (per file, in parallel: much more)\n\n  1 million events/day \u00d7 100 bytes/event = 100 MB/day\n  1 billion events/day \u00d7 100 bytes/event = 100 GB/day\n  = 3 TB/month \u2248 36 TB/year (uncompressed)\n  Parquet at 5x compression = 7 TB/year \u2014 easily fits in S3\n\n  100 million users \u00d7 1 KB profile = 100 GB \u2014 fits in memory on a large machine!</pre><h4>Batch ETL vs Streaming \u2014 The Core Trade-Off</h4><table><tr><th>Dimension</th><td>Batch ETL</td><td>Streaming</td></tr><tr><th>Data freshness</th><td>Hours to daily</td><td>Seconds to minutes</td></tr><tr><th>Complexity</th><td>Lower (simpler tooling)</td><td>Higher (state, ordering, fault tolerance)</td></tr><tr><th>Cost</th><td>Lower (bursty compute)</td><td>Higher (always-running compute)</td></tr><tr><th>Failure recovery</th><td>Simple: re-run the batch</td><td>Complex: replay from Kafka offset</td></tr><tr><th>Best for</th><td>Daily reports, historical analysis</td><td>Fraud detection, live dashboards, alerting</td></tr><tr><th>Tooling</th><td>Spark, dbt, Airflow</td><td>Kafka, Flink, Spark Structured Streaming</td></tr></table></div></div><div class=\"level level-2\"><div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Batch ETL Architecture + CDC</div><div class=\"rich\"><h4>Pattern: Designing a Batch ETL System</h4><p>Interview: \"Design a system to compute daily sales metrics for a 50M-user e-commerce platform.\"</p><pre>=== Requirements ===\nFunctional:\n  - Daily revenue by product category and region\n  - Top 100 products by revenue per day\n  - Customer lifetime value (CLV) updated daily\n\nNon-functional:\n  - Pipeline must complete by 6am (SLA: 3 hours after midnight)\n  - Data must be queryable by the BI team by 7am\n  - Failures must auto-retry, team alerted if SLA missed\n\n=== Volume math ===\n  50M users \u00d7 avg 1 order/day = 50M orders/day\n  50M orders \u00d7 3 items avg   = 150M order items/day\n  150M rows \u00d7 200 bytes      = 30GB/day uncompressed\n  Parquet compression (5x)   = 6GB/day to write\n  Running 1 year             = 2.2TB stored \u2014 very manageable\n\n=== Architecture ===\n  Source: OLTP PostgreSQL \u2192 CDC (Debezium) \u2192 Kafka topics\n  Landing zone: Kafka \u2192 raw Parquet on S3 (append every 5 min)\n  Daily batch: Airflow DAG at 00:30 UTC\n    Task 1: Spark reads raw S3 Parquet, computes dim + fact tables\n    Task 2: dbt runs transformation models + tests\n    Task 3: Snowflake/BigQuery tables updated for BI queries\n    Task 4: SLA check \u2014 alert if not done by 03:00 UTC</pre><h4>CDC \u2014 Change Data Capture</h4><p><strong>CDC (Change Data Capture)</strong> captures database changes (INSERT/UPDATE/DELETE) in real time from the database's transaction log, without polling. Debezium is the open-source tool that reads PostgreSQL/MySQL WAL logs and publishes changes as Kafka messages.</p><pre># Why CDC instead of \"SELECT * WHERE updated_at > last_run_time\"?\n\n# Problem with polling:\n  1. Misses DELETEs (deleted rows cannot be queried)\n  2. Missing updated_at on some tables\n  3. High load on source DB (full table scans)\n  4. Possible race condition: row updated between runs\n\n# CDC reads from the WAL (Write-Ahead Log) directly:\n  - Captures every INSERT/UPDATE/DELETE as it happens\n  - Zero load on source tables\n  - No missed changes\n  - Before/after images of every row change\n\n# Debezium Kafka message format:\n{\n  \"op\": \"u\",         # u=update, c=create, d=delete\n  \"before\": {\"user_id\": 42, \"tier\": \"silver\"},\n  \"after\":  {\"user_id\": 42, \"tier\": \"gold\"},\n  \"ts_ms\": 1704067200000\n}</pre></div></div><div class=\"level level-3\"><div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Lambda vs Kappa Architecture</div><div class=\"rich\"><h4>Pattern: Lambda Architecture + Lambda vs Kappa</h4><p><strong>Lambda Architecture:</strong> run two parallel systems \u2014 a batch layer for accuracy (recomputes everything correctly, but slowly) and a speed layer for freshness (approximate real-time results using streaming). Serving layer merges both.</p><pre>Lambda Architecture:\n\nRaw data \u2500\u2500\u252c\u2500\u2500 Batch Layer (Spark, daily job) \u2500\u2500\u2500\u2500\u2500\u2500\u25ba Batch views (exact)\n           \u2502                                                  \u2502\n           \u2514\u2500\u2500 Speed Layer (Kafka+Flink, real-time) \u2500\u2500\u25ba Real-time views \u2500\u2500\u25ba Serving Layer \u2500\u2500\u25ba App\n\nPros: batch layer corrects errors in real-time layer\nCons: two codebases to maintain, possible inconsistencies between layers\n\nKappa Architecture:\n  Only a speed layer. Use streaming for EVERYTHING, including historical reprocessing.\n  If you need to reprocess, replay from Kafka (if retention long enough) or S3.\n  Pros: one codebase. Cons: streaming code is harder, Kafka storage costs.\n\nTrend at FAANG: Kappa is winning for new systems.\nLambda still common in legacy systems built 2015-2020.</pre><h4>Trade-Offs Matrix \u2014 For System Design Interviews</h4><table><tr><th>Requirement</th><td>Batch</td><td>Micro-batch (Spark SS)</td><td>Streaming (Flink/Kafka)</td></tr><tr><th>Data freshness needed</th><td>Hours/daily</td><td>5-30 min</td><td><1 min</td></tr><tr><th>Historical reprocessing</th><td>Easy: re-run job</td><td>Moderate</td><td>Complex: replay from log</td></tr><tr><th>Exactly-once guarantee</th><td>Easy: idempotent writes</td><td>Possible with checkpointing</td><td>Hard: requires distributed transactions</td></tr><tr><th>Dev complexity</th><td>Low</td><td>Medium</td><td>High</td></tr><tr><th>Infrastructure cost</th><td>Low (bursty)</td><td>Medium (semi-continuous)</td><td>High (always-on)</td></tr></table></div></div><div class=\"level level-4\"><div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Real-Time Fraud Detection Design</div><div class=\"rich\"><h4>Complete System Design: Real-Time Fraud Detection</h4><pre>Interview: \"Design a real-time fraud detection system for a payments company.\"\n\n=== Functional Requirements ===\n  - Flag suspicious transactions within 500ms of payment attempt\n  - 10,000 transactions/second peak\n  - Fraud signal based on: velocity (too many txns), location change,\n    unusual amount, high-risk merchant category\n\n=== Architecture ===\n  Step 1: Ingestion\n    Payment service \u2192 Kafka topic \"transactions\" (partitioned by user_id)\n    Partitioning by user_id ensures all events for one user \u2192 same consumer\n\n  Step 2: Feature computation (Flink, <100ms)\n    - 5-min tumbling window: count txns per user_id \u2192 velocity feature\n    - 1-hour rolling window: SUM(amount) per user \u2192 amount feature\n    - Point lookup: last known location from Redis \u2192 location change\n\n  Step 3: Fraud scoring\n    - ML model (loaded in Flink job): score transaction given features\n    - Or: rule engine: IF velocity > 10 AND amount > $1000 \u2192 high risk\n\n  Step 4: Decision output\n    - Low risk: allow \u2192 write event to \"fraud_scores\" Kafka topic\n    - High risk: block \u2192 write to \"blocked_txns\" + alert fraud team\n\n  Step 5: Storage\n    - All scored transactions \u2192 Kafka \u2192 S3 Parquet (audit log)\n    - Daily batch: Spark recomputes features with full history \u2192 retrain ML model\n\n=== Key design decisions to state ===\n  1. Why Kafka? \u2192 decouples payment service from fraud service\n  2. Why Flink? \u2192 stateful streaming, low latency, exactly-once\n  3. Why Redis? \u2192 O(1) user state lookups (cannot query DW in 500ms)\n  4. Why batch layer too? \u2192 retrain ML model with corrected labels weekly</pre></div></div></div>",
        "KeyConcepts": [
            "DE system design adds: volume math, latency requirements, storage formats, streaming vs batch trade-offs.",
            "Back-of-envelope: 1B events/day \u00d7 100 bytes = 100GB/day. With 5x Parquet compression = 20GB/day.",
            "Batch: simple, cheaper, hours of latency. Streaming: complex, expensive, seconds of latency.",
            "CDC: reads database transaction log (WAL). Captures INSERT/UPDATE/DELETE including hard DELETES. No source load.",
            "Lambda: batch (accurate) + streaming (fast) layers. Complex: two codebases. Used in legacy systems.",
            "Kappa: streaming only. Simpler codebase. Historical reprocessing via log replay. Winning in new systems.",
            "Fraud detection: Kafka \u2192 Flink (stateful windows) \u2192 ML scoring \u2192 Redis (block/allow) \u2192 result topic.",
            "Always justify architecture choices: 'I chose Kafka because...', 'I chose Flink over Spark SS because...'"
        ],
        "Tasks": [
            "<strong>Step 1:</strong> Back-of-envelope: 100M daily active users, each viewing 20 pages/day, each page view is 500 bytes. Calculate: daily data volume (uncompressed + Parquet compressed), monthly storage cost at $0.023/GB-month on S3.",
            "<strong>Step 2:</strong> Design a batch ETL system for a SaaS company: daily computation of MRR (monthly recurring revenue) by plan tier and country. Draw the pipeline from OLTP to BI-ready table.",
            "<strong>Step 3:</strong> Explain CDC to a non-technical stakeholder in 3 sentences. Then explain to a technical interviewer why it is better than polling.",
            "<strong>Step 4:</strong> Mock interview: 'Design a real-time recommendation system for Netflix (movies to show on homepage, updated every 5 minutes based on viewing history).' Use the structured framework: requirements \u2192 volume math \u2192 architecture \u2192 trade-offs."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Mock Design Spotify Top 10 Songs + Full Mock Interview Loop #3",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Mock Design Spotify Top 10 Songs + Full Mock Interview Loop #3 \u2014 trace through 2-3 edge cases before writing any code.",
                "Interview: always start with volume math before architecture. Shows structured engineering thinking.",
                "CDC vs polling: CDC catches DELETEs (polling cannot). CDC has zero source DB load. CDC is always preferred.",
                "Lambda vs Kappa: Lambda = two codebases (batch + streaming). Kappa = one codebase (streaming only). New systems = Kappa.",
                "Fraud detection latency budget: 500ms total = 50ms Kafka \u2192 200ms Flink \u2192 100ms Redis lookup \u2192 150ms ML scoring."
            ]
        },
        "HardProblem": "Boss Problem (Stripe): Design a complete data platform for Stripe's analytics needs. Requirements: (1) 100M transactions/day, real-time fraud detection within 200ms; (2) Daily financial reports (revenue by country, product, merchant type) available by 6am UTC; (3) Ad-hoc SQL queries by 200 data analysts on 5 years of historical data; (4) GDPR: delete all data for a user within 30 days of request; (5) Data catalog: any analyst can discover what tables exist and their meaning. Design: the complete architecture (streaming + batch + storage + catalog + governance), justify every component choice, and explain the failure modes."
    },
    {
        "Week": 12,
        "Day": "Sunday",
        "Phase": "3. Architect (System)",
        "Theme": "Rest",
        "SpecificTopic": "Rest & Review",
        "ActionItem_Deliverable": "Review System Design Cheat Sheet",
        "LeetCodeProblem": null,
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Dimensional Modeling Basics</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson-levels\"><div class=\"level level-1\">\n<div class=\"level-badge\">\ud83d\udfe2 Level 1 \u2014 Active Rest \u2014 How Memory Consolidates</div>\n<div class=\"rich\">\n<h4>The Neuroscience of Spaced Repetition</h4>\n<p>Rest days are not wasted days. During rest, your brain replays what it learned \u2014 a process called <strong>memory consolidation</strong>. Research consistently shows that <em>attempting to recall from memory</em> is 3\u00d7 more effective at building durable knowledge than re-reading notes. The act of struggling to remember \u2014 even failing \u2014 strengthens the neural pathway more than passive review.</p>\n<p>Think of it like muscle recovery: a weightlifter who trains 7 days straight without rest gets weaker, not stronger. The growth happens during recovery. Your brain is the same.</p>\n<h4>Today's Protocol (4 hours, structured)</h4>\n<ol>\n  <li><strong>Active Recall (60 min):</strong> Close everything. On a blank page, write every concept, formula, and pattern from this week from memory. Then check.</li>\n  <li><strong>Error Review (60 min):</strong> Find the hardest problem you struggled with. Attempt it again completely from scratch \u2014 no notes.</li>\n  <li><strong>Teach It (45 min):</strong> Open the AI chatbot and explain Gaps & Islands to it as if it's a junior engineer. If you stumble, that's your gap.</li>\n  <li><strong>Next Week Preview (15 min):</strong> Skim next Monday's topic at headline level only \u2014 priming improves retention when you study it deeply.</li>\n</ol>\n</div>\n</div><div class=\"level level-2\">\n<div class=\"level-badge\">\ud83d\udd35 Level 2 \u2014 Active Recall Quiz \u2014 No Notes Allowed</div>\n<div class=\"rich\">\n<h4>Answer These From Memory. Then Check.</h4>\n<pre>Window Functions:\n\u25a1 What is the difference between ROW_NUMBER, RANK, and DENSE_RANK?\n  (Write the output for a table with two tied rows)\n\u25a1 Write the syntax for a 7-day rolling average from memory\n\u25a1 What does PARTITION BY do differently from GROUP BY?\n\u25a1 Write the complete Gaps & Islands CTE from memory\n\u25a1 What is the difference between ROWS BETWEEN and RANGE BETWEEN?\n\u25a1 LAG(col, 1, 0) vs LAG(col, 1) \u2014 what is different?\n\u25a1 In what SQL execution order step do window functions run?\n\u25a1 Why can't you write WHERE rank &lt;= 3 in the same query as ROW_NUMBER?\n\nScore yourself:\n8/8 = \ud83c\udfc6 Mastery level \u2014 you're ready for interviews on this\n5-7/8 = \ud83d\udfe1 Good \u2014 review the 1-2 concepts you missed\n&lt; 5/8 = \ud83d\udd34 Go back to the specific day's content, re-read Level 1-2</pre>\n</div>\n</div><div class=\"level level-3\">\n<div class=\"level-badge\">\ud83d\udfe1 Level 3 \u2014 Spaced Repetition Schedule</div>\n<div class=\"rich\">\n<h4>When to Review This Week's Material</h4>\n<table>\n<tr><th>Review this content</th><th>When</th><th>How</th></tr>\n<tr><td>This week</td><td>Today</td><td>Active recall quiz above</td></tr>\n<tr><td>This week</td><td>In 3 days</td><td>Re-solve the hard problem from Day 4</td></tr>\n<tr><td>This week</td><td>In 7 days</td><td>LeetCode: solve problems 185, 180, 196</td></tr>\n<tr><td>This week</td><td>In 30 days</td><td>Explain all 4 concepts to someone else</td></tr>\n</table>\n<h4>The Feynman Technique</h4>\n<p>After studying any concept, close your notes and explain it as if you're teaching a newcomer. Use only simple words. Avoid jargon. If you stumble, that's exactly where your understanding breaks down \u2014 return to just that part. This technique built Richard Feynman's legendary ability to explain complex physics simply.</p>\n</div>\n</div><div class=\"level level-4\">\n<div class=\"level-badge\">\ud83d\udd34 Level 4 \u2014 Interview Readiness Self-Assessment</div>\n<div class=\"rich\">\n<pre>Rate yourself 1\u20135 on each topic (honest self-assessment):\n  1 = \"I need to re-read from scratch\"\n  3 = \"I understand but need to look at notes\"\n  5 = \"I can code this under pressure from memory\"\n\nROW_NUMBER / RANK / DENSE_RANK:        ___/5\nRolling windows (ROWS BETWEEN):        ___/5\nLAG/LEAD (adjacent row comparison):    ___/5\nGaps & Islands (date streaks):         ___/5\nSession detection (timestamp version): ___/5\n\nAction:\n  Score &lt;3: revisit that day's Level 1 and Level 2 content tomorrow\n  Score 3:  do one LeetCode problem on that specific topic\n  Score 5:  you're interview-ready \u2705 on this topic</pre>\n<h4>FAANG Interview Reality Check</h4>\n<p>At meta, Amazon, and Google data engineering interviews, you'll be asked to write SQL live on a shared screen without autocomplete or documentation. The most common reason candidates fail: they know the concept but freeze on the syntax under pressure. The fix: write every pattern from memory at least 5 times this week. Muscle memory is more reliable than conscious recall under stress.</p>\n</div>\n</div></div>",
        "KeyConcepts": [
            "Memory consolidation happens during rest \u2014 the brain replays and stabilizes learned patterns during downtime.",
            "Active recall (testing yourself) builds 3x stronger memory than re-reading notes.",
            "Spaced repetition: review at day 1, day 3, day 7, day 30 for maximum retention.",
            "The Feynman Technique: if you can't explain it simply, you don't fully understand it yet.",
            "Error analysis: the problems you got WRONG are your highest-value review items \u2014 spend the most time there."
        ],
        "Tasks": [
            "<strong>Active Recall:</strong> Write the Gaps & Islands CTE from memory on paper. Check it against Day 4.",
            "<strong>Re-attempt:</strong> Pick the hardest problem from this week and solve it in 20 minutes from scratch.",
            "<strong>Teach it:</strong> Explain RANK vs DENSE_RANK to the AI chatbot as if it's a junior. Get it right without notes.",
            "<strong>Preview:</strong> Skim next week's Monday topic for 10 minutes \u2014 just headline concepts."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Review System Design Cheat Sheet",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Review System Design Cheat Sheet \u2014 trace through 2-3 edge cases before writing any code.",
                "Use the AI chatbot to quiz yourself \u2014 ask it 'Give me a SQL window function interview question.'",
                "Write SQL from memory in a plain text editor \u2014 no autocomplete. This builds true recall.",
                "The hardest concept this week: spend 30 minutes writing a full explanation in your own words."
            ]
        },
        "HardProblem": "Connect-the-dots: Write 2-3 paragraphs explaining how ROW_NUMBER, rolling averages, LAG, and Gaps & Islands could ALL be used in a single production pipeline for a ride-sharing company. What problem does each solve? How do they complement each other? What would break if you removed any one of them?"
    },
    {
        "Week": 13,
        "Day": "Monday",
        "Phase": "3. Architect (System)",
        "Theme": "Behavioral",
        "SpecificTopic": "Story Mining",
        "ActionItem_Deliverable": "Write down 5 stories from Virtusa/Projects",
        "LeetCodeProblem": "<strong>LC 1 \u2013 Two Sum (speed run)</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>dbt Intro + Mock #2</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson\">\n<h3>\ud83e\udde0 Hour 1 \u2014 STAR Method: Structuring Your Stories for FAANG</h3>\n<p>Behavioral interviews at FAANG companies assess leadership principles. Amazon has 16 LPs; Meta, Google, and Microsoft each have their own frameworks. The STAR method is the universal structure for answering behavioral questions compellingly.</p>\n<h4>STAR Framework</h4>\n<pre>Situation: Set the scene \u2014 what was the context?\n    \"At Virtusa, we had a critical ETL pipeline that processed $2M/day in transactions...\"\n\nTask: What was YOUR specific responsibility?\n    \"I was the sole data engineer responsible for redesigning the pipeline...\"\n\nAction: What did YOU do? (Use I, not we. Be specific.)\n    \"I identified the bottleneck using Spark's execution plan... I implemented batch micro-aggregations...\n     I deployed the fix using blue-green deployment to avoid downtime...\"\n\nResult: Quantifiable outcome.\n    \"Reduced processing time from 4 hours to 22 minutes (91% improvement).\n     Pipeline failure rate dropped from 15% to 0.2%.\"</pre>\n<h4>Mapping Stories to Amazon Leadership Principles</h4>\n<table border=\"1\" style=\"width:100%;border-collapse:collapse\">\n  <tr><th>LP</th><th>Question type</th><th>Best story themes</th></tr>\n  <tr><td>Deliver Results</td><td>\"Tell me about a time you met a challenging deadline\"</td><td>Pipeline optimization, on-time delivery</td></tr>\n  <tr><td>Ownership</td><td>\"Tell me about a time you took ownership beyond your role\"</td><td>Fixing someone else's bug, proactive monitoring</td></tr>\n  <tr><td>Invent & Simplify</td><td>\"Tell me about a technical innovation you drove\"</td><td>New architecture, automation</td></tr>\n  <tr><td>Dive Deep</td><td>\"How did you debug a complex system issue?\"</td><td>Production incident postmortems</td></tr>\n  <tr><td>Bias for Action</td><td>\"Tell me about a time you made a decision with incomplete data\"</td><td>Incident response, outage mitigation</td></tr>\n</table>\n<h4>What Makes a FAANG-Level Answer</h4>\n<ul>\n  <li>\u2705 Quantified impact (time, money, percentage, scale)</li>\n  <li>\u2705 YOUR specific actions (not the team's)</li>\n  <li>\u2705 Technical depth that shows expertise</li>\n  <li>\u2705 What you learned / would do differently</li>\n  <li>\u274c Vague: \"We improved the pipeline\"</li>\n  <li>\u274c No result: trailing off without an outcome</li>\n  <li>\u274c Too long: behavioral answers should be 2-3 minutes max</li>\n</ul>\n</div>",
        "KeyConcepts": [
            "STAR: Situation, Task, Action, Result \u2014 always end with a quantified result.",
            "Use 'I' not 'we' \u2014 interviewers want to know YOUR contribution, not the team's.",
            "Prepare 5-7 versatile stories that can apply to multiple leadership principles.",
            "Technical behavioral answers should include the specific technology/approach you used.",
            "Quantify results: time saved, cost reduced, percentage improvement, scale achieved.",
            "End every story with what you learned \u2014 shows growth mindset and self-awareness.",
            "2-3 minute answer length \u2014 practice with a timer to avoid rambling."
        ],
        "Tasks": [
            "<strong>Step 1 \u2014 Mine 5 stories from your Virtusa work</strong>: major project, optimization, conflict, failure, innovation.",
            "<strong>Step 2 \u2014 Write each story in STAR format</strong> with specific quantified results.",
            "<strong>Step 3 \u2014 Map each story to 2-3 Amazon LPs</strong> in a spreadsheet.",
            "<strong>Step 4 \u2014 Record yourself saying one story</strong>: watch it back and check timing, clarity, and whether 'I vs we' is correct."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Write down 5 stories from Virtusa/Projects",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Write down 5 stories from Virtusa/Projects \u2014 trace through 2-3 edge cases before writing any code.",
                "Write your stories in a spreadsheet: Column A = story, Columns B-N = which LP it maps to.",
                "Amazon's most frequently asked LP: 'Deliver Results' \u2014 have 2-3 stories ready for it.",
                "Conflicts with coworkers: focus on data-driven resolution and outcome, not personal criticism.",
                "Failure stories are expected \u2014 prepare one honest failure with strong lessons learned."
            ]
        },
        "HardProblem": "Stretch Exercise: The hardest behavioral question \u2014 'Tell me about a time you disagreed with your manager and what happened.' This tests your backbone, communication skills, and professionalism simultaneously. Write a full STAR answer where you: raised a data-driven concern professionally, the outcome benefitted from your input, AND you maintained a good working relationship. This is often a dealbreaker question at senior DE levels."
    },
    {
        "Week": 13,
        "Day": "Tuesday",
        "Phase": "3. Architect (System)",
        "Theme": "Behavioral",
        "SpecificTopic": "STAR Method",
        "ActionItem_Deliverable": "Format stories: Situation-Task-Action-Result",
        "LeetCodeProblem": "<strong>LC 121 \u2013 Best Time to Buy Stock (speed)</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Sets & Deduplication</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson\">\n<h3>\ud83e\udde0 Hour 1 \u2014 STAR Method: Structuring Your Stories for FAANG</h3>\n<p>Behavioral interviews at FAANG companies assess leadership principles. Amazon has 16 LPs; Meta, Google, and Microsoft each have their own frameworks. The STAR method is the universal structure for answering behavioral questions compellingly.</p>\n<h4>STAR Framework</h4>\n<pre>Situation: Set the scene \u2014 what was the context?\n    \"At Virtusa, we had a critical ETL pipeline that processed $2M/day in transactions...\"\n\nTask: What was YOUR specific responsibility?\n    \"I was the sole data engineer responsible for redesigning the pipeline...\"\n\nAction: What did YOU do? (Use I, not we. Be specific.)\n    \"I identified the bottleneck using Spark's execution plan... I implemented batch micro-aggregations...\n     I deployed the fix using blue-green deployment to avoid downtime...\"\n\nResult: Quantifiable outcome.\n    \"Reduced processing time from 4 hours to 22 minutes (91% improvement).\n     Pipeline failure rate dropped from 15% to 0.2%.\"</pre>\n<h4>Mapping Stories to Amazon Leadership Principles</h4>\n<table border=\"1\" style=\"width:100%;border-collapse:collapse\">\n  <tr><th>LP</th><th>Question type</th><th>Best story themes</th></tr>\n  <tr><td>Deliver Results</td><td>\"Tell me about a time you met a challenging deadline\"</td><td>Pipeline optimization, on-time delivery</td></tr>\n  <tr><td>Ownership</td><td>\"Tell me about a time you took ownership beyond your role\"</td><td>Fixing someone else's bug, proactive monitoring</td></tr>\n  <tr><td>Invent & Simplify</td><td>\"Tell me about a technical innovation you drove\"</td><td>New architecture, automation</td></tr>\n  <tr><td>Dive Deep</td><td>\"How did you debug a complex system issue?\"</td><td>Production incident postmortems</td></tr>\n  <tr><td>Bias for Action</td><td>\"Tell me about a time you made a decision with incomplete data\"</td><td>Incident response, outage mitigation</td></tr>\n</table>\n<h4>What Makes a FAANG-Level Answer</h4>\n<ul>\n  <li>\u2705 Quantified impact (time, money, percentage, scale)</li>\n  <li>\u2705 YOUR specific actions (not the team's)</li>\n  <li>\u2705 Technical depth that shows expertise</li>\n  <li>\u2705 What you learned / would do differently</li>\n  <li>\u274c Vague: \"We improved the pipeline\"</li>\n  <li>\u274c No result: trailing off without an outcome</li>\n  <li>\u274c Too long: behavioral answers should be 2-3 minutes max</li>\n</ul>\n</div>",
        "KeyConcepts": [
            "STAR: Situation, Task, Action, Result \u2014 always end with a quantified result.",
            "Use 'I' not 'we' \u2014 interviewers want to know YOUR contribution, not the team's.",
            "Prepare 5-7 versatile stories that can apply to multiple leadership principles.",
            "Technical behavioral answers should include the specific technology/approach you used.",
            "Quantify results: time saved, cost reduced, percentage improvement, scale achieved.",
            "End every story with what you learned \u2014 shows growth mindset and self-awareness.",
            "2-3 minute answer length \u2014 practice with a timer to avoid rambling."
        ],
        "Tasks": [
            "<strong>Step 1 \u2014 Mine 5 stories from your Virtusa work</strong>: major project, optimization, conflict, failure, innovation.",
            "<strong>Step 2 \u2014 Write each story in STAR format</strong> with specific quantified results.",
            "<strong>Step 3 \u2014 Map each story to 2-3 Amazon LPs</strong> in a spreadsheet.",
            "<strong>Step 4 \u2014 Record yourself saying one story</strong>: watch it back and check timing, clarity, and whether 'I vs we' is correct."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Format stories: Situation-Task-Action-Result",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Format stories: Situation-Task-Action-Result \u2014 trace through 2-3 edge cases before writing any code.",
                "Write your stories in a spreadsheet: Column A = story, Columns B-N = which LP it maps to.",
                "Amazon's most frequently asked LP: 'Deliver Results' \u2014 have 2-3 stories ready for it.",
                "Conflicts with coworkers: focus on data-driven resolution and outcome, not personal criticism.",
                "Failure stories are expected \u2014 prepare one honest failure with strong lessons learned."
            ]
        },
        "HardProblem": "Stretch Exercise: The hardest behavioral question \u2014 'Tell me about a time you disagreed with your manager and what happened.' This tests your backbone, communication skills, and professionalism simultaneously. Write a full STAR answer where you: raised a data-driven concern professionally, the outcome benefitted from your input, AND you maintained a good working relationship. This is often a dealbreaker question at senior DE levels."
    },
    {
        "Week": 13,
        "Day": "Wednesday",
        "Phase": "3. Architect (System)",
        "Theme": "Behavioral",
        "SpecificTopic": "Conflict & Failure",
        "ActionItem_Deliverable": "Prepare Tell me about a time you failed",
        "LeetCodeProblem": "<strong>LC 347 \u2013 Top K (speed run)</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Case Study: Social Media</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson\">\n<h3>\ud83e\udde0 Hour 1 \u2014 STAR Method: Structuring Your Stories for FAANG</h3>\n<p>Behavioral interviews at FAANG companies assess leadership principles. Amazon has 16 LPs; Meta, Google, and Microsoft each have their own frameworks. The STAR method is the universal structure for answering behavioral questions compellingly.</p>\n<h4>STAR Framework</h4>\n<pre>Situation: Set the scene \u2014 what was the context?\n    \"At Virtusa, we had a critical ETL pipeline that processed $2M/day in transactions...\"\n\nTask: What was YOUR specific responsibility?\n    \"I was the sole data engineer responsible for redesigning the pipeline...\"\n\nAction: What did YOU do? (Use I, not we. Be specific.)\n    \"I identified the bottleneck using Spark's execution plan... I implemented batch micro-aggregations...\n     I deployed the fix using blue-green deployment to avoid downtime...\"\n\nResult: Quantifiable outcome.\n    \"Reduced processing time from 4 hours to 22 minutes (91% improvement).\n     Pipeline failure rate dropped from 15% to 0.2%.\"</pre>\n<h4>Mapping Stories to Amazon Leadership Principles</h4>\n<table border=\"1\" style=\"width:100%;border-collapse:collapse\">\n  <tr><th>LP</th><th>Question type</th><th>Best story themes</th></tr>\n  <tr><td>Deliver Results</td><td>\"Tell me about a time you met a challenging deadline\"</td><td>Pipeline optimization, on-time delivery</td></tr>\n  <tr><td>Ownership</td><td>\"Tell me about a time you took ownership beyond your role\"</td><td>Fixing someone else's bug, proactive monitoring</td></tr>\n  <tr><td>Invent & Simplify</td><td>\"Tell me about a technical innovation you drove\"</td><td>New architecture, automation</td></tr>\n  <tr><td>Dive Deep</td><td>\"How did you debug a complex system issue?\"</td><td>Production incident postmortems</td></tr>\n  <tr><td>Bias for Action</td><td>\"Tell me about a time you made a decision with incomplete data\"</td><td>Incident response, outage mitigation</td></tr>\n</table>\n<h4>What Makes a FAANG-Level Answer</h4>\n<ul>\n  <li>\u2705 Quantified impact (time, money, percentage, scale)</li>\n  <li>\u2705 YOUR specific actions (not the team's)</li>\n  <li>\u2705 Technical depth that shows expertise</li>\n  <li>\u2705 What you learned / would do differently</li>\n  <li>\u274c Vague: \"We improved the pipeline\"</li>\n  <li>\u274c No result: trailing off without an outcome</li>\n  <li>\u274c Too long: behavioral answers should be 2-3 minutes max</li>\n</ul>\n</div>",
        "KeyConcepts": [
            "STAR: Situation, Task, Action, Result \u2014 always end with a quantified result.",
            "Use 'I' not 'we' \u2014 interviewers want to know YOUR contribution, not the team's.",
            "Prepare 5-7 versatile stories that can apply to multiple leadership principles.",
            "Technical behavioral answers should include the specific technology/approach you used.",
            "Quantify results: time saved, cost reduced, percentage improvement, scale achieved.",
            "End every story with what you learned \u2014 shows growth mindset and self-awareness.",
            "2-3 minute answer length \u2014 practice with a timer to avoid rambling."
        ],
        "Tasks": [
            "<strong>Step 1 \u2014 Mine 5 stories from your Virtusa work</strong>: major project, optimization, conflict, failure, innovation.",
            "<strong>Step 2 \u2014 Write each story in STAR format</strong> with specific quantified results.",
            "<strong>Step 3 \u2014 Map each story to 2-3 Amazon LPs</strong> in a spreadsheet.",
            "<strong>Step 4 \u2014 Record yourself saying one story</strong>: watch it back and check timing, clarity, and whether 'I vs we' is correct."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Prepare Tell me about a time you failed",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Prepare Tell me about a time you failed \u2014 trace through 2-3 edge cases before writing any code.",
                "Write your stories in a spreadsheet: Column A = story, Columns B-N = which LP it maps to.",
                "Amazon's most frequently asked LP: 'Deliver Results' \u2014 have 2-3 stories ready for it.",
                "Conflicts with coworkers: focus on data-driven resolution and outcome, not personal criticism.",
                "Failure stories are expected \u2014 prepare one honest failure with strong lessons learned."
            ]
        },
        "HardProblem": "Stretch Exercise: The hardest behavioral question \u2014 'Tell me about a time you disagreed with your manager and what happened.' This tests your backbone, communication skills, and professionalism simultaneously. Write a full STAR answer where you: raised a data-driven concern professionally, the outcome benefitted from your input, AND you maintained a good working relationship. This is often a dealbreaker question at senior DE levels."
    },
    {
        "Week": 13,
        "Day": "Thursday",
        "Phase": "3. Architect (System)",
        "Theme": "Behavioral",
        "SpecificTopic": "Resume Walkthrough",
        "ActionItem_Deliverable": "Perfect the Tell me about yourself pitch",
        "LeetCodeProblem": "<strong>LC 56 \u2013 Merge Intervals (speed run)</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Decorators</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson\">\n<h3>\ud83c\udfc1 Hour 1 \u2014 Mental Preparation: The Final 48 Hours</h3>\n<p>The night before and day of the interview, technical cramming hurts more than it helps. Your goal is to show up mentally sharp, confident, and in peak cognitive state. Here's the science-backed protocol.</p>\n<h4>48 Hours Before: Light Review Mode</h4>\n<ul>\n  <li>Review only your personal notes \u2014 not new material</li>\n  <li>Re-read your 5 behavioral stories once</li>\n  <li>Draw your best system design (1-2 drawings) from memory</li>\n  <li>No LeetCode grinding \u2014 it increases anxiety without improving performance</li>\n</ul>\n<h4>24 Hours Before: Reset Mode</h4>\n<ul>\n  <li>Exercise: 30-45 min moderate cardio \u2014 increases BDNF, sharpens cognition next day</li>\n  <li>Sleep 8 hours \u2014 memory consolidation happens during deep sleep</li>\n  <li>Avoid alcohol \u2014 disrupts REM sleep and slow-wave sleep</li>\n  <li>Prepare logistics: confirm interview time/format, test Zoom/CoderPad link</li>\n</ul>\n<h4>Day Of: Performance Mode</h4>\n<pre>Morning:\n- Eat a real breakfast (glucose is brain fuel)\n- 20-min light walk or stretch\n- Read your notes ONCE \u2014 then put them away\n\nIn the interview:\n- Think aloud: \"Let me make sure I understand the problem...\"\n- Clarify before coding: \"Can I assume the data fits in memory?\"\n- Show your work: narrate every decision (\"I'm using DENSE_RANK because there should be no gaps...\")\n- Stuck? Say it: \"Let me think through the edge cases...\" (silence is NOT a signal of failure)\n- Wrong answer? Own it: \"That approach has a flaw \u2014 let me reconsider...\"</pre>\n<h4>The Interview Mindset Reframe</h4>\n<p>You are not being tested \u2014 you are having a technical conversation with a peer. The interviewer WANTS you to succeed (hiring is expensive and painful). They are rooting for you to show your best self. Reframe nerves as excitement.</p>\n</div>",
        "KeyConcepts": [
            "No new material 48 hours before the interview \u2014 only review of existing notes.",
            "Sleep is more valuable than last-minute studying \u2014 memory consolidation happens during sleep.",
            "Think aloud during technical questions \u2014 interviewers need to see your reasoning process.",
            "Clarify before coding \u2014 this shows senior-level professionalism, not weakness.",
            "Being stuck is normal and expected \u2014 show your systematic debugging approach.",
            "Own mistakes immediately \u2014 'that won't work because...' signals strong technical judgment.",
            "Behavioral: have exactly one story of failure ready \u2014 it shows self-awareness and growth."
        ],
        "Tasks": [
            "<strong>Final Review Sprint (1 hour):</strong> Window Functions cheat sheet \u2192 Gaps & Islands pattern \u2192 SCD Type 2 MERGE \u2192 Parquet internals \u2192 Kafka partition design \u2192 Spark Catalyst phases.",
            "<strong>Behavioral dry-run (30 min):</strong> Say each STAR story aloud \u2014 time each one at under 3 minutes.",
            "<strong>System design sketch (30 min):</strong> Draw the batch ETL and streaming architectures from memory.",
            "<strong>Mindset (rest of day):</strong> Close the books. Walk, exercise, eat well. You have done the work. Trust your preparation."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Perfect the Tell me about yourself pitch",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Perfect the Tell me about yourself pitch \u2014 trace through 2-3 edge cases before writing any code.",
                "Have a glass of water with you \u2014 cognitive performance degrades with even mild dehydration.",
                "If nerves strike: box breathing (4-4-4-4: inhale-hold-exhale-hold) before starting.",
                "Read the problem twice before touching the keyboard \u2014 rushing on easy problems is the #1 mistake.",
                "End the interview by asking a thoughtful question: 'What does the data engineering roadmap look like for this team?'"
            ]
        },
        "HardProblem": "Final Reflection: Write down the 3 concepts you feel LEAST confident about. For each one, write a 3-sentence explanation of it as if you are teaching it to a junior engineer. If you can explain it simply and accurately in 3 sentences, you understand it at interview level. If you can't, spend 30 minutes on just that one topic."
    },
    {
        "Week": 13,
        "Day": "Friday",
        "Phase": "3. Architect (System)",
        "Theme": "Behavioral",
        "SpecificTopic": "Mock Interview",
        "ActionItem_Deliverable": "Full Loop (Coding + System + Behavioral)",
        "LeetCodeProblem": "<strong>LC 200 \u2013 Number of Islands (speed run)</strong> \u2014 Analyze Big-O Time &amp; Space before starting.",
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>S3 Partitioning</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson\">\n<h3>\ud83c\udfc1 Hour 1 \u2014 Mental Preparation: The Final 48 Hours</h3>\n<p>The night before and day of the interview, technical cramming hurts more than it helps. Your goal is to show up mentally sharp, confident, and in peak cognitive state. Here's the science-backed protocol.</p>\n<h4>48 Hours Before: Light Review Mode</h4>\n<ul>\n  <li>Review only your personal notes \u2014 not new material</li>\n  <li>Re-read your 5 behavioral stories once</li>\n  <li>Draw your best system design (1-2 drawings) from memory</li>\n  <li>No LeetCode grinding \u2014 it increases anxiety without improving performance</li>\n</ul>\n<h4>24 Hours Before: Reset Mode</h4>\n<ul>\n  <li>Exercise: 30-45 min moderate cardio \u2014 increases BDNF, sharpens cognition next day</li>\n  <li>Sleep 8 hours \u2014 memory consolidation happens during deep sleep</li>\n  <li>Avoid alcohol \u2014 disrupts REM sleep and slow-wave sleep</li>\n  <li>Prepare logistics: confirm interview time/format, test Zoom/CoderPad link</li>\n</ul>\n<h4>Day Of: Performance Mode</h4>\n<pre>Morning:\n- Eat a real breakfast (glucose is brain fuel)\n- 20-min light walk or stretch\n- Read your notes ONCE \u2014 then put them away\n\nIn the interview:\n- Think aloud: \"Let me make sure I understand the problem...\"\n- Clarify before coding: \"Can I assume the data fits in memory?\"\n- Show your work: narrate every decision (\"I'm using DENSE_RANK because there should be no gaps...\")\n- Stuck? Say it: \"Let me think through the edge cases...\" (silence is NOT a signal of failure)\n- Wrong answer? Own it: \"That approach has a flaw \u2014 let me reconsider...\"</pre>\n<h4>The Interview Mindset Reframe</h4>\n<p>You are not being tested \u2014 you are having a technical conversation with a peer. The interviewer WANTS you to succeed (hiring is expensive and painful). They are rooting for you to show your best self. Reframe nerves as excitement.</p>\n</div>",
        "KeyConcepts": [
            "No new material 48 hours before the interview \u2014 only review of existing notes.",
            "Sleep is more valuable than last-minute studying \u2014 memory consolidation happens during sleep.",
            "Think aloud during technical questions \u2014 interviewers need to see your reasoning process.",
            "Clarify before coding \u2014 this shows senior-level professionalism, not weakness.",
            "Being stuck is normal and expected \u2014 show your systematic debugging approach.",
            "Own mistakes immediately \u2014 'that won't work because...' signals strong technical judgment.",
            "Behavioral: have exactly one story of failure ready \u2014 it shows self-awareness and growth."
        ],
        "Tasks": [
            "<strong>Final Review Sprint (1 hour):</strong> Window Functions cheat sheet \u2192 Gaps & Islands pattern \u2192 SCD Type 2 MERGE \u2192 Parquet internals \u2192 Kafka partition design \u2192 Spark Catalyst phases.",
            "<strong>Behavioral dry-run (30 min):</strong> Say each STAR story aloud \u2014 time each one at under 3 minutes.",
            "<strong>System design sketch (30 min):</strong> Draw the batch ETL and streaming architectures from memory.",
            "<strong>Mindset (rest of day):</strong> Close the books. Walk, exercise, eat well. You have done the work. Trust your preparation."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Full Loop (Coding + System + Behavioral)",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Full Loop (Coding + System + Behavioral) \u2014 trace through 2-3 edge cases before writing any code.",
                "Have a glass of water with you \u2014 cognitive performance degrades with even mild dehydration.",
                "If nerves strike: box breathing (4-4-4-4: inhale-hold-exhale-hold) before starting.",
                "Read the problem twice before touching the keyboard \u2014 rushing on easy problems is the #1 mistake.",
                "End the interview by asking a thoughtful question: 'What does the data engineering roadmap look like for this team?'"
            ]
        },
        "HardProblem": "Final Reflection: Write down the 3 concepts you feel LEAST confident about. For each one, write a 3-sentence explanation of it as if you are teaching it to a junior engineer. If you can explain it simply and accurately in 3 sentences, you understand it at interview level. If you can't, spend 30 minutes on just that one topic."
    },
    {
        "Week": 13,
        "Day": "Saturday",
        "Phase": "3. Architect (System)",
        "Theme": "Behavioral",
        "SpecificTopic": "Mental Prep",
        "ActionItem_Deliverable": "Review high-level concepts (No heavy code)",
        "LeetCodeProblem": null,
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Mock Assessment</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson\">\n<h3>\ud83c\udfc1 Hour 1 \u2014 Mental Preparation: The Final 48 Hours</h3>\n<p>The night before and day of the interview, technical cramming hurts more than it helps. Your goal is to show up mentally sharp, confident, and in peak cognitive state. Here's the science-backed protocol.</p>\n<h4>48 Hours Before: Light Review Mode</h4>\n<ul>\n  <li>Review only your personal notes \u2014 not new material</li>\n  <li>Re-read your 5 behavioral stories once</li>\n  <li>Draw your best system design (1-2 drawings) from memory</li>\n  <li>No LeetCode grinding \u2014 it increases anxiety without improving performance</li>\n</ul>\n<h4>24 Hours Before: Reset Mode</h4>\n<ul>\n  <li>Exercise: 30-45 min moderate cardio \u2014 increases BDNF, sharpens cognition next day</li>\n  <li>Sleep 8 hours \u2014 memory consolidation happens during deep sleep</li>\n  <li>Avoid alcohol \u2014 disrupts REM sleep and slow-wave sleep</li>\n  <li>Prepare logistics: confirm interview time/format, test Zoom/CoderPad link</li>\n</ul>\n<h4>Day Of: Performance Mode</h4>\n<pre>Morning:\n- Eat a real breakfast (glucose is brain fuel)\n- 20-min light walk or stretch\n- Read your notes ONCE \u2014 then put them away\n\nIn the interview:\n- Think aloud: \"Let me make sure I understand the problem...\"\n- Clarify before coding: \"Can I assume the data fits in memory?\"\n- Show your work: narrate every decision (\"I'm using DENSE_RANK because there should be no gaps...\")\n- Stuck? Say it: \"Let me think through the edge cases...\" (silence is NOT a signal of failure)\n- Wrong answer? Own it: \"That approach has a flaw \u2014 let me reconsider...\"</pre>\n<h4>The Interview Mindset Reframe</h4>\n<p>You are not being tested \u2014 you are having a technical conversation with a peer. The interviewer WANTS you to succeed (hiring is expensive and painful). They are rooting for you to show your best self. Reframe nerves as excitement.</p>\n</div>",
        "KeyConcepts": [
            "No new material 48 hours before the interview \u2014 only review of existing notes.",
            "Sleep is more valuable than last-minute studying \u2014 memory consolidation happens during sleep.",
            "Think aloud during technical questions \u2014 interviewers need to see your reasoning process.",
            "Clarify before coding \u2014 this shows senior-level professionalism, not weakness.",
            "Being stuck is normal and expected \u2014 show your systematic debugging approach.",
            "Own mistakes immediately \u2014 'that won't work because...' signals strong technical judgment.",
            "Behavioral: have exactly one story of failure ready \u2014 it shows self-awareness and growth."
        ],
        "Tasks": [
            "<strong>Final Review Sprint (1 hour):</strong> Window Functions cheat sheet \u2192 Gaps & Islands pattern \u2192 SCD Type 2 MERGE \u2192 Parquet internals \u2192 Kafka partition design \u2192 Spark Catalyst phases.",
            "<strong>Behavioral dry-run (30 min):</strong> Say each STAR story aloud \u2014 time each one at under 3 minutes.",
            "<strong>System design sketch (30 min):</strong> Draw the batch ETL and streaming architectures from memory.",
            "<strong>Mindset (rest of day):</strong> Close the books. Walk, exercise, eat well. You have done the work. Trust your preparation."
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Review high-level concepts (No heavy code)",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Review high-level concepts (No heavy code) \u2014 trace through 2-3 edge cases before writing any code.",
                "Have a glass of water with you \u2014 cognitive performance degrades with even mild dehydration.",
                "If nerves strike: box breathing (4-4-4-4: inhale-hold-exhale-hold) before starting.",
                "Read the problem twice before touching the keyboard \u2014 rushing on easy problems is the #1 mistake.",
                "End the interview by asking a thoughtful question: 'What does the data engineering roadmap look like for this team?'"
            ]
        },
        "HardProblem": "Final Reflection: Write down the 3 concepts you feel LEAST confident about. For each one, write a 3-sentence explanation of it as if you are teaching it to a junior engineer. If you can explain it simply and accurately in 3 sentences, you understand it at interview level. If you can't, spend 30 minutes on just that one topic."
    },
    {
        "Week": 13,
        "Day": "Sunday",
        "Phase": "3. Architect (System)",
        "Theme": "Execution",
        "SpecificTopic": "READY",
        "ActionItem_Deliverable": "Go get the offer",
        "LeetCodeProblem": null,
        "Status": null,
        "Notes": null,
        "Warmup": "<strong>\u26a1 Warm-Up Review:</strong> Explain <em>Governance</em> \u2014 describe its biggest performance tradeoff in 3 sentences, from memory, without notes.",
        "Basics": "<div class=\"lesson\">\n<h3>\ud83d\ude80 Day 91 \u2014 You Are Ready. Go Get the Offer.</h3>\n<p>13 weeks of deliberate practice. 91 days of consistency. You have covered more depth and breadth across data engineering than the vast majority of candidates who walk into FAANG interviews. This is your final checkpoint.</p>\n<h4>The Complete FAANG DE Knowledge Map You Now Own</h4>\n<pre>\u2705 SQL Analytics (Window Functions, Gaps & Islands, CTEs, Execution Plans, Indexes, Joins, NULL)\n\u2705 Data Modeling (Star Schema, SCD Types, Normalization, NoSQL, Kimball Methodology)\n\u2705 Python Logic (Hash Maps, Generators, Decorators, Memory, File I/O, Algorithms)\n\u2705 Storage Internals (Parquet, Avro, Compression, S3 Partitioning, Delta Lake, Iceberg)\n\u2705 Spark (Catalyst, Shuffle, Partitions, Broadcast Join, Memory, Skew, OOM)\n\u2705 Orchestration (Airflow DAG, Scheduler, XCom, Backfill, Dynamic DAGs)\n\u2705 Streaming (Kafka, Pub/Sub, Consumer Groups, Exactly-Once, Windowing)\n\u2705 System Design (Batch ETL, CDC, Streaming Architecture, Back-of-Envelope)\n\u2705 Quality (Data Contracts, dbt Tests, Observability, Idempotency, Schema Evolution)\n\u2705 Behavioral (STAR Method, Amazon LPs, Conflict, Failure, Innovation stories)</pre>\n<h4>Your Interview Day Execution Checklist</h4>\n<pre>Coding Round:\n\u25a1 Read the problem twice before touching the keyboard\n\u25a1 Clarify: NULLs? Duplicates? Result format? Scale?\n\u25a1 Write 3-5 rows of test data on paper/whiteboard\n\u25a1 CTE-first for SQL; function-first for Python\n\u25a1 State complexity: \"This is O(N log N) because...\"\n\u25a1 Check edge cases after writing solution\n\nSystem Design Round:\n\u25a1 Clarify requirements and scale first\n\u25a1 Back-of-envelope estimation before architecture\n\u25a1 Draw from source to consumer (left to right on whiteboard)\n\u25a1 Propose AND explain tradeoffs of each choice\n\u25a1 Mention observability, failure modes, and costs\n\nBehavioral Round:\n\u25a1 STAR format for every answer\n\u25a1 Quantified results in every answer\n\u25a1 Use \"I\" not \"we\"\n\u25a1 End with \"What I learned was...\"</pre>\n<h4>Final Words</h4>\n<p>You did not just prepare for an interview. You became a more capable, more knowledgeable data engineer. Whatever happens in the interview, that knowledge stays with you. Now go get what you worked for. \ud83d\udcaa</p>\n</div>",
        "KeyConcepts": [
            "You have completed a comprehensive, expert-level FAANG Data Engineering curriculum.",
            "The knowledge you built is permanent \u2014 it serves you in the interview and every day after.",
            "Interviews test a sample of your knowledge under pressure \u2014 they are not comprehensive exams.",
            "Confidence comes from preparation \u2014 you have done the preparation.",
            "Technical depth + communication + systematic thinking = the FAANG DE trifecta.",
            "Failure in one interview is data, not a verdict \u2014 adjust and try again.",
            "The best data engineers never stop learning \u2014 this curriculum is a foundation, not a ceiling."
        ],
        "Tasks": [
            "Complete the Day 91 knowledge map audit \u2014 check off every topic you feel confident on.",
            "Run through your STAR stories one final time.",
            "Get 8 hours of sleep.",
            "Go get the offer. \ud83d\ude80"
        ],
        "PracticeProblem": {
            "problem": "<strong>\ud83d\udccb Daily Objective:</strong> Go get the offer",
            "hints": [
                "<strong>\ud83c\udfaf Daily Action Focus:</strong> Go get the offer \u2014 trace through 2-3 edge cases before writing any code.",
                "Day of interview: review nothing new. Eat, exercise, breathe. Trust your preparation.",
                "In the interview: slow down. Rushing is the #1 mistake of over-prepared candidates.",
                "If you get a question you don't know: say so honestly, then reason through it out loud.",
                "After each interview: note every question asked \u2014 it helps when you interview again if needed."
            ]
        },
        "HardProblem": "The Ultimate Boss Problem: You are given 45 minutes to design a real-time ride-sharing analytics platform (like Uber). Inputs: driver location updates (100K/min), ride requests (10K/min), trip completions (5K/min). Outputs: live surge pricing per geo-cell (updated every 30 seconds), driver earnings dashboard (updated every minute), daily aggregate reports (by 6am). Design the full system: ingestion, processing, storage, serving, and monitoring. Include technology choices with justifications, failure modes, and cost estimate at 10x scale."
    }
]