def L(n, emoji, title, body):
    return f'<div class="level level-{n}"><div class="level-badge">{emoji} Level {n} â€” {title}</div><div class="rich">{body}</div></div>'

WEEKS4_TO_7 = {

    # hash_maps
    'hash_maps': {
        'basics': '<div class="lesson-levels"><div class="level level-1"><div class="level-badge">ğŸŸ¢ Level 1 â€” Why These Structures Matter in DE</div><div class="rich"><h4>Why Python Data Engineers Need These Structures</h4><p>SQL handles sets of rows. Python handles the transformations, pipelines, and logic that SQL cannot: parsing nested JSON, building lookups between datasets, deduplicating records, implementing custom aggregations, streaming transformations, and interview-style algorithmic challenges.</p><p>Week 4 is about the Python data structures and patterns that show up constantly in data engineering interviews at FAANG â€” hash maps, sets, generators, sliding windows, and heaps. These are NOT just LeetCode prep. They solve real data problems every day.</p><h4>Hash Maps (Dictionaries) â€” The Single Most Important Python Structure</h4><p>A hash map (Python: <code>dict</code>) stores key-value pairs and provides O(1) average case lookup. Internally, Python\'s dict uses a hash table: it hashes the key, computes an array index, and stores the value there. This means finding any key requires computing one hash function â€” NOT scanning 10M items. This is the fundamental property that makes hash maps so powerful.</p><pre># Hash map lookup time does not grow with size\nd = {}  # empty dict â€” takes up ~200 bytes\nfor i in range(10_000_000):\n    d[i] = i * 2  # 10 million entries\n\n# This lookup is STILL O(1) â€” same speed as the empty dict\nprint(d[9_999_999])  # instant\n\n# Compare to a list:\nlst = list(range(10_000_000))\n9_999_999 in lst  # O(n) â€” has to scan all 10M items\nlst.index(9_999_999)  # O(n) â€” same\n\n# Rule: if you find yourself doing "x in my_list" in a loop, use a set or dict instead.</pre><h4>The Three Core Hash Map Patterns in Data Engineering</h4><ul><li><strong>Frequency counting</strong>: count occurrences of any key in O(n)</li><li><strong>Grouping / bucketing</strong>: group rows by a key without sorting</li><li><strong>Two-pass lookup</strong>: build a lookup table in pass 1, answer queries in pass 2</li></ul></div></div><div class="level level-2"><div class="level-badge">ğŸ”µ Level 2 â€” Hash Map Patterns â€” Counter, Group, Join</div><div class="rich"><h4>Pattern 1: Frequency Counting â€” The Counter Pattern</h4><p><code>collections.Counter</code> is a specialized dict for counting. It is the right tool for any "how many times does X appear?" question.</p><pre>from collections import Counter\n\n# Raw data: a stream of user events\nevents = ["click","scroll","click","purchase","click","scroll","purchase"]\n\n# Count frequencies in O(n)\nfreq = Counter(events)\n# Counter({"click": 3, "scroll": 2, "purchase": 2})\n\n# Most common events\ntop_3 = freq.most_common(3)  # [("click",3), ("scroll",2), ("purchase",2)]\n\n# In data engineering: count most frequent API error codes\nerror_logs = ["404","500","404","503","404","500","404"]\nerror_counts = Counter(error_logs)\nprint(error_counts.most_common(2))  # [(\'404\', 4), (\'500\', 2)]\n\n# Interview pattern: does any value appear more than N/2 times?\ndef majority_element(nums):\n    c = Counter(nums)\n    return max(c, key=c.get)  # element with highest count</pre><h4>Pattern 2: Grouping â€” defaultdict</h4><pre>from collections import defaultdict\n\n# Group transactions by customer_id\ntransactions = [\n    {"cust": 42, "amount": 100},\n    {"cust": 55, "amount": 200},\n    {"cust": 42, "amount": 50},\n]\n\nby_customer = defaultdict(list)  # default value is an empty list\nfor txn in transactions:\n    by_customer[txn["cust"]].append(txn["amount"])\n\n# Result: {42: [100, 50], 55: [200]}\n\n# Now compute per-customer totals in O(n) total:\ntotals = {cust: sum(amounts) for cust, amounts in by_customer.items()}</pre><h4>Pattern 3: Two-Pass Lookup â€” Find Complement</h4><pre># Classic interview: Two Sum â€” find pair that sums to target\n# Brute force: O(n^2) â€” check every pair\n# Hash map approach: O(n) â€” one pass\n\ndef two_sum(nums, target):\n    seen = {}  # value â†’ index\n    for i, num in enumerate(nums):\n        complement = target - num\n        if complement in seen:   # O(1) lookup!\n            return [seen[complement], i]\n        seen[num] = i\n    return []\n\n# Data engineering version: join two datasets in memory\n# (when SQL JOIN is not available, e.g. in a Python streaming pipeline)\ndef hash_join(left_rows, right_rows):\n    # Pass 1: build lookup from right table\n    lookup = {row["id"]: row for row in right_rows}  # O(n)\n\n    # Pass 2: look up each left row in O(1)\n    for left in left_rows:  # O(m)\n        right = lookup.get(left["right_id"])\n        if right:\n            yield {**left, **right}  # merged row\n    # Total: O(n + m) instead of O(n * m)</pre></div></div><div class="level level-3"><div class="level-badge">ğŸŸ¡ Level 3 â€” Sets + Sliding Window Technique</div><div class="rich"><h4>Sets â€” O(1) Membership and Deduplication</h4><p>A Python <code>set</code> is essentially a dict with only keys (no values). It provides O(1) membership testing and deduplication. In data engineering: dedup large record streams, compute intersection/difference of ID lists.</p><pre># Deduplication â€” preserve only unique records\nuser_ids = [1, 2, 3, 2, 4, 1, 5, 3]\nunique_ids = list(set(user_ids))  # [1, 2, 3, 4, 5] â€” order not preserved!\n\n# If order matters: use dict.fromkeys()\nunique_ordered = list(dict.fromkeys(user_ids))  # [1, 2, 3, 4, 5] â€” order preserved\n\n# Set operations for data reconciliation:\ndb_users = {1, 2, 3, 4, 5}\napp_users = {3, 4, 5, 6, 7}\n\nonly_in_db  = db_users - app_users   # {1, 2}    â€” in DB but not app\nonly_in_app = app_users - db_users   # {6, 7}    â€” in app but not DB\nin_both     = db_users & app_users   # {3, 4, 5} â€” intersection\nin_either   = db_users | app_users   # {1,2,3,4,5,6,7} â€” union\n\n# DE use case: find orders with no matching customer (data quality check)\norder_customer_ids = {o["cust_id"] for o in orders}\ncustomer_ids       = {c["id"] for c in customers}\norphan_orders = order_customer_ids - customer_ids  # cust IDs with no record</pre><h4>Sliding Window â€” Fixed and Variable Width</h4><p>Sliding window is NOT a data structure â€” it\'s an algorithmic technique. Instead of computing an aggregate for every possible window from scratch (O(nÃ—k)), you maintain a running state as the window slides, re-using previous computation: O(n).</p><pre># Fixed window: 3-day rolling sum\ndef rolling_sum(values, k):\n    window_sum = sum(values[:k])  # compute first window\n    results = [window_sum]\n    for i in range(k, len(values)):\n        window_sum += values[i]       # add new element\n        window_sum -= values[i - k]   # remove oldest element\n        results.append(window_sum)\n    return results\n\n# Variable window: longest subarray with sum <= budget\ndef longest_under_budget(costs, budget):\n    left = 0\n    current_sum = 0\n    max_len = 0\n    for right in range(len(costs)):\n        current_sum += costs[right]\n        while current_sum > budget:   # shrink window from left\n            current_sum -= costs[left]\n            left += 1\n        max_len = max(max_len, right - left + 1)\n    return max_len</pre></div></div><div class="level level-4"><div class="level-badge">ğŸ”´ Level 4 â€” Heaps + Complexity Cheat Sheet</div><div class="rich"><h4>Heaps â€” Priority Queues for Streaming Top-K</h4><p>A <strong>heap</strong> (Python: <code>heapq</code>) is a binary tree that always keeps the smallest element at the root. It provides O(log n) insert and O(log n) extract-min. This makes it ideal for "top K" problems in streaming data â€” maintain a heap of exactly K items, each incoming item either replaces the smallest or is discarded.</p><pre>import heapq\n\n# Find top-3 highest-revenue customers from a stream of millions\n# No need to sort 10M records â€” O(n log k) with k=3\ndef top_k_customers(stream, k):\n    heap = []  # min-heap of (revenue, cust_id)\n    for cust_id, revenue in stream:\n        heapq.heappush(heap, (revenue, cust_id))  # add to heap\n        if len(heap) > k:\n            heapq.heappop(heap)  # remove smallest â€” keeps top K\n    return sorted(heap, reverse=True)  # highest first\n\n# heapq.nlargest is the Pythonic version:\ntop_3 = heapq.nlargest(3, stream, key=lambda x: x[1])\n\n# Merge K sorted streams (very common in data engineering)\n# e.g., merge sorted log files from 10 servers\ndef merge_k_sorted(sorted_streams):\n    heap = []\n    iterators = [iter(s) for s in sorted_streams]\n    for i, it in enumerate(iterators):\n        val = next(it, None)\n        if val is not None:\n            heapq.heappush(heap, (val, i))  # (value, stream_index)\n    while heap:\n        val, i = heapq.heappop(heap)\n        yield val\n        nxt = next(iterators[i], None)\n        if nxt is not None:\n            heapq.heappush(heap, (nxt, i))</pre><p><strong>Complexity cheat sheet for FAANG interviews:</strong></p><table><tr><th>Structure</th><td>Insert</td><td>Lookup</td><td>Delete</td><td>Best for</td></tr><tr><th>dict</th><td>O(1)</td><td>O(1)</td><td>O(1)</td><td>Counting, grouping, two-pointer complement</td></tr><tr><th>set</th><td>O(1)</td><td>O(1)</td><td>O(1)</td><td>Dedup, membership, set math</td></tr><tr><th>list</th><td>O(1) append</td><td>O(n)</td><td>O(n)</td><td>Ordered sequence, indexed access</td></tr><tr><th>heapq</th><td>O(log n)</td><td>O(1) min</td><td>O(log n)</td><td>Top-K, streaming priority</td></tr><tr><th>deque</th><td>O(1) both ends</td><td>O(n)</td><td>O(1) ends</td><td>Sliding window, BFS queue</td></tr></table></div></div></div>',
        'key_concepts': ['dict provides O(1) average insert, lookup, delete. Uses hash table internally.', 'Counter: frequency counting in O(n). most_common(k) returns top-k elements.', 'defaultdict: auto-initializes missing keys â€” ideal for grouping records by key.', 'Two-pass hash join: build lookup dict in O(n), query in O(m). Total O(n+m) vs O(n*m) nested loop.', 'set: O(1) membership. Supports union | , intersection &, difference -. Use for dedup + set math.', 'dict.fromkeys(list): deduplicate while preserving insertion order (unlike set()).', 'Sliding window: maintain running state instead of recomputing from scratch. O(n) vs O(n*k).', 'heapq: min-heap. Push/pop in O(log n). Use for top-K streaming, merge K sorted sequences.'],
        'hints': ["Seeing 'x in my_list' inside a loop? That's O(nÂ²). Convert to set first: O(n) total.", 'Counter(iterable) counts in one line. Combine two counters: c1 + c2 adds counts, c1 & c2 keeps minimums.', 'heapq is a MIN-heap. For max-heap: store values as negatives (-val, key) then negate on extraction.', 'Sliding window: two-pointer left/right. Move right always; move left only when constraint violated.'],
        'tasks': ['<strong>Step 1:</strong> Given a list of 1M user_ids with duplicates, find the top-5 most frequent users using Counter. Then dedup the list preserving order using dict.fromkeys().', '<strong>Step 2:</strong> Implement a hash join: two lists of dicts (orders and customers) sharing a customer_id field. Merge them in O(n+m) using a dict lookup.', '<strong>Step 3:</strong> Sliding window: find the max sum of any 3 consecutive days from 365 daily revenue values. Implement in O(n) â€” no nested loop.', '<strong>Step 4:</strong> Top-K streaming: simulate a stream of (user_id, purchase_amount) tuples. Find the top-10 spenders using heapq without storing the entire stream in memory.'],
        'hard_problem': 'Boss Problem (Google): You receive a stream of (user_id, search_query, timestamp) events, 100M events/day. (1) Find the top-100 most searched queries in the last 24 hours â€” you cannot load all 100M events into RAM. Use a heap + streaming approach. (2) Find all users who searched for the same query more than 5 times â€” are they bots? Use Counter + filter. (3) Find all pairs of users who searched for the SAME rare query (appears < 10 times globally). How do you efficiently match pairs? (4) Time complexity analysis: what is the big-O of each part?',
    },

    # generators
    'generators': {
        'basics': '<div class="lesson-levels"><div class="level level-1"><div class="level-badge">ğŸŸ¢ Level 1 â€” Generators â€” The Memory Problem</div><div class="rich"><h4>The Memory Problem â€” Why Generators Exist</h4><p>Imagine you need to process a 100GB log file. The naive approach: <code>lines = open("log.txt").readlines()</code>. This reads all 100GB into RAM at once. If you only have 8GB of RAM, your process crashes. Even if it doesn\'t crash, you\'ve consumed 100GB of memory for a computation that might only need 100MB at any one time.</p><p>This is the fundamental problem generators solve: <strong>lazy evaluation</strong>. Instead of computing all values upfront and storing them in memory, a generator computes the NEXT value only when asked. It uses O(1) memory regardless of the total sequence size.</p><h4>Generator Functions â€” The yield Keyword</h4><pre># Regular function: returns a list (loads everything into memory)\ndef read_all_lines(filename):\n    return open(filename).readlines()  # 100GB in RAM if large file!\n\n# Generator function: yields one line at a time (O(1) memory)\ndef stream_lines(filename):\n    with open(filename) as f:\n        for line in f:          # f iterates line by line â€” OS handles this\n            yield line.strip()  # pause here, return line, resume on next()\n\n# Usage: process 100GB file with 8GB RAM â€” works perfectly\nfor line in stream_lines("huge_log.txt"):\n    process(line)  # only one line lives in memory at any time\n\n# A generator is a function that remembers WHERE IT WAS in execution.\n# "yield" means: return this value, pause here, resume from here next time.</pre><h4>Generator Expressions â€” The Lazy List Comprehension</h4><pre># List comprehension: creates all values in memory immediately\nsquares_list = [x**2 for x in range(10_000_000)]   # 80MB in memory\n\n# Generator expression: creates values on demand, same syntax with ()\nsquares_gen  = (x**2 for x in range(10_000_000))    # tiny (generator object)\n\n# Both work identically in a for loop:\ntotal = sum(x**2 for x in range(10_000_000))  # never stores all 10M values!\n\n# Rule for data engineering: use generator expression whenever you will\n# iterate through results ONCE (sum, filter, write to file, stream out).\n# Use list when you need to: index into results, iterate multiple times, or check len().</pre></div></div><div class="level level-2"><div class="level-badge">ğŸ”µ Level 2 â€” ETL Pipeline with Generator Chaining</div><div class="rich"><h4>Real-World DE Pattern: ETL Pipeline with Generators</h4><p>The most powerful use of generators in data engineering: chaining them into a pipeline. Each stage of the ETL is a generator that reads from the previous stage. The entire pipeline runs with O(1) memory â€” data flows through one record at a time.</p><pre># ETL Pipeline: read â†’ parse â†’ filter â†’ transform â†’ write\n# Each function is a generator. Data flows record-by-record.\n\ndef read_csv(filepath):\n    """Stage 1: Read file line by line."""\n    with open(filepath) as f:\n        next(f)  # skip header\n        for line in f:\n            yield line.strip()\n\ndef parse_rows(lines):\n    """Stage 2: Parse CSV text into dicts."""\n    for line in lines:  # lines is itself a generator!\n        parts = line.split(",")\n        yield {"user_id": parts[0], "amount": float(parts[1]), "date": parts[2]}\n\ndef filter_active(rows):\n    """Stage 3: Keep only recent rows."""\n    cutoff = "2024-01-01"\n    for row in rows:\n        if row["date"] >= cutoff:\n            yield row\n\ndef enrich(rows, lookup):\n    """Stage 4: Add metadata from lookup table."""\n    for row in rows:\n        row["region"] = lookup.get(row["user_id"], "unknown")\n        yield row\n\n# Chain: data flows through all stages lazily, one record at a time\nlookup = build_region_lookup()  # a dict â€” O(m) memory\npipeline = enrich(\n    filter_active(\n        parse_rows(\n            read_csv("events.csv")  # 10GB file\n        )\n    ),\n    lookup\n)\n\nfor record in pipeline:  # triggers execution â€” pulls one record at a time\n    write_to_warehouse(record)\n# Total memory: O(1) for the pipeline + O(m) for the lookup dict</pre></div></div><div class="level level-3"><div class="level-badge">ğŸŸ¡ Level 3 â€” Decorators + Context Managers</div><div class="rich"><h4>Decorators â€” Functions That Wrap Functions</h4><p>A <strong>decorator</strong> is a function that takes another function as input, wraps it with additional behavior, and returns the wrapped version. The <code>@</code> syntax is shorthand for <code>func = decorator(func)</code>.</p><p>In data engineering: decorators handle retry logic, caching, metrics, logging, and access control â€” without cluttering the core business logic.</p><pre>import time\nimport functools\n\n# Decorator: retry an operation up to N times on failure\ndef retry(max_attempts=3, delay=1.0):\n    def decorator(func):\n        @functools.wraps(func)  # preserve original function metadata\n        def wrapper(*args, **kwargs):\n            for attempt in range(1, max_attempts + 1):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    if attempt == max_attempts:\n                        raise  # re-raise on final attempt\n                    print(f"Attempt {attempt} failed: {e}. Retrying...")\n                    time.sleep(delay)\n        return wrapper\n    return decorator\n\n# Usage: apply retry behavior to any function with one line\n@retry(max_attempts=5, delay=2.0)\ndef fetch_from_api(endpoint):\n    """Fetches data from a flaky external API."""\n    response = requests.get(endpoint)\n    response.raise_for_status()\n    return response.json()\n\n# Equivalent to: fetch_from_api = retry(5, 2.0)(fetch_from_api)</pre><h4>Context Managers â€” Guaranteed Resource Cleanup</h4><pre># Context manager: database connection that always closes\nfrom contextlib import contextmanager\n\n@contextmanager\ndef db_connection(conn_string):\n    conn = create_connection(conn_string)\n    try:\n        yield conn          # caller gets the connection here\n    finally:\n        conn.close()        # ALWAYS runs, even if exception occurs\n\n# Usage: connection guaranteed to close even on exception\nwith db_connection("postgresql://...") as conn:\n    conn.execute("INSERT ...")</pre></div></div><div class="level level-4"><div class="level-badge">ğŸ”´ Level 4 â€” Memory Profiling + 500GB Pipeline</div><div class="rich"><h4>Memory Profiling and Optimization Strategies</h4><p>In production data engineering, understanding memory at a deep level prevents outages. The three main memory killers in Python DE pipelines:</p><ul><li><strong>Loading entire files/datasets into lists</strong>: Replace with generators + streaming</li><li><strong>Multiple copies of the same string</strong>: Python interns short strings but not long ones â€” use string interning or category dtypes in Pandas</li><li><strong>DataFrames with wrong dtypes</strong>: a DataFrame with int64 columns that hold values 0-100 uses 8x more memory than int8</li></ul><pre># Memory profiling with memory_profiler\n# pip install memory-profiler\nfrom memory_profiler import profile\n\n@profile\ndef load_and_process():\n    data = pd.read_csv("large.csv")     # baseline memory\n    # LINE 1: 2.0 GiB\n    data = optimize_dtypes(data)         # shrink dtypes\n    # LINE 2: 0.5 GiB â€” 75% reduction!\n    return data\n\n# Dtype optimization:\ndef optimize_dtypes(df):\n    for col in df.select_dtypes("int64").columns:\n        df[col] = pd.to_numeric(df[col], downcast="integer")\n    for col in df.select_dtypes("float64").columns:\n        df[col] = pd.to_numeric(df[col], downcast="float")\n    for col in df.select_dtypes("object").columns:\n        if df[col].nunique() / len(df) < 0.5:  # <50% unique â†’ category\n            df[col] = df[col].astype("category")\n    return df\n\n# Processing chunks: alternative to generators for files too large for memory\nfor chunk in pd.read_csv("huge.csv", chunksize=100_000):\n    process(chunk)  # process 100K rows at a time</pre><p><strong>FAANG interview question:</strong> "Your pipeline processes 500GB of logs daily. Memory on your EC2 instance is 32GB. Walk me through how you would process this." Correct answer: generators + chunked reading + streaming writes, never load the full dataset simultaneously.</p></div></div></div>',
        'key_concepts': ["Generator function: uses 'yield' to pause/resume. Returns a generator object, not a list.", 'Generator uses O(1) memory: it never holds all values at once â€” computes next value on demand.', 'Generator expression: (expr for x in iterable) â€” identical syntax to list comp but lazy.', 'Pipeline chaining: each ETL stage is a generator reading from the previous. Data flows one record at a time.', "Decorator: higher-order function that wraps another. '@retry' adds retry logic without changing core code.", '@functools.wraps preserves function name/docstring when writing decorators.', "Context manager: 'with' block guarantees cleanup (finally) even on exceptions.", 'Memory optimization: dtype downcast (int64â†’int8), category dtype for low-cardinality strings.'],
        'hints': ['Can you iterate through results with a single for loop? â†’ use generator (saves memory).', 'Need to index into results, get len(), or iterate multiple times? â†’ convert to list first.', "Chunked pandas: pd.read_csv('file.csv', chunksize=100_000) processes file in chunks without full load.", 'Generators are lazy â€” they do NOTHING until iterated. Chaining generators costs zero compute up front.'],
        'tasks': ['<strong>Step 1:</strong> Write a generator that reads a CSV file line by line and yields parsed dict rows. Compare memory usage vs reading the full file into a list.', "<strong>Step 2:</strong> Build a 3-stage generator pipeline: read â†’ filter (only rows where amount > 100) â†’ transform (add a 'revenue_tier' field). Process a 1M-row file.", '<strong>Step 3:</strong> Write a @retry(max_attempts=3, delay=1) decorator. Apply it to a function that simulates random network failures. Verify it retries correctly.', '<strong>Step 4:</strong> Write optimize_dtypes(df). Apply it to a DataFrame with int64 and object columns. Report memory usage before and after with df.memory_usage(deep=True).sum().'],
        'hard_problem': 'Boss Problem (Netflix): Your nightly ETL reads from an S3 bucket containing 50,000 gzipped JSON files (total ~500GB uncompressed). Each file has user viewing events. (1) Write a generator pipeline: stream S3 object list â†’ download each file â†’ gunzip â†’ parse JSON lines â†’ filter by date â†’ yield events. The pipeline should use <2GB RAM regardless of total file count. (2) For each event, do a dict lookup against a 20M-row user metadata table loaded in RAM. What is the maximum size this lookup dict can be? (3) If processing takes 14 hours, how do you parallelize using multiprocessing.Pool while keeping memory bounded?',
    },

    # row_vs_columnar
    'row_vs_columnar': {
        'basics': '<div class="lesson-levels"><div class="level level-1"><div class="level-badge">ğŸŸ¢ Level 1 â€” Row vs Columnar â€” The Layout Trade-Off</div><div class="rich"><h4>The Fundamental Trade-Off in Storage Layout</h4><p>When you store data on disk, you have a fundamental choice: store all columns of one row together (row-oriented) or store all values of one column together (column-oriented). This decision affects compression ratios, query speed, write speed, and how much data you scan for any given query.</p><h4>Row-Oriented Storage â€” How It Works</h4><p>In a row-oriented format (CSV, MySQL, PostgreSQL by default), all columns of Row 1 are stored together on disk, then all columns of Row 2, etc.</p><pre>Disk layout for a 3-row, 4-column table:\n\nRow 1: [user_id=1][name=Alice][age=30][country=US]\nRow 2: [user_id=2][name=Bob][age=25][country=UK]\nRow 3: [user_id=3][name=Carol][age=28][country=US]\n\nQuery: SELECT country, SUM(age) GROUP BY country\nWhat disk must read: ALL of [user_id, name, age, country] for every row\nEven though user_id and name are not needed at all!\n\nFor 100 columns, analytical queries typically need 3-5.\nRow storage reads 100 columns to use 5 = 95% wasted I/O.</pre><h4>Column-Oriented Storage â€” How It Works</h4><p>In a column-oriented format (Parquet, ORC), all values of column 1 are stored together, then all of column 2, etc.</p><pre>Disk layout (same data, columnar):\n\nColumn user_id: [1][2][3]\nColumn name:    [Alice][Bob][Carol]\nColumn age:     [30][25][28]\nColumn country: [US][UK][US]\n\nQuery: SELECT country, SUM(age) GROUP BY country\nWhat disk must read: ONLY [age] and [country] column blocks\nuser_id and name are not read at all â€” they live in different disk blocks!\n\nFor a 100-column table querying 5 columns: reads 5% of the data.</pre><p>âœï¸ <strong>Rule:</strong> Row storage is optimal for OLTP (write one complete row, read one complete row). Column storage is optimal for OLAP (read few columns across all rows, aggregate).</p></div></div><div class="level level-2"><div class="level-badge">ğŸ”µ Level 2 â€” Compression, Formats, Parquet Internals</div><div class="rich"><h4>Why Columnar Format Compresses So Well</h4><p>Compression works by finding patterns and encoding them more efficiently. Column storage makes compression dramatically more effective because <strong>values in the same column have the same type and similar values</strong>.</p><ul><li><strong>country column:</strong> ["US","UK","US","US","CA","US","UK"] â€” only a few unique values repeat. Dictionary encoding: US=1, UK=2, CA=3 â†’ stores [1,2,1,1,3,1,2] as integers. 90%+ compression.</li><li><strong>age column:</strong> [30,25,28,31,22,25,28] â€” small integers, all similar range. Delta encoding: store [30, -5, +3, +3, -9, +3, +3]. Small deltas compress extremely well.</li><li><strong>timestamp column:</strong> [1704067200, 1704067800, 1704068400] â€” always increasing, constant difference. RLE encodes as: start=1704067200, step=600, count=3.</li></ul><table><tr><th>Format</th><td>Compression</td><td>Read speed</td><td>Write speed</td><td>Best for</td></tr><tr><th>CSV</th><td>None (text)</td><td>Slow (parse text)</td><td>Fast</td><td>Data exchange, debugging</td></tr><tr><th>JSON</th><td>None</td><td>Very slow (parse tree)</td><td>Medium</td><td>APIs, nested data</td></tr><tr><th>Avro</th><td>Medium</td><td>Medium (row-oriented)</td><td>Very fast</td><td>Kafka messages, write-heavy</td></tr><tr><th>Parquet</th><td>High (columnar)</td><td>Very fast for queries</td><td>Slower</td><td>Analytics, data warehouses</td></tr><tr><th>ORC</th><td>High (columnar)</td><td>Very fast (Hive/Spark)</td><td>Slower</td><td>Hive workloads, transactional</td></tr><tr><th>Delta Lake</th><td>High + ACID</td><td>Very fast + time-travel</td><td>Medium</td><td>Lakehouse: ACID + analytics</td></tr></table><h4>Parquet File Structure â€” What\'s Inside</h4><pre>Parquet file internals:\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  MAGIC BYTES: "PAR1"                        â”‚\nâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚\nâ”‚  Row Group 1 (e.g., 128MB of rows)          â”‚\nâ”‚    Column chunk: user_id   [encoded values] â”‚\nâ”‚    Column chunk: age       [encoded values] â”‚\nâ”‚    Column chunk: country   [encoded values] â”‚\nâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚\nâ”‚  Row Group 2 (next 128MB of rows)           â”‚\nâ”‚    Column chunks...                         â”‚\nâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚\nâ”‚  Footer: schema + row group offsets         â”‚\nâ”‚    min/max statistics per column per chunk  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n# Statistics in footer enable predicate pushdown:\n# WHERE country = "US" â†’ check if "US" is within [min,max] of each row group\n# Skip entire row groups that cannot contain the value â†’ zero bytes read!</pre></div></div><div class="level level-3"><div class="level-badge">ğŸŸ¡ Level 3 â€” S3 Partitioning + Small File Problem</div><div class="rich"><h4>S3 Partitioning â€” Hive-Style Directory Layout</h4><p>When storing large datasets on S3 (or HDFS), the directory structure acts as a physical partition. Spark/Hive reads ONLY partition directories that match the query filter, completely skipping all other directories.</p><pre># Good partition structure (by year/month/day)\ns3://my-bucket/events/\n    year=2024/\n        month=01/\n            day=01/\n                part-00000.parquet\n                part-00001.parquet\n            day=02/\n                ...\n        month=02/...\n\n# Query: WHERE year=2024 AND month=01 AND day=01\n# Reads: ONLY s3://my-bucket/events/year=2024/month=01/day=01/\n# Skips: all other months and days entirely\n\n# Spark reads partition from path automatically:\ndf = spark.read.parquet("s3://my-bucket/events/")\ndf.filter((df.year==2024) & (df.month==1)).show()\n# Spark sees partition filter â†’ reads only matching directories</pre><h4>The Small File Problem â€” The Silent Performance Killer</h4><p>Data lakes accumulate thousands of tiny files (1-10MB each) over time, typically from: streaming jobs writing every minute, or daily partitions with many sub-partitions.</p><p>The problem: Parquet/HDFS has per-file overhead. 10,000 files of 5MB each reads SLOWER than 50 files of 1GB each â€” even though it\'s the same total data. Every file requires one filesystem API call, one file open, reading the footer, and metadata validation.</p><pre># Detect small file problem in Spark:\ndf.rdd.getNumPartitions()  # if this is > 10,000, you have too many small files\n\n# Fix: coalesce (narrow transform) or repartition (full shuffle)\ndf.coalesce(200).write.parquet("output/")  # merge to 200 files â€” no shuffle\ndf.repartition(200).write.parquet("output/")  # redistribute evenly â€” full shuffle\n\n# Rule: target file size 128MBâ€“1GB. coalesce < 200 files is usually fine.</pre></div></div><div class="level level-4"><div class="level-badge">ğŸ”´ Level 4 â€” Delta Lake â€” ACID on Data Lakes</div><div class="rich"><h4>Delta Lake â€” ACID Transactions on a Data Lake</h4><p>Traditional data lakes (S3 + Parquet) have no transaction support: if you DELETE or UPDATE records, you have to rewrite entire Parquet files. If your job fails halfway through, you have partial data written â€” corrupted state. <strong>Delta Lake</strong> adds ACID transactions and time travel to any Parquet-based data lake.</p><pre># Delta Lake stores data as Parquet + a transaction log (_delta_log/)\ns3://my-bucket/orders/\n    _delta_log/\n        00000000000000000000.json  # commit 0: initial data\n        00000000000000000001.json  # commit 1: added rows\n        00000000000000000002.json  # commit 2: deleted rows\n    part-00000.parquet\n    part-00001.parquet\n\n# ACID operations:\nfrom delta.tables import DeltaTable\ndt = DeltaTable.forPath(spark, "s3://my-bucket/orders/")\n\n# DELETE with conditions (records are marked deleted in log, not physically removed)\ndt.delete("order_status = \'CANCELLED\'")\n\n# UPSERT (MERGE INTO): insert new rows, update existing ones\ndt.alias("target").merge(\n    updates_df.alias("source"),\n    "target.order_id = source.order_id"\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n\n# Time travel: query data as it was at a previous point\ndf_yesterday = spark.read.format("delta").option("timestampAsOf", "2024-01-01").load(...)</pre><p><strong>FAANG interview:</strong> "When would you choose Delta Lake over plain Parquet?"</p><ul><li>You need to DELETE or UPDATE individual records (GDPR right-to-be-forgotten)</li><li>You need ACID guarantees: multiple writers, no partial writes</li><li>You need time travel: audit trail, debugging, rollbacks</li><li>You process late-arriving data that needs to be merged into historical partitions</li><li>Plain Parquet is better: write-once append-only workloads where simplicity is valued</li></ul></div></div></div>',
        'key_concepts': ['Row storage: all columns of one row together. Fast for full-row reads/writes (OLTP).', 'Columnar storage: all values of one column together. Fast for analytical queries (scan few columns of many rows).', 'Columnar compression: same-type values in a block compress 5â€“10x better than mixed row data.', 'Parquet internals: row groups (128MB), column chunks within groups, statistics in footer.', "Footer statistics enable predicate pushdown: skip entire row groups that can't match WHERE condition.", 'S3 partitioning: Hive-style year=/month=/day= directories. Spark reads only matching partitions.', 'Small file problem: 10K 5MB files read slower than 50 1GB files. Target 128MBâ€“1GB per file.', 'Delta Lake: adds ACID + time travel to Parquet. Use when you need DELETE/UPDATE/upsert on a data lake.'],
        'hints': ["Interview: 'Why Parquet over CSV?' â†’ columnar compression, predicate pushdown, schema enforcement.", 'Footer statistics in Parquet: min/max per column chunk enable row group skipping â€” huge for filtered reads.', 'Small file fix: df.coalesce(200) (no shuffle, reduce only) or df.repartition(200) (shuffle, redistribute evenly).', 'Delta Lake vs Parquet: Parquet = simple, immutable. Delta = ACID + time travel + schema enforcement.'],
        'tasks': ['<strong>Step 1:</strong> Create a 1M-row DataFrame. Write it as CSV and as Parquet. Compare file sizes on disk. Then query only 2 of 10 columns â€” compare read times.', '<strong>Step 2:</strong> Write the same DataFrame partitioned by year and month. Confirm Spark uses partition pruning by checking the query plan with .explain().', '<strong>Step 3:</strong> Simulate small file problem: write 1,000 small Parquet files. Then coalesce to 10 files. Compare read performance.', '<strong>Step 4:</strong> Try a Delta Lake MERGE (upsert): start with 1M orders, receive 10K updates and 5K new rows. Use .merge() to apply changes. Read back and verify.'],
        'hard_problem': 'Boss Problem (Databricks): You manage a data lake with 3TB of daily event data, stored as Parquet, partitioned by date. Problems: (1) 18 months of data â€” 540 date partitions Ã— 2,000 small files each = 1.08M files. Query times degraded from 2min to 45min. (2) GDPR requires deleting all data for specific user_ids within 30 days. (3) Late-arriving data lands 3-5 days after the event date, breaking partition assumptions. Design the migration plan to Delta Lake that fixes all three problems.',
    },

    # spark_logical_plan
    'spark_logical_plan': {
        'basics': '<div class="lesson-levels"><div class="level level-1"><div class="level-badge">ğŸŸ¢ Level 1 â€” What Spark Is â€” Driver/Executor + Lazy Eval</div><div class="rich"><h4>What Spark Actually Is â€” And Why It Exists</h4><p>Apache Spark was created in 2009 at UC Berkeley\'s AMPLab as a faster replacement for Hadoop MapReduce. Hadoop MapReduce wrote intermediate results to disk after every step. A 10-step pipeline wrote and read disk 10 times â€” massive I/O overhead. Spark keeps intermediate results <strong>in memory</strong>, passing data between stages without disk. For iterative algorithms (machine learning) and multi-step pipelines, this is 10â€“100x faster.</p><h4>How Spark Executes Code â€” The Driver/Executor Model</h4><pre>Spark Cluster Architecture:\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              DRIVER (your Python code)          â”‚\nâ”‚  - Runs on the master node                      â”‚\nâ”‚  - Builds the execution plan                    â”‚\nâ”‚  - Sends tasks to executors                     â”‚\nâ”‚  - Collects results                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                    â”‚ (sends serialized tasks)\n     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n     â–¼              â–¼              â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚Executor â”‚  â”‚Executor â”‚  â”‚Executor â”‚\nâ”‚ - Node 1â”‚  â”‚ - Node 2â”‚  â”‚ - Node 3â”‚\nâ”‚ - tasks â”‚  â”‚ - tasks â”‚  â”‚ - tasks â”‚\nâ”‚ - cache â”‚  â”‚ - cache â”‚  â”‚ - cache â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n# DataFrame operations run on executors in parallel\n# Driver only sees final results (unless you call .collect() â€” dangerous!)</pre><h4>Lazy Evaluation â€” Nothing Runs Until You Ask</h4><p>Spark is lazy: DataFrame transformations (filter, select, join, groupBy) do NOT execute immediately. They build a <strong>query plan</strong>. Only <strong>actions</strong> (collect(), count(), write(), show()) trigger actual execution.</p><pre>df = spark.read.parquet("s3://events/")   # no I/O yet\ndf2 = df.filter(df.year == 2024)           # no I/O yet â€” builds plan\ndf3 = df2.groupBy("country").count()       # no I/O yet â€” extends plan\n\ndf3.show()  # â† ACTION: NOW Spark executes the entire plan\n\n# Why lazy? Spark\'s optimizer (Catalyst) can rearrange operations:\n# - Pushes filters before joins (read less data)\n# - Eliminates unused columns early\n# - Merges multiple transformations into single scan\n# This often makes your unoptimized code run faster automatically.</pre></div></div><div class="level level-2"><div class="level-badge">ğŸ”µ Level 2 â€” The Shuffle + Broadcast Joins</div><div class="rich"><h4>The Shuffle â€” The Most Expensive Spark Operation</h4><p>A <strong>shuffle</strong> occurs when data needs to be redistributed across executors based on a key â€” for example, during a <code>groupBy</code>, <code>join</code>, or <code>repartition</code>. Shuffle is expensive because: (1) executors must write their partition to disk, (2) data is sent over the network, (3) receiving executors read from disk and memory. In a job with 100 executors processing 1TB, a shuffle can move terabytes over the network.</p><pre># Operations that trigger a shuffle:\ndf.groupBy("country").count()            # â† SHUFFLE: all rows for same country\n                                         #   must land on same executor\ndf.join(other, "user_id")               # â† SHUFFLE: matching user_ids must\n                                         #   be co-located\ndf.repartition(200, "country")          # â† SHUFFLE: explicit redistribution\ndf.distinct()                            # â† SHUFFLE: deduplicate requires global view\n\n# Operations that do NOT shuffle (cheap "narrow" transformations):\ndf.filter(df.year == 2024)              # stays on same partition\ndf.select("user_id", "amount")          # stays on same partition\ndf.withColumn("total", df.a + df.b)    # stays on same partition\ndf.coalesce(10)                         # merge partitions, no redistribution\n\n# Reduce shuffles by:\n# 1. Partition data by join key BEFORE the join (pre-partition)\n# 2. Broadcast small tables (eliminates join shuffle entirely)\n# 3. Aggregate before joining (reduce data volume before shuffle)</pre><h4>Broadcast Join â€” Eliminating the Join Shuffle</h4><pre># Problem: joining a 100TB fact table with a 50MB dimension\n# Default join â†’ shuffle 100TB over network (catastrophically slow)\n\n# Solution: broadcast the small table to ALL executors\n# Each executor gets a full copy â€” no network movement for the large table\n\nfrom pyspark.sql.functions import broadcast\n\nresult = large_df.join(\n    broadcast(small_dim_df),  # â† small table copied to all executors\n    on="product_id"\n)  # No shuffle of large_df! Each executor joins locally.\n\n# Spark auto-broadcasts tables < spark.sql.autoBroadcastJoinThreshold (default 10MB)\n# Increase for larger dimensions if they fit in executor memory:\nspark.conf.set("spark.sql.autoBroadcastJoinThreshold", "100m")</pre></div></div><div class="level level-3"><div class="level-badge">ğŸŸ¡ Level 3 â€” Data Skew + Partitions vs Tasks</div><div class="rich"><h4>Data Skew â€” When One Executor Does 90% of the Work</h4><p><strong>Data skew</strong> is the #1 production Spark performance problem. It occurs when one partition key value has far more rows than others. Example: a NULL user_id for all guest orders means the "NULL" partition gets 80% of all rows. One executor processing 8TB while others process 100GB â€” the job is only as fast as the slowest executor.</p><pre># Detect skew: check partition sizes\ndf.groupBy(spark_partition_id()).count().show()  # see partition row counts\n# If one partition has 100M rows and others have 1M â†’ skew\n\n# Fix 1: Salt the key (add random suffix to distribute skewed keys)\nfrom pyspark.sql import functions as F\n\n# Add salt to the large table\nnum_salt_buckets = 10\nsalted_large = large_df.withColumn(\n    "salted_key",\n    F.concat(F.col("user_id"), F.lit("_"), (F.rand() * num_salt_buckets).cast("int"))\n)\n\n# Explode the small table to match all salted keys\nsalt_values = spark.range(num_salt_buckets).withColumnRenamed("id", "salt")\nsalted_small = small_df.crossJoin(salt_values).withColumn(\n    "salted_key",\n    F.concat(F.col("user_id"), F.lit("_"), F.col("salt"))\n)\n\n# Now join on salted_key â€” skew is evenly distributed!\nresult = salted_large.join(salted_small, "salted_key")\n\n# Fix 2: Filter skewed keys and process separately\nnormal = df.filter(F.col("user_id").isNotNull())\nnulls  = df.filter(F.col("user_id").isNull())\n# Process each differently, then union</pre><h4>Partitions vs Tasks â€” Understanding Parallelism</h4><p>In Spark, a <strong>partition</strong> is a chunk of data. A <strong>task</strong> processes one partition. The number of parallel tasks = number of executor cores. If you have 100 cores and 1,000 partitions â†’ 100 tasks run simultaneously, 10 rounds of 100 tasks = 10 rounds total.</p><pre># Rule of thumb: 2-4 partitions per CPU core\n# 100 cores â†’ 200-400 partitions\n\n# Too few partitions: executors are idle (not enough work to distribute)\n# Too many partitions: per-partition overhead dominates (too many small tasks)\n\ndf.rdd.getNumPartitions()  # check current partition count\ndf.repartition(400)  # redistribute to 400 partitions (shuffle)\ndf.coalesce(10)      # reduce to 10 partitions (no shuffle, but may be uneven)\n\nspark.conf.set("spark.sql.shuffle.partitions", "400")  # for groupBy/join output</pre></div></div><div class="level level-4"><div class="level-badge">ğŸ”´ Level 4 â€” Diagnosing a Slow Spark Job</div><div class="rich"><h4>FAANG Interview: Diagnosing a Slow Spark Job</h4><p>Interview question: "Your Spark job processes 10TB of data but takes 6 hours. Walk me through diagnosing it."</p><pre>Step 1 â€” Open the Spark UI (port 4040)\n  â†’ DAG tab: which stage takes the most time?\n  â†’ Stage details: which task is the slowest? (skew indicator)\n  â†’ Storage tab: how much data is cached? Is spill occurring?\n\nStep 2 â€” Identify the type of slowdown:\n\n  A. One stage takes 90% of job time:\n     â†’ Check if it contains a shuffle (groupBy, join)\n     â†’ If so: can you reduce data before the shuffle? Broadcast the smaller side?\n\n  B. One task in a stage takes 10x longer than others:\n     â†’ Data skew. Check: what is the key distribution?\n     â†’ Fix: salt the key, or handle skewed keys separately\n\n  C. Spill to disk (red in Spark UI):\n     â†’ Executor ran out of memory, wrote shuffle data to disk\n     â†’ Fix: increase executor memory, or increase spark.sql.shuffle.partitions\n       (more partitions = smaller per-partition size = less memory per partition)\n\n  D. Many stages, each fast individually but slow overall:\n     â†’ Wide dependency chain â€” hard to parallelize\n     â†’ Fix: checkpoint intermediate results, cache frequently reused DataFrames\n\ndf.cache()  # cache in memory â€” avoid recomputing in subsequent actions\ndf.persist(StorageLevel.DISK_ONLY)  # persist to disk if too large for memory</pre><p>âœï¸ <strong>The Spark UI is your most important debugging tool.</strong> FAANG interviewers expect you to know how to read it and what each metric means.</p></div></div></div>',
        'key_concepts': ['Spark = distributed in-memory computing. Driver builds plan, executors execute tasks on partitions.', 'Lazy evaluation: transformations build a plan. Actions (count/show/write) trigger execution.', 'Catalyst optimizer: rearranges your query plan for efficiency â€” predicate pushdown, column pruning.', 'Shuffle: data redistribution across network. Most expensive operation. Triggered by groupBy, join, distinct.', 'Narrow transformation: each partition maps to exactly one output partition. No shuffle. Fast.', 'Broadcast join: copies small table to all executors. Eliminates join shuffle for large+small joins.', 'Data skew: one partition key has disproportionate rows. Fix: salting (randomize key) or separate processing.', 'Partitions = data chunks. Tasks = work units (1 per partition). Target 2-4 partitions per core.', 'spark.sql.shuffle.partitions default = 200. Tune to 2-4x your number of cores.'],
        'hints': ['Seeing one stage taking 90% of job time? Check for shuffle. Can you broadcast the smaller side?', 'One task running 10x longer than others? Data skew. Check key value distribution with groupBy + count.', 'Spill to disk in Spark UI? Executor out of memory. Increase executor memory or spark.sql.shuffle.partitions.', 'Cache DataFrames reused multiple times: df.cache(). Unpersist when done: df.unpersist().'],
        'tasks': ['<strong>Step 1:</strong> Read 10M rows. Chain: filter â†’ groupBy â†’ count. Use .explain(True) to show the physical plan. Identify which operations cause shuffles.', '<strong>Step 2:</strong> Join a 1B-row fact table with a 5MB lookup table. Compare: regular join vs broadcast join. Measure time difference with spark.time().', '<strong>Step 3:</strong> Simulate data skew: create a dataset where 80% of rows have user_id=NULL. Run a groupBy. Observe one task taking 10x longer. Apply the salt fix.', '<strong>Step 4:</strong> Set spark.sql.shuffle.partitions to 10, 200, 1000 on the same groupBy query. Observe partition count and execution time changes.'],
        'hard_problem': "Boss Problem (Uber): Your daily Spark job aggregates 500B GPS ping events (5TB) to compute driver earnings per city. It runs in 14 hours but needs to run in 2 hours. Steps: (1) You find 3 shuffle stages dominate. How do you investigate each one? (2) One join between pings (5TB) and a driver_metadata table (2GB) causes a 3TB shuffle. How do you fix it without salting? (3) City='NULL' has 60% of all pings (GPS loss). How do you handle this skew? (4) The job reads the same 5TB raw data for 4 different aggregations. How do you avoid reading it 4 times?",
    },

    # kafka_pub_sub
    'kafka_pub_sub': {
        'basics': '<div class="lesson-levels"><div class="level level-1"><div class="level-badge">ğŸŸ¢ Level 1 â€” Why Kafka Exists â€” Core Concepts</div><div class="rich"><h4>Why Kafka Exists â€” The Message Bus Problem</h4><p>In 2010, LinkedIn had a problem: dozens of services needed to share data in real time. Payment service needed transaction events. Recommendation engine needed click events. Analytics needed all of it. Point-to-point connections between all services would require NÂ² connections and NÂ² custom APIs. Every new consumer meant updating every producer.</p><p>LinkedIn built <strong>Apache Kafka</strong> as a distributed, append-only commit log that acts as a central data bus. Producers write events to Kafka topics. Any number of consumers read those events independently, at their own pace, without affecting other consumers or the producers.</p><h4>Core Concepts</h4><table><tr><th>Concept</th><td>What it is</td><td>Analogy</td></tr><tr><th>Topic</th><td>Named channel of events</td><td>A TV channel â€” broadcast to all who tune in</td></tr><tr><th>Partition</th><td>Ordered, immutable log within a topic</td><td>A video tape â€” you can only append, reads have an offset</td></tr><tr><th>Offset</th><td>Position of a message within a partition</td><td>Page number in a book â€” resume from where you left off</td></tr><tr><th>Producer</th><td>Writes messages to a topic</td><td>A journalist who publishes articles</td></tr><tr><th>Consumer</th><td>Reads messages from a topic</td><td>A reader who subscribes to the newspaper</td></tr><tr><th>Consumer Group</th><td>Set of consumers sharing work</td><td>A team reading different sections of the newspaper</td></tr><tr><th>Broker</th><td>A Kafka server storing partitions</td><td>A post office branch</td></tr></table><p>âœï¸ <strong>Key insight:</strong> Messages in Kafka are NOT deleted when consumed. They are retained for a configurable period (typically 7 days). Multiple consumers read the same message independently â€” each gets their own offset pointer.</p></div></div><div class="level level-2"><div class="level-badge">ğŸ”µ Level 2 â€” Partitions + Consumer Groups</div><div class="rich"><h4>Partitions â€” The Key to Kafka\'s Scalability</h4><p>A topic is split into partitions. Each partition is an ordered, immutable log stored on one broker. Producers can write to any partition (using a key for deterministic routing). Each consumer in a group is assigned one or more partitions exclusively.</p><p><strong>The fundamental rule:</strong> one partition is consumed by exactly one consumer in a group at any time. This is how Kafka provides ordering guarantees and prevents duplicate processing within a group.</p><pre>Topic "user_events" with 4 partitions, Consumer Group A with 3 consumers:\n\nPartition 0 â†’ Consumer 1 (assigned exclusively)\nPartition 1 â†’ Consumer 1 (one consumer can have multiple partitions)\nPartition 2 â†’ Consumer 2\nPartition 3 â†’ Consumer 3\n\nIf Consumer 1 crashes:\n  Kafka rebalances â†’ Consumer 2 picks up Partition 0 and 1\n\n# Scaling rule: max parallelism = number of partitions\n# 4 partitions â†’ max 4 consumers in a group can work in parallel\n# Adding a 5th consumer â†’ it sits idle (no partition to consume)\n\n# Partition key routing:\n# producer.send("user_events", key="user_42", value=event)\n# â†’ All events for user_42 always go to the SAME partition\n# â†’ All events for user_42 are processed in ORDER by the same consumer</pre><h4>Consumer Group â€” Independent Consumers Read the Same Stream</h4><pre>Topic "orders":\n  Group A (analytics): reads orders â†’ writes to DW â†’ at offset 50,000\n  Group B (fraud):     reads orders â†’ checks for fraud â†’ at offset 49,500\n  Group C (email):     reads orders â†’ sends shipping emails â†’ at offset 51,000\n\n# Each group has its own independent offset tracking\n# Producer\'s throughput is unaffected by how fast consumers read\n# If Group B (fraud) is slow, it does not slow down Group A or C</pre></div></div><div class="level level-3"><div class="level-badge">ğŸŸ¡ Level 3 â€” Delivery Semantics + Kafka vs Queue</div><div class="rich"><h4>Delivery Semantics â€” At-Most-Once, At-Least-Once, Exactly-Once</h4><p>The most common Kafka interview question: what delivery guarantee does Kafka provide?</p><table><tr><th>Semantic</th><td>Messages delivered</td><td>Duplicates?</td><td>Data loss?</td><td>When to use</td></tr><tr><th>At-most-once</th><td>Maybe delivered</td><td>Never</td><td>Possible</td><td>Metrics sampling, non-critical logs</td></tr><tr><th>At-least-once</th><td>Guaranteed delivered</td><td>Possible</td><td>Never</td><td>Most common: process with idempotent consumer</td></tr><tr><th>Exactly-once</th><td>Exactly once end-to-end</td><td>Never</td><td>Never</td><td>Financial transactions, audit logs</td></tr></table><p><strong>How at-least-once works:</strong> Consumer reads message â†’ processes it â†’ commits offset. If the consumer crashes AFTER processing but BEFORE committing, the next consumer starts from the last committed offset and re-processes the same message. This is why consuming applications must be idempotent â€” processing the same message twice produces the correct result.</p><pre># Idempotent consumer pattern: INSERT ... ON CONFLICT DO NOTHING\nINSERT INTO processed_orders (order_id, status, updated_at)\nVALUES (%(order_id)s, %(status)s, %(ts)s)\nON CONFLICT (order_id) DO UPDATE\n  SET status = EXCLUDED.status,\n      updated_at = EXCLUDED.updated_at\n  WHERE orders.updated_at < EXCLUDED.updated_at;\n-- Running this query twice with the same data = same result (idempotent)</pre><h4>Kafka vs Message Queue â€” A Critical Distinction</h4><table><tr><th>Feature</th><td>Kafka (Log)</td><td>RabbitMQ/SQS (Queue)</td></tr><tr><th>Message retention</th><td>Retained for days (configurable)</td><td>Deleted after consumption</td></tr><tr><th>Multiple consumers</th><td>Yes â€” each group reads independently</td><td>No â€” one consumer gets each message</td></tr><tr><th>Replay</th><td>Yes â€” seek to any offset</td><td>No â€” once consumed, gone</td></tr><tr><th>Ordering</th><td>Guaranteed within partition</td><td>Generally no guarantee</td></tr><tr><th>Throughput</th><td>Millions/sec (sequential disk)</td><td>Thousands/sec</td></tr><tr><th>Best for</th><td>Event streaming, audit log, multiple consumers</td><td>Task queues, job distribution</td></tr></table></div></div><div class="level level-4"><div class="level-badge">ğŸ”´ Level 4 â€” Windowing + Real-Time Analytics Design</div><div class="rich"><h4>Windowing â€” Aggregating Over Time in a Stream</h4><p>Streams are infinite, but analytics need finite windows: "revenue per hour," "error rate per 5-minute window," "top 10 pages per day."</p><pre># Three types of windows in stream processing (Flink/Kafka Streams/Spark Structured Streaming):\n\n# 1. Tumbling Window: fixed, non-overlapping\n#    |---1min---|---1min---|---1min---|\n#    Each event belongs to exactly one window.\n\n# 2. Sliding Window: fixed size, overlapping (stride < window size)\n#    |---10min---|\n#        |---10min---|\n#            |---10min---|\n#    Each event appears in multiple windows.\n\n# 3. Session Window: dynamic, gap-based\n#    A session starts on first event, ends after N seconds of inactivity.\n#    Common for: user session analytics, page view sessions.\n\n# Late data problem:\n#   Event timestamp: 14:02 (when it happened on the device)\n#   Processing timestamp: 14:09 (when Kafka received it â€” 7min late)\n#   The 14:00-14:05 window is already closed! What do you do?\n\n# Solutions:\n#   Watermark: allow data up to N minutes late, then close the window\n#   Reprocess: store events in a buffer period, re-aggregate after watermark\n#   Lambda architecture: batch layer handles late data correctness,\n#   speed layer handles real-time approximate results</pre><h4>FAANG Interview: Design a Real-Time Analytics Pipeline</h4><p>Question: "Design a system to display real-time metrics for a ride-sharing app: active riders, active drivers, rides per minute by city, surge pricing signal."</p><pre>Sources: mobile apps emit GPS pings, ride state changes â†’ Kafka topics\n\nKafka topics:\n  - driver_pings (partitioned by driver_id) â†’ 100K msgs/sec\n  - ride_events  (partitioned by city)      â†’  10K msgs/sec\n\nStream processing (Flink):\n  - 1-minute tumbling window on ride_events â†’ count rides per city\n  - 5-minute window on driver_pings â†’ count active drivers per city\n  - Session window on driver_pings â†’ detect driver going offline\n\nOutput sinks:\n  - Redis (for dashboard): SET city:rides:nyc 125   (1-min rolling count)\n  - Kafka topic "metrics" â†’ dashboard WebSocket feed\n  - S3 Parquet â†’ hourly batch for BI reporting\n\nSurge pricing signal:\n  - rides_requested / active_drivers > 1.5 in last 5 minutes â†’ surge!</pre></div></div></div>',
        'key_concepts': ['Kafka = distributed append-only commit log. Producers write, consumers read independently.', 'Topic: named channel. Partition: ordered immutable log within a topic. Offset: position in partition.', 'Consumer group: each partition served by exactly one consumer in the group. Max parallelism = partition count.', 'Partition key routing: all events with the same key always go to the same partition (and same consumer).', 'Messages NOT deleted after consumption â€” retained for configurable period. Multiple groups read independently.', 'At-most-once: may lose. At-least-once: may duplicate (most common). Exactly-once: hardest, needed for finance.', 'Consumer must be idempotent for at-least-once: INSERT ON CONFLICT DO NOTHING, or overwrite partition.', 'Kafka vs queue: Kafka retains messages for replay. Queue deletes after consumption. Kafka supports many consumers.', 'Windowing: tumbling (non-overlapping), sliding (overlapping), session (gap-based). Late data needs watermarks.'],
        'hints': ["Interview: 'guarantee each message processed exactly once?' â†’ explain at-least-once + idempotent consumer.", 'Partition count limits parallelism: 4 partitions â†’ max 4 consumers per group. Adding a 5th consumer idles.', "Use partition key = user_id when ordering per-user matters (e.g., user's event stream must be ordered).", "Kafka is NOT a database. Don't use it as a primary store. Consumers should always write to a real store."],
        'tasks': ['<strong>Step 1:</strong> Design a Kafka topic layout for an e-commerce order pipeline: what topics? How many partitions each? What partition key?', '<strong>Step 2:</strong> Write an idempotent consumer for an orders topic: receive order events, INSERT into PostgreSQL with ON CONFLICT DO NOTHING.', "<strong>Step 3:</strong> Design: multiple consumers reading the same 'orders' topic independently â€” analytics, fraud detection, inventory update. How many consumer groups?", '<strong>Step 4:</strong> Model a late-arriving event problem: events arrive 5 minutes late. You have a 1-minute tumbling window for revenue. How do you handle late data?'],
        'hard_problem': "Boss Problem (LinkedIn): Design Kafka's original use case â€” the activity feed pipeline. Every LinkedIn user action (post, like, comment, view) is an event streamed to Kafka. (1) What topics and partitions? (2) 3 consumers need the same data: feed renderer, analytics, notifications â€” how many consumer groups? (3) A user with 50M followers posts â€” fan-out-on-write creates 50M messages. How does Kafka handle this? (4) You need to replay the last 7 days of events to rebuild the recommendation model. Are 7-day retention and 50M events/day feasible on Kafka?",
    },

    # dag_architecture
    'dag_architecture': {
        'basics': '<div class="lesson-levels"><div class="level level-1"><div class="level-badge">ğŸŸ¢ Level 1 â€” What DAGs Are + Airflow Core Concepts</div><div class="rich"><h4>What is a DAG and Why Orchestration Matters</h4><p>A data pipeline consists of multiple steps: extract from source, validate, transform, load, test, notify. These steps have dependencies â€” step 3 cannot start until steps 1 and 2 complete. Some steps can run in parallel (downloading from multiple sources simultaneously). Some steps must run in sequence (load only after validation passes).</p><p>A <strong>Directed Acyclic Graph (DAG)</strong> is the formal way to represent these dependencies: nodes are tasks, directed edges represent "must complete before." Acyclic means there are no circular dependencies (which would cause deadlock). Apache Airflow is the dominant DAG orchestration platform at FAANG companies.</p><pre># Example DAG: daily customer revenue pipeline\n\n     [extract_orders]   [extract_customers]   â† parallel execution\n            \\                 /\n          [validate_data]            â† waits for both\n                  |\n     [compute_revenue]              â† runs after validation\n         /          \\\\\n[load_to_dw]  [send_slack_report]   â† parallel execution\n         \\\\\n       [run_dbt_tests]              â† runs after load</pre><h4>Airflow Core Concepts</h4><table><tr><th>Concept</th><td>What It Does</td></tr><tr><th>DAG</th><td>A Python file defining the workflow graph</td></tr><tr><th>Task</th><td>A single unit of work (one node in the graph)</td></tr><tr><th>Operator</th><td>Type of task: PythonOperator, BashOperator, SparkSubmitOperator, etc.</td></tr><tr><th>Sensor</th><td>Special operator that waits for a condition (file arrives, API responds)</td></tr><tr><th>Schedule</th><td>Cron expression or timedelta defining when DAG runs</td></tr><tr><th>DAG Run</th><td>One execution instance of a DAG for a specific logical date</td></tr><tr><th>Task Instance</th><td>One execution of one task within a DAG Run</td></tr><tr><th>XCom</th><td>Mechanism for tasks to pass small values to downstream tasks</td></tr></table></div></div><div class="level level-2"><div class="level-badge">ğŸ”µ Level 2 â€” Operators, Sensors, Real DAG Code</div><div class="rich"><h4>Operators and Sensors â€” The Building Blocks</h4><p>An <strong>Operator</strong> defines what a task does. Airflow provides built-in operators for common actions, and you can write custom operators for anything else.</p><pre>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nfrom airflow.sensors.filesystem import FileSensor\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    "owner": "data-team",\n    "retries": 3,\n    "retry_delay": timedelta(minutes=5),\n    "email_on_failure": True,\n}\n\nwith DAG(\n    dag_id="daily_revenue_pipeline",\n    schedule_interval="0 3 * * *",    # 3am UTC daily\n    start_date=datetime(2024, 1, 1),\n    catchup=False,                    # don\'t backfill old runs\n    default_args=default_args,\n) as dag:\n\n    wait_for_source = FileSensor(\n        task_id="wait_for_source_file",\n        filepath="/data/source/orders_{{ ds }}.csv",\n        poke_interval=60,   # check every 60 seconds\n        timeout=3600,       # fail if not found within 1 hour\n    )\n\n    run_spark = SparkSubmitOperator(\n        task_id="compute_revenue",\n        application="s3://jobs/revenue_job.py",\n        conf={"spark.executor.memory": "8g"},\n    )\n\n    def validate(**context):\n        # access the execution date via context\n        ds = context["ds"]  # "2024-01-15"\n        count = check_row_count(ds)\n        if count < 1000:\n            raise ValueError(f"Only {count} rows â€” expected > 1000")\n\n    validate_task = PythonOperator(\n        task_id="validate_output",\n        python_callable=validate,\n    )\n\n    # Dependency chain\n    wait_for_source >> run_spark >> validate_task</pre></div></div><div class="level level-3"><div class="level-badge">ğŸŸ¡ Level 3 â€” XComs + Backfilling Pitfalls</div><div class="rich"><h4>XComs â€” Passing Data Between Tasks</h4><p>XCom (cross-communication) allows tasks to pass small values to downstream tasks. <strong>Critical rule:</strong> XComs are stored in the Airflow metadata database. They are for small values only: row counts, file paths, status strings, timestamps. Never XCom a DataFrame or large file â€” use S3/GCS paths instead.</p><pre># Task 1: pushes a value via XCom (auto-push with return)\ndef extract(**context):\n    rows = run_extraction()\n    return len(rows)  # auto-pushed to XCom key "return_value"\n\n# Task 2: pulls the value from upstream task\ndef validate(**context):\n    row_count = context["ti"].xcom_pull(\n        task_ids="extract_task",  # which task\n        key="return_value"        # which XCom key\n    )\n    if row_count < 1000:\n        raise ValueError(f"Too few rows: {row_count}")\n\n# Better pattern for large data: pass S3 path, not the data itself\ndef extract(**context):\n    ds = context["ds"]\n    path = f"s3://staging/orders/{ds}/data.parquet"\n    write_data_to_s3(path)\n    return path  # XCom the path, not the data</pre><h4>Backfilling â€” Running Historical Dates</h4><p><strong>Backfilling</strong> means running a DAG for historical dates â€” for example, you added a new column to your pipeline and need to reprocess the last 90 days of data. Airflow can trigger DAG runs for any historical date via the CLI or UI.</p><pre># Backfill via CLI: run every day from Jan 1 to Jan 31\nairflow dags backfill \\\n  --dag-id daily_revenue_pipeline \\\n  --start-date 2024-01-01 \\\n  --end-date 2024-01-31\n\n# âš ï¸  Backfill pitfalls:\n# 1. catchup=True (default) auto-backfills missed runs on restart\n#    â†’ set catchup=False to prevent surprise backfills\n# 2. Backfills run concurrently (all 31 days at once by default)\n#    â†’ set max_active_runs=1 to run one day at a time\n# 3. Tasks may not be idempotent â†’ backfill causes duplicate data\n#    â†’ always make your pipeline idempotent!</pre></div></div><div class="level level-4"><div class="level-badge">ğŸ”´ Level 4 â€” Dynamic DAGs + Design Best Practices</div><div class="rich"><h4>Dynamic DAGs â€” Generating at Scale</h4><p>When you need dozens or hundreds of similar pipelines (one DAG per client, per table, per region), manually writing each DAG is impractical. <strong>Dynamic DAGs</strong> generate the DAG structure programmatically from a config or database.</p><pre># Pattern: generate one DAG per client from a config file\n# clients.yaml: [{name: "acme", tables: ["orders","users"]}, ...]\n\nimport yaml\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\nwith open("/dags/config/clients.yaml") as f:\n    clients = yaml.safe_load(f)["clients"]\n\nfor client in clients:\n    dag_id = f"etl_{client[\'name\']}"\n\n    with DAG(\n        dag_id=dag_id,\n        schedule_interval="0 4 * * *",\n        ...\n    ) as dag:\n        for table in client["tables"]:\n            task = PythonOperator(\n                task_id=f"process_{table}",\n                python_callable=process_table,\n                op_args=[client["name"], table],\n            )\n        globals()[dag_id] = dag  # register DAG globally for Airflow scanner\n\n# Result: one DAG per client, all generated from config\n# Adding a new client = add one line to clients.yaml</pre><h4>FAANG Interview: DAG Design Best Practices</h4><ul><li><strong>Idempotent tasks:</strong> running the same task twice for the same date should produce the same result. No duplicate inserts. Use INSERT ... ON CONFLICT or DELETE then INSERT.</li><li><strong>Atomic writes:</strong> write to a staging location, atomically move to final location. Never overwrite production data partially.</li><li><strong>Alerting:</strong> every production DAG should have email_on_failure=True and PagerDuty/Slack integration for SLA misses.</li><li><strong>SLA monitoring:</strong> if a DAG that normally finishes at 4am does not complete by 6am, trigger an alert â€” the downstream team needs to know.</li><li><strong>Prefer sensors over long-running tasks:</strong> a FileSensor that checks every 60 seconds uses minimal resources. A sleep loop in a Python task holds an executor slot.</li></ul></div></div></div>',
        'key_concepts': ['DAG = Directed Acyclic Graph. Nodes are tasks, edges are dependencies. Acyclic = no circular dependencies.', 'Airflow: Python-defined DAGs. schedule_interval (cron), start_date, catchup, max_active_runs.', 'Operator types: PythonOperator, BashOperator, SparkSubmitOperator, SqlOperator, S3ToRedshiftOperator.', 'Sensor: waits for a condition. FileSensor, HttpSensor, S3KeySensor. Uses poke_interval to check periodically.', 'XCom: small values passed between tasks (paths, counts, flags). Never XCom large data â€” use S3 path instead.', 'catchup=False: prevents Airflow from backfilling all missed runs on restart. Almost always set this.', 'Idempotent tasks: running twice for the same date = same result. Essential for safe retries and backfills.', 'Dynamic DAGs: generate multiple DAGs from a config (loop in Python file). Use globals() to register.'],
        'hints': ['catchup=True (default) can auto-run hundreds of missed historical DAG runs â€” almost always set catchup=False.', 'XCom best practice: pass S3 paths, not data. The data lives in S3, the path lives in XCom.', 'Sensor vs sleep in task: sensor releases executor slot while waiting. Sleep holds the slot â€” avoid it.', 'max_active_runs=1: prevents concurrent DAG runs. Important for pipelines that write to the same partition.'],
        'tasks': ['<strong>Step 1:</strong> Write an Airflow DAG with 3 tasks: extract (PythonOperator), transform (SparkSubmitOperator), load (PythonOperator). Set up correct dependencies.', '<strong>Step 2:</strong> Add a FileSensor before extract that waits for a source file to appear in S3. Set poke_interval=60 and timeout=3600.', '<strong>Step 3:</strong> Make the extract task push a row count to XCom. Have the validate task pull it and raise ValueError if count < 1000.', '<strong>Step 4:</strong> Write a dynamic DAG generator that creates one DAG per client from a JSON config file with 5 client entries. Verify all 5 DAGs appear in Airflow UI.'],
        'hard_problem': 'Boss Problem (Spotify): You manage a daily pipeline: 200 Airflow DAGs, each processing data for one country. DAGs run at different times (by time zone) and share downstream dependencies (a global aggregation DAG that waits for ALL 200 country DAGs to finish). (1) Design the dependency structure. (2) One country DAG fails â€” should the global aggregation wait or proceed with 199/200 countries? (3) A new country is added â€” how do you add it without rewriting code? (4) The global aggregation consistently misses its 6am SLA by 15 minutes â€” diagnose and fix.',
    },

    # data_quality
    'data_quality': {
        'basics': '<div class="lesson-levels"><div class="level level-1"><div class="level-badge">ğŸŸ¢ Level 1 â€” Data Quality Dimensions + Where to Check</div><div class="rich"><h4>Data Quality â€” The Silent Business Risk</h4><p>Data quality failures are responsible for an estimated $3.1 trillion in losses annually in the US alone (IBM, 2016). At FAANG scale, bad data can cause: incorrect recommendations (Netflix), miscounted ad impressions (Google billing error), wrong product availability (Amazon), or payments to the wrong accounts (banking). Data quality is not a nice-to-have â€” it is a business-critical function.</p><h4>The Five Dimensions of Data Quality</h4><table><tr><th>Dimension</th><td>Definition</td><td>Example check</td></tr><tr><th>Completeness</th><td>Are required values present?</td><td>WHERE user_id IS NULL â†’ count should be 0</td></tr><tr><th>Uniqueness</th><td>No duplicate records?</td><td>COUNT(*) vs COUNT(DISTINCT pk) â†’ must be equal</td></tr><tr><th>Validity</th><td>Values within expected range/format?</td><td>age BETWEEN 0 AND 120, email LIKE \'%@%\'</td></tr><tr><th>Consistency</th><td>Same fact represented the same way?</td><td>revenue = SUM(line_items) for every order</td></tr><tr><th>Timeliness</th><td>Data arrives when expected?</td><td>MAX(event_time) > NOW() - INTERVAL 1 HOUR</td></tr></table><h4>Data Quality Checks â€” Where to Put Them</h4><pre>Three places to add quality checks:\n\n1. INGESTION: validate data as it arrives from source\n   â†’ Reject/quarantine rows that fail schema/range checks\n   â†’ Stop bad data from entering the warehouse\n\n2. TRANSFORMATION: check intermediate results after each step\n   â†’ Assert: if I join A to B, I should get X rows (Â± 5%)\n   â†’ Check: did revenue change by >50% overnight? (anomaly)\n\n3. OUTPUT: check the final data before publishing to consumers\n   â†’ Ensure tables are not empty\n   â†’ Verify foreign key integrity\n   â†’ Check business rules (all orders have a payment)</pre></div></div><div class="level level-2"><div class="level-badge">ğŸ”µ Level 2 â€” dbt + Schema Evolution</div><div class="rich"><h4>dbt â€” Data Build Tool</h4><p>dbt (data build tool) is the standard tool for SQL-based transformations in modern data warehouses. It lets you write SQL SELECT queries (models), run them as CREATE TABLE / CREATE VIEW, test them automatically, document them, and track lineage.</p><pre># dbt model: models/revenue/daily_revenue.sql\n-- This SQL becomes a table in your data warehouse\n{{ config(materialized=\'table\') }}\n\nSELECT\n    order_date,\n    SUM(revenue)     AS total_revenue,\n    COUNT(order_id)  AS order_count\nFROM {{ ref(\'stg_orders\') }}   -- reference another dbt model\nWHERE order_status = \'COMPLETED\'\nGROUP BY order_date\n\n# dbt test: models/revenue/schema.yml\n# Tests run automatically after each model build\nmodels:\n  - name: daily_revenue\n    columns:\n      - name: order_date\n        tests:\n          - not_null\n          - unique\n      - name: total_revenue\n        tests:\n          - not_null\n          - dbt_utils.expression_is_true:\n              expression: "> 0"  # revenue must be positive</pre><h4>Schema Evolution â€” Handling Schema Changes Safely</h4><pre># Adding a column: backwards compatible â€” safe\nALTER TABLE orders ADD COLUMN promo_code VARCHAR(50);\n-- Existing readers ignore the new column (Parquet, Avro do this automatically)\n\n# Removing a column: BREAKING CHANGE\n-- Readers that expect the column will fail!\n-- Strategy: deprecate first (document), migrate readers, then drop\n\n# Changing a column type: almost always BREAKING\n-- VARCHAR(50) â†’ VARCHAR(200): safe (wider)\n-- INT â†’ BIGINT: safe in Parquet (widening)\n-- INT â†’ VARCHAR: breaking (downstream code fails)\n\n# Schema enforcement in Parquet (PySpark):\ndf.write.option("mergeSchema", "true").parquet("s3://data/orders/")\n# mergeSchema=True: allows new columns to be added to existing Parquet files\n# Missing columns in old files appear as NULL in combined reads</pre></div></div><div class="level level-3"><div class="level-badge">ğŸŸ¡ Level 3 â€” Idempotency + Unit Testing</div><div class="rich"><h4>Idempotency â€” The Most Important Pipeline Property</h4><p><strong>Idempotency:</strong> running a pipeline multiple times for the same date produces the same result. This is essential for: safe retries after failures, safe backfills, and debugging in production.</p><pre># âŒ NON-IDEMPOTENT: running twice creates duplicate rows\nINSERT INTO daily_revenue\nSELECT order_date, SUM(revenue) FROM orders GROUP BY order_date;\n-- Run twice for 2024-01-15 â†’ two rows for 2024-01-15!\n\n# âœ… IDEMPOTENT pattern 1: DELETE THEN INSERT (delete-and-replace)\nDELETE FROM daily_revenue WHERE order_date = \'2024-01-15\';\nINSERT INTO daily_revenue\nSELECT order_date, SUM(revenue) FROM orders WHERE order_date=\'2024-01-15\' GROUP BY 1;\n\n# âœ… IDEMPOTENT pattern 2: INSERT ... ON CONFLICT UPDATE\nINSERT INTO daily_revenue(order_date, total_revenue)\nSELECT order_date, SUM(revenue) FROM orders GROUP BY order_date\nON CONFLICT (order_date) DO UPDATE SET total_revenue=EXCLUDED.total_revenue;\n\n# âœ… IDEMPOTENT pattern 3: overwrite the partition\ndf.write\\\n  .partitionBy("order_date")\\\n  .mode("overwrite")\\\n  .parquet("s3://revenue/")\n# Rewrites only the affected partition â€” other partitions untouched</pre><h4>Unit Testing Data Pipelines</h4><pre>import pytest\nfrom pyspark.sql import SparkSession\n\ndef test_revenue_computation():\n    spark = SparkSession.builder.master("local[2]").getOrCreate()\n\n    # Create small test dataframe\n    input_data = [\n        ("2024-01-15", "COMPLETED", 100.0),\n        ("2024-01-15", "COMPLETED", 50.0),\n        ("2024-01-15", "CANCELLED", 200.0),   # should be excluded\n    ]\n    orders_df = spark.createDataFrame(input_data, ["order_date","status","revenue"])\n\n    # Run the transformation\n    result = compute_daily_revenue(orders_df)\n\n    # Assert expected output\n    rows = result.collect()\n    assert len(rows) == 1, "Expected one result row"\n    assert rows[0].total_revenue == 150.0, "Cancelled orders should be excluded"\n    assert rows[0].order_date == "2024-01-15"</pre></div></div><div class="level level-4"><div class="level-badge">ğŸ”´ Level 4 â€” Governance + Handling Bad Data in Production</div><div class="rich"><h4>Data Governance â€” Lineage, Catalog, Access Control</h4><p>Data governance answers: what data do we have, what is it for, who should access it, where did it come from? At FAANG scale, this is essential because hundreds of engineers use the same data warehouse, and a new engineer must be able to find trustworthy data without asking.</p><ul><li><strong>Data Catalog</strong>: searchable index of all tables with descriptions, owners, update frequency. Tools: AWS Glue, Amundsen, DataHub.</li><li><strong>Data Lineage</strong>: visualizes the transformation chain: "This dashboard metric comes from table X, which is built from source table Y, which ingests from API Z." Enables impact analysis: "If API Z changes, what breaks?"</li><li><strong>Column-level lineage</strong>: even more granular â€” traces which source columns contributed to each output column.</li><li><strong>PII governance</strong>: tables containing personal data (name, email, SSN) require special access controls, encryption, and retention policies.</li><li><strong>Row-level security</strong>: different rows visible to different users (e.g., each sales rep sees only their region\'s data).</li></ul><h4>FAANG Interview: "How do you handle bad data in production?"</h4><pre>Framework answer:\n\n1. DETECT: automated quality checks at every pipeline stage\n   - dbt tests on output models\n   - Great Expectations or custom SQL assertions\n   - Anomaly detection (revenue dropped 80%? â†’ alert)\n\n2. ALERT: PagerDuty or Slack notification immediately\n   - Include: which table, which check failed, expected vs actual\n\n3. QUARANTINE: do not propagate bad data downstream\n   - Move failing records to quarantine table\n   - Good records continue to production\n\n4. INVESTIGATE: what is the root cause?\n   - Source system change? (schema evolution)\n   - Missing upstream data? (late arrival)\n   - Pipeline bug? (code change)\n\n5. FIX + BACKFILL: fix the pipeline, reprocess affected dates</pre></div></div></div>',
        'key_concepts': ['5 quality dimensions: Completeness, Uniqueness, Validity, Consistency, Timeliness.', 'Add quality checks at: ingestion (reject bad data), transformation (assert intermediate results), output (verify before publish).', 'dbt: write SQL SELECT â†’ runs as table/view. Built-in tests: not_null, unique, accepted_values, relationships.', 'Schema evolution: adding columns = safe. Removing/renaming columns = breaking. Changing types = usually breaking.', 'Idempotency: pipeline can run multiple times for same date â†’ same result. Critical for safe retries + backfills.', "Idempotent patterns: DELETE+INSERT, INSERT ON CONFLICT UPDATE, write.mode('overwrite').partitionBy().", 'Unit testing: create small input DataFrame, run transformation, assert expected output. Use pytest + local Spark.', 'Data governance: catalog (what exists), lineage (where it comes from), access control (who sees what).'],
        'hints': ['dbt test types: not_null, unique (built-in), accepted_values, relationships (built-in), custom SQL assertions.', 'Non-idempotent pipelines are the #1 cause of duplicate data bugs at scale. Always design for idempotency.', 'Great Expectations: Python library for defining and running data validation rules (similar to dbt tests but in Python).', "Anomaly detection alert: if today's row count < 80% or > 120% of last 7-day average â†’ alert. Simple and powerful."],
        'tasks': ['<strong>Step 1:</strong> Write 5 SQL quality checks for an orders table: NULL check, uniqueness, range validation (amount > 0), foreign key integrity, and timeliness (max order_date within last 2 hours).', '<strong>Step 2:</strong> Write a dbt model and schema.yml with at least 3 tests (not_null, unique, expression_is_true). Run dbt test.', '<strong>Step 3:</strong> Make a non-idempotent INSERT idempotent using all 3 patterns: DELETE+INSERT, ON CONFLICT, and Spark overwrite partition. Verify by running twice.', '<strong>Step 4:</strong> Write a pytest unit test for a Spark transformation that computes daily revenue. Include edge cases: empty input, all-NULL revenue, negative amounts.'],
        'hard_problem': "Boss Problem (Airbnb): You are the data quality lead. Your nightly ETL ingests from 20 source databases across 5 countries. (1) Design a quality check framework: what checks run at each stage, who is alerted when they fail, and how do you prevent bad data from reaching BI reports? (2) A source system in Germany changes the 'price' column from EUR decimal to STRING 'â‚¬X.XX' without warning. How does your system detect this? What happens to the pipeline? How do you fix it? (3) A data engineer accidentally writes non-idempotent code that ran 3 times â€” you now have 3x the rows for 2024-01-15 only. How do you fix production data?",
    },

    # system_design_batch_etl
    'system_design_batch_etl': {
        'basics': '<div class="lesson-levels"><div class="level level-1"><div class="level-badge">ğŸŸ¢ Level 1 â€” DE System Design + Back-of-Envelope Math</div><div class="rich"><h4>Data Engineering System Design â€” What\'s Different from Software Engineering</h4><p>Software engineering system design interviews focus on: web services, APIs, databases, caching, load balancers. Data engineering system design adds: data volume math, pipeline latency, storage formats, streaming vs batch trade-offs, failure modes at scale, and cost optimization.</p><h4>Back-of-Envelope Math â€” Essential for DE Interviews</h4><p>FAANG interviewers expect you to quickly estimate: how much storage do we need? How long will it take to process? What network bandwidth is required? This is called "back-of-envelope" calculation. Here are the key numbers to memorize:</p><pre>Useful numbers:\n  1 byte       = 1 character\n  1 KB         = 1,000 bytes\n  1 MB         = 1,000 KB\n  1 GB         = 1,000 MB\n  1 TB         = 1,000 GB\n  1 PB         = 1,000 TB\n\n  SSD read     = 500 MB/sec\n  HDD read     = 100 MB/sec\n  Network LAN  = 1 GB/sec\n  S3/cloud     = 100-500 MB/sec (per file, in parallel: much more)\n\n  1 million events/day Ã— 100 bytes/event = 100 MB/day\n  1 billion events/day Ã— 100 bytes/event = 100 GB/day\n  = 3 TB/month â‰ˆ 36 TB/year (uncompressed)\n  Parquet at 5x compression = 7 TB/year â€” easily fits in S3\n\n  100 million users Ã— 1 KB profile = 100 GB â€” fits in memory on a large machine!</pre><h4>Batch ETL vs Streaming â€” The Core Trade-Off</h4><table><tr><th>Dimension</th><td>Batch ETL</td><td>Streaming</td></tr><tr><th>Data freshness</th><td>Hours to daily</td><td>Seconds to minutes</td></tr><tr><th>Complexity</th><td>Lower (simpler tooling)</td><td>Higher (state, ordering, fault tolerance)</td></tr><tr><th>Cost</th><td>Lower (bursty compute)</td><td>Higher (always-running compute)</td></tr><tr><th>Failure recovery</th><td>Simple: re-run the batch</td><td>Complex: replay from Kafka offset</td></tr><tr><th>Best for</th><td>Daily reports, historical analysis</td><td>Fraud detection, live dashboards, alerting</td></tr><tr><th>Tooling</th><td>Spark, dbt, Airflow</td><td>Kafka, Flink, Spark Structured Streaming</td></tr></table></div></div><div class="level level-2"><div class="level-badge">ğŸ”µ Level 2 â€” Batch ETL Architecture + CDC</div><div class="rich"><h4>Pattern: Designing a Batch ETL System</h4><p>Interview: "Design a system to compute daily sales metrics for a 50M-user e-commerce platform."</p><pre>=== Requirements ===\nFunctional:\n  - Daily revenue by product category and region\n  - Top 100 products by revenue per day\n  - Customer lifetime value (CLV) updated daily\n\nNon-functional:\n  - Pipeline must complete by 6am (SLA: 3 hours after midnight)\n  - Data must be queryable by the BI team by 7am\n  - Failures must auto-retry, team alerted if SLA missed\n\n=== Volume math ===\n  50M users Ã— avg 1 order/day = 50M orders/day\n  50M orders Ã— 3 items avg   = 150M order items/day\n  150M rows Ã— 200 bytes      = 30GB/day uncompressed\n  Parquet compression (5x)   = 6GB/day to write\n  Running 1 year             = 2.2TB stored â€” very manageable\n\n=== Architecture ===\n  Source: OLTP PostgreSQL â†’ CDC (Debezium) â†’ Kafka topics\n  Landing zone: Kafka â†’ raw Parquet on S3 (append every 5 min)\n  Daily batch: Airflow DAG at 00:30 UTC\n    Task 1: Spark reads raw S3 Parquet, computes dim + fact tables\n    Task 2: dbt runs transformation models + tests\n    Task 3: Snowflake/BigQuery tables updated for BI queries\n    Task 4: SLA check â€” alert if not done by 03:00 UTC</pre><h4>CDC â€” Change Data Capture</h4><p><strong>CDC (Change Data Capture)</strong> captures database changes (INSERT/UPDATE/DELETE) in real time from the database\'s transaction log, without polling. Debezium is the open-source tool that reads PostgreSQL/MySQL WAL logs and publishes changes as Kafka messages.</p><pre># Why CDC instead of "SELECT * WHERE updated_at > last_run_time"?\n\n# Problem with polling:\n  1. Misses DELETEs (deleted rows cannot be queried)\n  2. Missing updated_at on some tables\n  3. High load on source DB (full table scans)\n  4. Possible race condition: row updated between runs\n\n# CDC reads from the WAL (Write-Ahead Log) directly:\n  - Captures every INSERT/UPDATE/DELETE as it happens\n  - Zero load on source tables\n  - No missed changes\n  - Before/after images of every row change\n\n# Debezium Kafka message format:\n{\n  "op": "u",         # u=update, c=create, d=delete\n  "before": {"user_id": 42, "tier": "silver"},\n  "after":  {"user_id": 42, "tier": "gold"},\n  "ts_ms": 1704067200000\n}</pre></div></div><div class="level level-3"><div class="level-badge">ğŸŸ¡ Level 3 â€” Lambda vs Kappa Architecture</div><div class="rich"><h4>Pattern: Lambda Architecture + Lambda vs Kappa</h4><p><strong>Lambda Architecture:</strong> run two parallel systems â€” a batch layer for accuracy (recomputes everything correctly, but slowly) and a speed layer for freshness (approximate real-time results using streaming). Serving layer merges both.</p><pre>Lambda Architecture:\n\nRaw data â”€â”€â”¬â”€â”€ Batch Layer (Spark, daily job) â”€â”€â”€â”€â”€â”€â–º Batch views (exact)\n           â”‚                                                  â”‚\n           â””â”€â”€ Speed Layer (Kafka+Flink, real-time) â”€â”€â–º Real-time views â”€â”€â–º Serving Layer â”€â”€â–º App\n\nPros: batch layer corrects errors in real-time layer\nCons: two codebases to maintain, possible inconsistencies between layers\n\nKappa Architecture:\n  Only a speed layer. Use streaming for EVERYTHING, including historical reprocessing.\n  If you need to reprocess, replay from Kafka (if retention long enough) or S3.\n  Pros: one codebase. Cons: streaming code is harder, Kafka storage costs.\n\nTrend at FAANG: Kappa is winning for new systems.\nLambda still common in legacy systems built 2015-2020.</pre><h4>Trade-Offs Matrix â€” For System Design Interviews</h4><table><tr><th>Requirement</th><td>Batch</td><td>Micro-batch (Spark SS)</td><td>Streaming (Flink/Kafka)</td></tr><tr><th>Data freshness needed</th><td>Hours/daily</td><td>5-30 min</td><td><1 min</td></tr><tr><th>Historical reprocessing</th><td>Easy: re-run job</td><td>Moderate</td><td>Complex: replay from log</td></tr><tr><th>Exactly-once guarantee</th><td>Easy: idempotent writes</td><td>Possible with checkpointing</td><td>Hard: requires distributed transactions</td></tr><tr><th>Dev complexity</th><td>Low</td><td>Medium</td><td>High</td></tr><tr><th>Infrastructure cost</th><td>Low (bursty)</td><td>Medium (semi-continuous)</td><td>High (always-on)</td></tr></table></div></div><div class="level level-4"><div class="level-badge">ğŸ”´ Level 4 â€” Real-Time Fraud Detection Design</div><div class="rich"><h4>Complete System Design: Real-Time Fraud Detection</h4><pre>Interview: "Design a real-time fraud detection system for a payments company."\n\n=== Functional Requirements ===\n  - Flag suspicious transactions within 500ms of payment attempt\n  - 10,000 transactions/second peak\n  - Fraud signal based on: velocity (too many txns), location change,\n    unusual amount, high-risk merchant category\n\n=== Architecture ===\n  Step 1: Ingestion\n    Payment service â†’ Kafka topic "transactions" (partitioned by user_id)\n    Partitioning by user_id ensures all events for one user â†’ same consumer\n\n  Step 2: Feature computation (Flink, <100ms)\n    - 5-min tumbling window: count txns per user_id â†’ velocity feature\n    - 1-hour rolling window: SUM(amount) per user â†’ amount feature\n    - Point lookup: last known location from Redis â†’ location change\n\n  Step 3: Fraud scoring\n    - ML model (loaded in Flink job): score transaction given features\n    - Or: rule engine: IF velocity > 10 AND amount > $1000 â†’ high risk\n\n  Step 4: Decision output\n    - Low risk: allow â†’ write event to "fraud_scores" Kafka topic\n    - High risk: block â†’ write to "blocked_txns" + alert fraud team\n\n  Step 5: Storage\n    - All scored transactions â†’ Kafka â†’ S3 Parquet (audit log)\n    - Daily batch: Spark recomputes features with full history â†’ retrain ML model\n\n=== Key design decisions to state ===\n  1. Why Kafka? â†’ decouples payment service from fraud service\n  2. Why Flink? â†’ stateful streaming, low latency, exactly-once\n  3. Why Redis? â†’ O(1) user state lookups (cannot query DW in 500ms)\n  4. Why batch layer too? â†’ retrain ML model with corrected labels weekly</pre></div></div></div>',
        'key_concepts': ['DE system design adds: volume math, latency requirements, storage formats, streaming vs batch trade-offs.', 'Back-of-envelope: 1B events/day Ã— 100 bytes = 100GB/day. With 5x Parquet compression = 20GB/day.', 'Batch: simple, cheaper, hours of latency. Streaming: complex, expensive, seconds of latency.', 'CDC: reads database transaction log (WAL). Captures INSERT/UPDATE/DELETE including hard DELETES. No source load.', 'Lambda: batch (accurate) + streaming (fast) layers. Complex: two codebases. Used in legacy systems.', 'Kappa: streaming only. Simpler codebase. Historical reprocessing via log replay. Winning in new systems.', 'Fraud detection: Kafka â†’ Flink (stateful windows) â†’ ML scoring â†’ Redis (block/allow) â†’ result topic.', "Always justify architecture choices: 'I chose Kafka because...', 'I chose Flink over Spark SS because...'"],
        'hints': ['Interview: always start with volume math before architecture. Shows structured engineering thinking.', 'CDC vs polling: CDC catches DELETEs (polling cannot). CDC has zero source DB load. CDC is always preferred.', 'Lambda vs Kappa: Lambda = two codebases (batch + streaming). Kappa = one codebase (streaming only). New systems = Kappa.', 'Fraud detection latency budget: 500ms total = 50ms Kafka â†’ 200ms Flink â†’ 100ms Redis lookup â†’ 150ms ML scoring.'],
        'tasks': ['<strong>Step 1:</strong> Back-of-envelope: 100M daily active users, each viewing 20 pages/day, each page view is 500 bytes. Calculate: daily data volume (uncompressed + Parquet compressed), monthly storage cost at $0.023/GB-month on S3.', '<strong>Step 2:</strong> Design a batch ETL system for a SaaS company: daily computation of MRR (monthly recurring revenue) by plan tier and country. Draw the pipeline from OLTP to BI-ready table.', '<strong>Step 3:</strong> Explain CDC to a non-technical stakeholder in 3 sentences. Then explain to a technical interviewer why it is better than polling.', "<strong>Step 4:</strong> Mock interview: 'Design a real-time recommendation system for Netflix (movies to show on homepage, updated every 5 minutes based on viewing history).' Use the structured framework: requirements â†’ volume math â†’ architecture â†’ trade-offs."],
        'hard_problem': "Boss Problem (Stripe): Design a complete data platform for Stripe's analytics needs. Requirements: (1) 100M transactions/day, real-time fraud detection within 200ms; (2) Daily financial reports (revenue by country, product, merchant type) available by 6am UTC; (3) Ad-hoc SQL queries by 200 data analysts on 5 years of historical data; (4) GDPR: delete all data for a user within 30 days of request; (5) Data catalog: any analyst can discover what tables exist and their meaning. Design: the complete architecture (streaming + batch + storage + catalog + governance), justify every component choice, and explain the failure modes.",
    },

}
